{"cells":[{"cell_type":"markdown","metadata":{"id":"Jx_pafU4toh5"},"source":["For google colab environment"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":62178,"status":"ok","timestamp":1739125023921,"user":{"displayName":"Dinu Ion George","userId":"14034259797438922081"},"user_tz":-120},"id":"xe8Eih0hHc2-","outputId":"743f5891-2c7b-47d2-8a69-f16556e350f8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2642,"status":"ok","timestamp":1739125026566,"user":{"displayName":"Dinu Ion George","userId":"14034259797438922081"},"user_tz":-120},"id":"YmNUmwcdHc3B","outputId":"977a2ccb-e8fd-46cb-86ca-d2216ba8c1a0"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/Othercomputers/ASUS Main/MLCP/03_Classifier/__ColabEnvironment\n"]}],"source":["cd \"/content/drive/Othercomputers/ASUS Main/MLCP/03_Classifier/__ColabEnvironment\""]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2559,"status":"ok","timestamp":1739125029135,"user":{"displayName":"Dinu Ion George","userId":"14034259797438922081"},"user_tz":-120},"id":"qIIAqc0VIDm4","outputId":"bf48376b-8484-4bcc-9275-d9663e9db1a3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting scikit-multilearn\n","  Downloading scikit_multilearn-0.2.0-py3-none-any.whl.metadata (6.0 kB)\n","Downloading scikit_multilearn-0.2.0-py3-none-any.whl (89 kB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/89.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.4/89.4 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: scikit-multilearn\n","Successfully installed scikit-multilearn-0.2.0\n"]}],"source":["# install dependencies\n","\n","# !pip install catboost\n","!pip install scikit-multilearn\n","# !pip install --upgrade tensorflow-addons\n","# !pip install sentence-transformers\n","# !pip install --upgrade tensorflow-addons\n","# !pip install dask[dataframe]\n"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1739125029162,"user":{"displayName":"Dinu Ion George","userId":"14034259797438922081"},"user_tz":-120},"id":"T2lmJ49jGPJE","outputId":"00f6a6fc-7191-4e93-dee0-4f2a22cf489f"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/Othercomputers/ASUS Main/MLCP/03_Classifier\n"]}],"source":["import sys\n","import os\n","\n","# Add the parent directory of app_config to the Python path\n","sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n","print(os.path.abspath(os.path.join(os.getcwd(), '..')))\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2220,"status":"ok","timestamp":1739125031387,"user":{"displayName":"Dinu Ion George","userId":"14034259797438922081"},"user_tz":-120},"id":"7T7YDX_3GPJE","outputId":"ab9fe356-f80d-44e0-c36c-0394a5bce357"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/Othercomputers/ASUS Main/MLCP\n"]}],"source":["from app_config import AppConfig\n","\n","# define configuration proxy\n","configProxy = AppConfig()\n","CONFIG = configProxy.return_config()\n","\n","print(CONFIG['BASE_DIR'])"]},{"cell_type":"markdown","metadata":{"id":"-d7Bz7antoh8"},"source":["Start code here"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":269},"executionInfo":{"elapsed":2129,"status":"error","timestamp":1739027432803,"user":{"displayName":"Dinu Ion George","userId":"14034259797438922081"},"user_tz":-120},"id":"vnQee3H_Hc3B","outputId":"93859f2d-fe9d-4e36-8869-ed550be4af9a"},"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-31747437e9e0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mapp_src\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCustomEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert-base-uncased\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"This is a test\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"This is another test\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"This is a third test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/Othercomputers/ASUS Main/MLCP/03_Classifier/app_src/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mCustomEncoder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCustomEncoder\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mCustomEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mClassifierChainWrapper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClassifierChainWrapper\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mClassifierChainWrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mDecisionTreeEvaluator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDecisionTreeEvaluator\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mDecisionTreeEvaluator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/Othercomputers/ASUS Main/MLCP/03_Classifier/app_src/CustomEncoder.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# related third-party\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTFAutoModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Check the dependencies satisfy the minimal versions required.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdependency_versions_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m from .utils import (\n\u001b[1;32m     28\u001b[0m     \u001b[0mOptionalDependencyNotAvailable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/dependency_versions_check.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdependency_versions_table\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequire_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_version_core\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbackbone_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBackboneConfigMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBackboneMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mchat_template_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDocstringParsingException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeHintParsingException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_json_schema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconstants\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIMAGENET_DEFAULT_MEAN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMAGENET_DEFAULT_STD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMAGENET_STANDARD_MEAN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMAGENET_STANDARD_STD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m from .doc import (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/chat_template_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_jinja_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mjinja2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mjinja2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExtension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mjinja2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msandbox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImmutableSandboxedEnvironment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jinja2/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbccache\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFileSystemBytecodeCache\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mFileSystemBytecodeCache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbccache\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMemcachedBytecodeCache\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mMemcachedBytecodeCache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0menvironment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEnvironment\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mEnvironment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0menvironment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTemplate\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mTemplate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTemplateAssertionError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mTemplateAssertionError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jinja2/environment.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCodeType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmarkupsafe\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMarkup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_compile_bytecode\u001b[0;34m(data, name, bytecode_path, source_path)\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["from app_src import CustomEncoder\n","\n","encoder = CustomEncoder(\"bert-base-uncased\")\n","data = [\"This is a test\", \"This is another test\", \"This is a third test\"]\n","\n","encodded_sentence = encoder.encode_problem_statement(data)\n","print(encodded_sentence)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":332},"executionInfo":{"elapsed":23706,"status":"error","timestamp":1738784679685,"user":{"displayName":"Dinu Ion George","userId":"14034259797438922081"},"user_tz":-120},"id":"AL8NTNOKHc3C","outputId":"abb35947-c589-4a59-e7ac-fde63c1314b8"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n","Dask dataframe query planning is disabled because dask-expr is not installed.\n","\n","You can install it with `pip install dask[dataframe]` or `conda install dask`.\n","This will raise in a future version.\n","\n","  warnings.warn(msg, FutureWarning)\n"]},{"output_type":"error","ename":"TypeError","evalue":"ClassifierChainWrapper.__init__() takes 2 positional arguments but 4 were given","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-97d0783496a7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mclassifierChainWrapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassifierChainWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bert-base-uncased\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mclassifierChainWrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmetrics_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifierChainWrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: ClassifierChainWrapper.__init__() takes 2 positional arguments but 4 were given"]}],"source":["from app_src import ClassifierChainWrapper\n","from sklearn.ensemble import RandomForestClassifier\n","\n","classifierChainWrapper = ClassifierChainWrapper(RandomForestClassifier(), \"bert-base-uncased\", 5)\n","classifierChainWrapper.fit()\n","metrics_results = classifierChainWrapper.predict()\n","\n","for metric_name, metric_value in metrics_results.items():\n","    print(f\"{metric_name}: {metric_value}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"elapsed":21,"status":"error","timestamp":1738784694006,"user":{"displayName":"Dinu Ion George","userId":"14034259797438922081"},"user_tz":-120},"id":"2V2DZ5O5wpXk","outputId":"d20aeebd-4329-42fc-84ea-6727ce8dad68"},"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"'DecisionTreeEvaluator' object has no attribute 'benchmark_models'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-f48c8ed72720>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdecisionTreeEvaluator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecisionTreeEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdecisionTreeEvaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbenchmark_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_tags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# decisionTreeEvaluator.benchmark_models(encoder_batch_size=32, number_of_tags=10)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# decisionTreeEvaluator.benchmark_models(encoder_batch_size=32, number_of_tags=15)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'DecisionTreeEvaluator' object has no attribute 'benchmark_models'"]}],"source":["from app_src import DecisionTreeEvaluator\n","\n","decisionTreeEvaluator = DecisionTreeEvaluator()\n","decisionTreeEvaluator.benchmark_models(encoder_batch_size=32, number_of_tags=5)\n","# decisionTreeEvaluator.benchmark_models(encoder_batch_size=32, number_of_tags=10)\n","# decisionTreeEvaluator.benchmark_models(encoder_batch_size=32, number_of_tags=15)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4154,"status":"ok","timestamp":1738441980041,"user":{"displayName":"Dinu Ion George","userId":"14034259797438922081"},"user_tz":-120},"id":"UMkaJZsOBid0","outputId":"607490d9-83ea-4484-8a63-e7bc11428d7b"},"outputs":[{"output_type":"stream","name":"stdout","text":["2.18.0\n","CUDA version: 12.5.1\n","CUDNN version: 9\n"]}],"source":["import tensorflow as tf\n","print(tf.__version__)\n","\n","build_info = tf.sysconfig.get_build_info()\n","print(\"CUDA version:\", build_info.get(\"cuda_version\", \"Unknown\"))\n","print(\"CUDNN version:\", build_info.get(\"cudnn_version\", \"Unknown\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":729,"referenced_widgets":["7ee763e22b064a0fbcf9b42ccf97b9ca","40fb4eea828a47eebde3deffb34eccaf","6c28c29db2b54a68809be4927f2e6561","156e7b3dc5c74377abf997088f5d5039","143f3f72648947628be426536c54165a","d465a2294ac346fcaeb1e40f905431ed","e8217faab1a64813989a48f3bc71980e","31789b2281b447e9a533802256b73e32","ffa3a842b49943ba8fffe30c63b7e803","0e0b5259abee4a00b2aed377427faebc","b1298a3565b24588878eab99a2f778d2"]},"executionInfo":{"elapsed":222505,"status":"ok","timestamp":1738622095096,"user":{"displayName":"Dinu Ion George","userId":"14034259797438922081"},"user_tz":-120},"id":"_DhP3q-Htoh9","outputId":"8783a6b1-64b3-4e8d-bf2b-f3990fc2ee04"},"outputs":[{"output_type":"stream","name":"stdout","text":["GPU is available\n","GPU details: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n","Dask dataframe query planning is disabled because dask-expr is not installed.\n","\n","You can install it with `pip install dask[dataframe]` or `conda install dask`.\n","This will raise in a future version.\n","\n","  warnings.warn(msg, FutureWarning)\n","/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["model.safetensors:  24%|##3       | 126M/532M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ee763e22b064a0fbcf9b42ccf97b9ca"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFMPNetModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing TFMPNetModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFMPNetModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights or buffers of the TF 2.0 model TFMPNetModel were not initialized from the PyTorch model and are newly initialized: ['mpnet.pooler.dense.weight', 'mpnet.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 962ms/step - auc: 0.5180 - binary_accuracy: 0.4868 - label_wise_accuracy: 0.5003 - label_wise_f1_score: 0.4110 - label_wise_macro_f1: 0.4369 - loss: 0.7062 - prc_auc: 0.3934 - precision: 0.3981 - recall: 0.6753 - subset_accuracy: 0.0170 - subset_f1: 0.4971 - subset_precision: 0.3978 - subset_recall: 0.6687\n","Epoch 1: Validation Metrics:\n","loss: 0.7000761032104492\n","val_label_wise_f1_score: [0.6046511  0.31999996 0.         0.6666666  0.        ]\n","val_label_wise_accuracy: [0.46875 0.46875 0.6875  0.53125 0.71875]\n","val_binary_accuracy: 0.5161765217781067\n","val_precision: 0.40509089827537537\n","val_recall: 0.5279620885848999\n","val_label_wise_macro_f1: 0.35385945439338684\n","val_subset_accuracy: 0.03492647036910057\n","val_subset_precision: 0.4010416865348816\n","val_subset_recall: 0.5241115093231201\n","val_subset_f1: 0.45356810092926025\n","val_auc: 0.5249963998794556\n","val_prc_auc: 0.40708646178245544\n","\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 1s/step - auc: 0.5181 - binary_accuracy: 0.4868 - label_wise_accuracy: 0.5008 - label_wise_f1_score: 0.4104 - label_wise_macro_f1: 0.4367 - loss: 0.7061 - prc_auc: 0.3935 - precision: 0.3981 - recall: 0.6750 - subset_accuracy: 0.0171 - subset_f1: 0.4970 - subset_precision: 0.3979 - subset_recall: 0.6683 - val_auc: 0.5250 - val_binary_accuracy: 0.5162 - val_label_wise_accuracy: 0.5750 - val_label_wise_f1_score: 0.3183 - val_label_wise_macro_f1: 0.3539 - val_loss: 0.6918 - val_prc_auc: 0.4071 - val_precision: 0.4051 - val_recall: 0.5280 - val_subset_accuracy: 0.0349 - val_subset_f1: 0.4536 - val_subset_precision: 0.4010 - val_subset_recall: 0.5241\n","Transformer model saved to /content/drive/Othercomputers/ASUS Main/MLCP/05_MODELS/02_Transformer_Models/transformer_model_20250203_223120\n","Model saved to /content/drive/Othercomputers/ASUS Main/MLCP/05_MODELS/01_Custom_Models/custom_model_20250203_223120.weights.h5\n"]}],"source":["import os\n","import tensorflow as tf\n","\n","# Check if GPU is available and print details\n","if tf.config.experimental.list_physical_devices('GPU'):\n","    print(\"GPU is available\")\n","    print(\"GPU details:\", tf.config.list_physical_devices('GPU'))\n","else:\n","    print(\"GPU not available, using CPU\")\n","\n","# # If GPU is available, try setting memory growth\n","# gpus = tf.config.list_physical_devices('GPU')\n","# if gpus:\n","#     try:\n","#         # Currently, memory growth needs to be the same across GPUs\n","#         for gpu in gpus:\n","#             tf.config.experimental.set_memory_growth(gpu, True)\n","#         logical_gpus = tf.config.list_logical_devices('GPU')\n","#         print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n","#     except RuntimeError as e:\n","#         # Memory growth must be set before GPUs have been initialized\n","#         print('Error')\n","#         print(e)\n","\n","# local application/library specific imports\n","from app_src import SentenceTransformerWrapper\n","from app_config import AppConfig\n","\n","# define configuration proxy\n","configProxy = AppConfig()\n","CONFIG = configProxy.return_config()\n","\n","NUMBER_OF_TAGS = 5\n","\n","sentenceTransformerWrapper = SentenceTransformerWrapper(\"microsoft/mpnet-base\", NUMBER_OF_TAGS)\n","sentenceTransformerWrapper.train_model(\n","    train_dataset_path=CONFIG[f'TOP_{NUMBER_OF_TAGS}_TRAINING_DATASET_PATH'],\n","    val_dataset_path=CONFIG[f'TOP_{NUMBER_OF_TAGS}_VALIDATION_DATASET_PATH'],\n","    epochs=1,\n","    batch_size=32,\n","    train_model=True\n","    )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":59070,"status":"ok","timestamp":1738622242973,"user":{"displayName":"Dinu Ion George","userId":"14034259797438922081"},"user_tz":-120},"id":"Pd4K7EOOtoh9","outputId":"d6499eee-9aa8-4b77-fa81-b902b942e2ee"},"outputs":[{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFMPNetModel.\n","\n","All the layers of TFMPNetModel were initialized from the model checkpoint at /content/drive/Othercomputers/ASUS Main/MLCP/05_MODELS/02_Transformer_Models/transformer_model_20250203_223120.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMPNetModel for predictions without further training.\n","/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adamw', because it has 2 variables whereas the saved optimizer has 6 variables. \n","  saveable.load_own_variables(weights_store.get(inner_path))\n"]},{"output_type":"stream","name":"stdout","text":["Model loaded from /content/drive/Othercomputers/ASUS Main/MLCP/05_MODELS/01_Custom_Models/custom_model_20250203_223120.weights.h5\n","\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 990ms/step - auc: 0.5208 - binary_accuracy: 0.5071 - label_wise_accuracy: 0.5067 - label_wise_f1_score: 0.3501 - label_wise_macro_f1: 0.3516 - loss: 0.6924 - prc_auc: 0.4088 - precision: 0.3994 - recall: 0.5128 - subset_accuracy: 0.0333 - subset_f1: 0.4498 - subset_precision: 0.4007 - subset_recall: 0.5159\n","Evaluation Metrics:\n","Loss: 0.6929304599761963\n","Label F1 Scores: [0.56410253 0.4705882  0.09999999 0.43243238 0.        ]\n","Label Accuracies: [0.46875 0.4375  0.4375  0.34375 0.75   ]\n","Accuracy: 0.5071220993995667\n","Precision: 0.3944847583770752\n","Recall: 0.5101351141929626\n","F1 Score: 0.3509281873703003\n","Subset Accuracy: 0.028343023732304573\n","Subset Precision: 0.3935804069042206\n","Subset Recall: 0.5103682279586792\n","Subset F1: 0.44326552748680115\n","AUC: 0.5183724164962769\n","PRC AUC: 0.4017588496208191\n"]},{"output_type":"execute_result","data":{"text/plain":["{'Loss': 0.6929304599761963,\n"," 'Label F1 Scores': array([0.56410253, 0.4705882 , 0.09999999, 0.43243238, 0.        ],\n","       dtype=float32),\n"," 'Label Accuracies': array([0.46875, 0.4375 , 0.4375 , 0.34375, 0.75   ], dtype=float32),\n"," 'Accuracy': 0.5071220993995667,\n"," 'Precision': 0.3944847583770752,\n"," 'Recall': 0.5101351141929626,\n"," 'F1 Score': 0.3509281873703003,\n"," 'Subset Accuracy': 0.028343023732304573,\n"," 'Subset Precision': 0.3935804069042206,\n"," 'Subset Recall': 0.5103682279586792,\n"," 'Subset F1': 0.44326552748680115,\n"," 'AUC': 0.5183724164962769,\n"," 'PRC AUC': 0.4017588496208191}"]},"metadata":{},"execution_count":12}],"source":["import os\n","\n","# local application/library specific imports\n","from app_src import SentenceTransformerWrapper\n","from app_config import AppConfig\n","\n","# define configuration proxy\n","configProxy = AppConfig()\n","CONFIG = configProxy.return_config()\n","\n","NUMBER_OF_TAGS = 5\n","\n","sentenceTransformerWrapper = SentenceTransformerWrapper(\"microsoft/mpnet-base\", NUMBER_OF_TAGS)\n","\n","sentenceTransformerWrapper.benchmark_model(\n","    test_dataset_path=CONFIG[f'TOP_{NUMBER_OF_TAGS}_TESTING_DATASET_PATH'],\n","    batch_size=32,\n","    model_path= os.path.join(CONFIG[\"MODEL_SAVE_PATH_ROOT\"], 'custom_model_20250203_223120.weights.h5'),\n","    transformer_model_path= os.path.join(CONFIG[\"TRANSFORMER_SAVE_PATH_ROOT\"], 'transformer_model_20250203_223120')\n","    )"]},{"cell_type":"code","source":["# local application/library specific imports\n","from app_src import TransformerEvaluator\n","from app_config import AppConfig\n","\n","# define configuration proxy\n","configProxy = AppConfig()\n","CONFIG = configProxy.return_config()\n","\n","transformer_evaluator = TransformerEvaluator()\n","transformer_evaluator.evaluate_models(\n","    epochs=10,\n","    batch_size=32,\n","    number_of_tags=5,\n","    train_model=True,\n","    threshold=0.5,\n","    transformer_model_path=os.path.join(CONFIG[\"TRANSFORMER_SAVE_PATH_ROOT\"], 'transformer_model_20250208_171427')\n","  )\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mCcdWL_XfxIG","executionInfo":{"status":"ok","timestamp":1739037692387,"user_tz":-120,"elapsed":144890,"user":{"displayName":"Dinu Ion George","userId":"14034259797438922081"}},"outputId":"89c35841-e63c-4be1-9703-029b16a6528a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training and evaluating model: sentence-transformers/all-mpnet-base-v2\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFMPNetModel.\n","\n","All the layers of TFMPNetModel were initialized from the model checkpoint at /content/drive/Othercomputers/ASUS Main/MLCP/05_MODELS/02_Transformer_Models/transformer_model_20250208_171427.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMPNetModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - auc: 0.5511 - binary_accuracy: 0.5369 - label_wise_accuracy: 0.5573 - label_wise_f1_score: 0.4525 - label_wise_macro_f1: 0.4328 - loss: 0.6937 - prc_auc: 0.3364 - precision: 0.3567 - recall: 0.5860 - subset_accuracy: 0.0276 - subset_f1: 0.4476 - subset_precision: 0.3572 - subset_recall: 0.6044\n","Epoch 1: Validation Metrics:\n","loss: 0.685583233833313\n","val_label_wise_f1_score: [0.39999992 0.42424238 0.7619047  0.36363634 0.19999996]\n","val_label_wise_accuracy: [0.71875 0.40625 0.84375 0.34375 0.75   ]\n","val_binary_accuracy: 0.5483333468437195\n","val_precision: 0.35689353942871094\n","val_recall: 0.5410053133964539\n","val_label_wise_macro_f1: 0.4147389233112335\n","val_subset_accuracy: 0.02916666679084301\n","val_subset_precision: 0.3473958373069763\n","val_subset_recall: 0.5519444346427917\n","val_subset_f1: 0.42459172010421753\n","val_auc: 0.563468337059021\n","val_prc_auc: 0.3505057692527771\n","\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 128ms/step - auc: 0.5513 - binary_accuracy: 0.5370 - label_wise_accuracy: 0.5577 - label_wise_f1_score: 0.4523 - label_wise_macro_f1: 0.4329 - loss: 0.6937 - prc_auc: 0.3366 - precision: 0.3568 - recall: 0.5861 - subset_accuracy: 0.0276 - subset_f1: 0.4477 - subset_precision: 0.3573 - subset_recall: 0.6045 - val_auc: 0.5635 - val_binary_accuracy: 0.5483 - val_label_wise_accuracy: 0.6125 - val_label_wise_f1_score: 0.4300 - val_label_wise_macro_f1: 0.4147 - val_loss: 0.6866 - val_prc_auc: 0.3505 - val_precision: 0.3569 - val_recall: 0.5410 - val_subset_accuracy: 0.0292 - val_subset_f1: 0.4246 - val_subset_precision: 0.3474 - val_subset_recall: 0.5519\n","Epoch 2/10\n","\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - auc: 0.6690 - binary_accuracy: 0.6054 - label_wise_accuracy: 0.6192 - label_wise_f1_score: 0.5068 - label_wise_macro_f1: 0.4899 - loss: 0.6651 - prc_auc: 0.4738 - precision: 0.4155 - recall: 0.6235 - subset_accuracy: 0.0490 - subset_f1: 0.5099 - subset_precision: 0.4180 - subset_recall: 0.6585\n","Epoch 2: Validation Metrics:\n","loss: 0.6582651734352112\n","val_label_wise_f1_score: [0.30769226 0.4137931  0.69565207 0.37499997 0.19999996]\n","val_label_wise_accuracy: [0.71875 0.46875 0.78125 0.375   0.75   ]\n","val_binary_accuracy: 0.57874995470047\n","val_precision: 0.37536656856536865\n","val_recall: 0.5079365372657776\n","val_label_wise_macro_f1: 0.41401436924934387\n","val_subset_accuracy: 0.03541666641831398\n","val_subset_precision: 0.36145833134651184\n","val_subset_recall: 0.5265277624130249\n","val_subset_f1: 0.42733949422836304\n","val_auc: 0.6100860834121704\n","val_prc_auc: 0.41884559392929077\n","\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 85ms/step - auc: 0.6692 - binary_accuracy: 0.6055 - label_wise_accuracy: 0.6192 - label_wise_f1_score: 0.5060 - label_wise_macro_f1: 0.4900 - loss: 0.6651 - prc_auc: 0.4741 - precision: 0.4156 - recall: 0.6236 - subset_accuracy: 0.0490 - subset_f1: 0.5100 - subset_precision: 0.4182 - subset_recall: 0.6586 - val_auc: 0.6101 - val_binary_accuracy: 0.5787 - val_label_wise_accuracy: 0.6187 - val_label_wise_f1_score: 0.3984 - val_label_wise_macro_f1: 0.4140 - val_loss: 0.6685 - val_prc_auc: 0.4188 - val_precision: 0.3754 - val_recall: 0.5079 - val_subset_accuracy: 0.0354 - val_subset_f1: 0.4273 - val_subset_precision: 0.3615 - val_subset_recall: 0.5265\n","Epoch 3/10\n","\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - auc: 0.7420 - binary_accuracy: 0.6455 - label_wise_accuracy: 0.6567 - label_wise_f1_score: 0.5491 - label_wise_macro_f1: 0.5314 - loss: 0.6412 - prc_auc: 0.6220 - precision: 0.4553 - recall: 0.6435 - subset_accuracy: 0.0802 - subset_f1: 0.5505 - subset_precision: 0.4621 - subset_recall: 0.6849\n","Epoch 3: Validation Metrics:\n","loss: 0.6354659199714661\n","val_label_wise_f1_score: [0.26666662 0.4545454  0.7272727  0.42424238 0.19999996]\n","val_label_wise_accuracy: [0.65625 0.625   0.8125  0.40625 0.75   ]\n","val_binary_accuracy: 0.6029166579246521\n","val_precision: 0.4010050296783447\n","val_recall: 0.5277777910232544\n","val_label_wise_macro_f1: 0.44448867440223694\n","val_subset_accuracy: 0.06041666492819786\n","val_subset_precision: 0.3982638716697693\n","val_subset_recall: 0.544583261013031\n","val_subset_f1: 0.45919081568717957\n","val_auc: 0.6365660429000854\n","val_prc_auc: 0.45439836382865906\n","\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 85ms/step - auc: 0.7421 - binary_accuracy: 0.6456 - label_wise_accuracy: 0.6567 - label_wise_f1_score: 0.5481 - label_wise_macro_f1: 0.5315 - loss: 0.6412 - prc_auc: 0.6221 - precision: 0.4555 - recall: 0.6436 - subset_accuracy: 0.0802 - subset_f1: 0.5506 - subset_precision: 0.4622 - subset_recall: 0.6850 - val_auc: 0.6366 - val_binary_accuracy: 0.6029 - val_label_wise_accuracy: 0.6500 - val_label_wise_f1_score: 0.4145 - val_label_wise_macro_f1: 0.4445 - val_loss: 0.6544 - val_prc_auc: 0.4544 - val_precision: 0.4010 - val_recall: 0.5278 - val_subset_accuracy: 0.0604 - val_subset_f1: 0.4592 - val_subset_precision: 0.3983 - val_subset_recall: 0.5446\n","Epoch 4/10\n","\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - auc: 0.7762 - binary_accuracy: 0.6790 - label_wise_accuracy: 0.6891 - label_wise_f1_score: 0.5766 - label_wise_macro_f1: 0.5674 - loss: 0.6214 - prc_auc: 0.6706 - precision: 0.4926 - recall: 0.6624 - subset_accuracy: 0.1126 - subset_f1: 0.5932 - subset_precision: 0.5088 - subset_recall: 0.7151\n","Epoch 4: Validation Metrics:\n","loss: 0.6164834499359131\n","val_label_wise_f1_score: [0.42105258 0.49999994 0.7826087  0.41666663 0.18181816]\n","val_label_wise_accuracy: [0.65625 0.6875  0.84375 0.5625  0.71875]\n","val_binary_accuracy: 0.6329166889190674\n","val_precision: 0.43001121282577515\n","val_recall: 0.5079365372657776\n","val_label_wise_macro_f1: 0.44775015115737915\n","val_subset_accuracy: 0.10208333283662796\n","val_subset_precision: 0.42552080750465393\n","val_subset_recall: 0.5277429819107056\n","val_subset_f1: 0.46965429186820984\n","val_auc: 0.6516899466514587\n","val_prc_auc: 0.4699982702732086\n","\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 85ms/step - auc: 0.7762 - binary_accuracy: 0.6790 - label_wise_accuracy: 0.6892 - label_wise_f1_score: 0.5757 - label_wise_macro_f1: 0.5674 - loss: 0.6213 - prc_auc: 0.6707 - precision: 0.4927 - recall: 0.6624 - subset_accuracy: 0.1128 - subset_f1: 0.5933 - subset_precision: 0.5089 - subset_recall: 0.7151 - val_auc: 0.6517 - val_binary_accuracy: 0.6329 - val_label_wise_accuracy: 0.6938 - val_label_wise_f1_score: 0.4604 - val_label_wise_macro_f1: 0.4478 - val_loss: 0.6435 - val_prc_auc: 0.4700 - val_precision: 0.4300 - val_recall: 0.5079 - val_subset_accuracy: 0.1021 - val_subset_f1: 0.4697 - val_subset_precision: 0.4255 - val_subset_recall: 0.5277\n","Epoch 5/10\n","\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - auc: 0.7904 - binary_accuracy: 0.7060 - label_wise_accuracy: 0.7117 - label_wise_f1_score: 0.5764 - label_wise_macro_f1: 0.5729 - loss: 0.6048 - prc_auc: 0.6893 - precision: 0.5267 - recall: 0.6508 - subset_accuracy: 0.1565 - subset_f1: 0.6065 - subset_precision: 0.5379 - subset_recall: 0.6992\n","Epoch 5: Validation Metrics:\n","loss: 0.6006518602371216\n","val_label_wise_f1_score: [0.47619042 0.49999994 0.7826087  0.15384611 0.18181816]\n","val_label_wise_accuracy: [0.65625 0.6875  0.84375 0.65625 0.71875]\n","val_binary_accuracy: 0.6587499380111694\n","val_precision: 0.4597701132297516\n","val_recall: 0.4761904776096344\n","val_label_wise_macro_f1: 0.43652769923210144\n","val_subset_accuracy: 0.13958333432674408\n","val_subset_precision: 0.45260417461395264\n","val_subset_recall: 0.4977083206176758\n","val_subset_f1: 0.4730260968208313\n","val_auc: 0.6588786840438843\n","val_prc_auc: 0.4754840135574341\n","\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 85ms/step - auc: 0.7905 - binary_accuracy: 0.7061 - label_wise_accuracy: 0.7117 - label_wise_f1_score: 0.5753 - label_wise_macro_f1: 0.5729 - loss: 0.6048 - prc_auc: 0.6894 - precision: 0.5268 - recall: 0.6508 - subset_accuracy: 0.1566 - subset_f1: 0.6066 - subset_precision: 0.5380 - subset_recall: 0.6991 - val_auc: 0.6589 - val_binary_accuracy: 0.6587 - val_label_wise_accuracy: 0.7125 - val_label_wise_f1_score: 0.4189 - val_label_wise_macro_f1: 0.4365 - val_loss: 0.6352 - val_prc_auc: 0.4755 - val_precision: 0.4598 - val_recall: 0.4762 - val_subset_accuracy: 0.1396 - val_subset_f1: 0.4730 - val_subset_precision: 0.4526 - val_subset_recall: 0.4977\n","Epoch 6/10\n","\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - auc: 0.7946 - binary_accuracy: 0.7167 - label_wise_accuracy: 0.7193 - label_wise_f1_score: 0.5733 - label_wise_macro_f1: 0.5712 - loss: 0.5910 - prc_auc: 0.6943 - precision: 0.5421 - recall: 0.6455 - subset_accuracy: 0.1748 - subset_f1: 0.6207 - subset_precision: 0.5636 - subset_recall: 0.6945\n","Epoch 6: Validation Metrics:\n","loss: 0.5873966813087463\n","val_label_wise_f1_score: [0.47619042 0.49999994 0.7826087  0.18181816 0.18181816]\n","val_label_wise_accuracy: [0.65625 0.6875  0.84375 0.71875 0.71875]\n","val_binary_accuracy: 0.6650000214576721\n","val_precision: 0.46915167570114136\n","val_recall: 0.48280423879623413\n","val_label_wise_macro_f1: 0.4391574561595917\n","val_subset_accuracy: 0.14166666567325592\n","val_subset_precision: 0.4685763716697693\n","val_subset_recall: 0.5063194036483765\n","val_subset_f1: 0.485234797000885\n","val_auc: 0.6610775589942932\n","val_prc_auc: 0.47835785150527954\n","\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 85ms/step - auc: 0.7946 - binary_accuracy: 0.7167 - label_wise_accuracy: 0.7193 - label_wise_f1_score: 0.5722 - label_wise_macro_f1: 0.5712 - loss: 0.5910 - prc_auc: 0.6943 - precision: 0.5421 - recall: 0.6455 - subset_accuracy: 0.1749 - subset_f1: 0.6207 - subset_precision: 0.5637 - subset_recall: 0.6944 - val_auc: 0.6611 - val_binary_accuracy: 0.6650 - val_label_wise_accuracy: 0.7250 - val_label_wise_f1_score: 0.4245 - val_label_wise_macro_f1: 0.4392 - val_loss: 0.6290 - val_prc_auc: 0.4784 - val_precision: 0.4692 - val_recall: 0.4828 - val_subset_accuracy: 0.1417 - val_subset_f1: 0.4852 - val_subset_precision: 0.4686 - val_subset_recall: 0.5063\n","Epoch 7/10\n","\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - auc: 0.7960 - binary_accuracy: 0.7159 - label_wise_accuracy: 0.7185 - label_wise_f1_score: 0.5700 - label_wise_macro_f1: 0.5662 - loss: 0.5794 - prc_auc: 0.6957 - precision: 0.5414 - recall: 0.6400 - subset_accuracy: 0.1860 - subset_f1: 0.6257 - subset_precision: 0.5750 - subset_recall: 0.6895\n","Epoch 7: Validation Metrics:\n","loss: 0.5762372016906738\n","val_label_wise_f1_score: [0.47619042 0.49999994 0.7826087  0.18181816 0.18181816]\n","val_label_wise_accuracy: [0.65625 0.6875  0.84375 0.71875 0.71875]\n","val_binary_accuracy: 0.6666666865348816\n","val_precision: 0.4715026021003723\n","val_recall: 0.48148149251937866\n","val_label_wise_macro_f1: 0.4380333721637726\n","val_subset_accuracy: 0.15208333730697632\n","val_subset_precision: 0.4729166030883789\n","val_subset_recall: 0.5038889050483704\n","val_subset_f1: 0.48624879121780396\n","val_auc: 0.6615892648696899\n","val_prc_auc: 0.4797009229660034\n","\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 85ms/step - auc: 0.7960 - binary_accuracy: 0.7159 - label_wise_accuracy: 0.7185 - label_wise_f1_score: 0.5689 - label_wise_macro_f1: 0.5662 - loss: 0.5793 - prc_auc: 0.6957 - precision: 0.5414 - recall: 0.6401 - subset_accuracy: 0.1861 - subset_f1: 0.6257 - subset_precision: 0.5751 - subset_recall: 0.6895 - val_auc: 0.6616 - val_binary_accuracy: 0.6667 - val_label_wise_accuracy: 0.7250 - val_label_wise_f1_score: 0.4245 - val_label_wise_macro_f1: 0.4380 - val_loss: 0.6245 - val_prc_auc: 0.4797 - val_precision: 0.4715 - val_recall: 0.4815 - val_subset_accuracy: 0.1521 - val_subset_f1: 0.4862 - val_subset_precision: 0.4729 - val_subset_recall: 0.5039\n","Epoch 8/10\n","\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - auc: 0.7964 - binary_accuracy: 0.7183 - label_wise_accuracy: 0.7201 - label_wise_f1_score: 0.5711 - label_wise_macro_f1: 0.5666 - loss: 0.5696 - prc_auc: 0.6960 - precision: 0.5448 - recall: 0.6401 - subset_accuracy: 0.1993 - subset_f1: 0.6331 - subset_precision: 0.5866 - subset_recall: 0.6906\n","Epoch 8: Validation Metrics:\n","loss: 0.5667797327041626\n","val_label_wise_f1_score: [0.47619042 0.49999994 0.7826087  0.16666663 0.18181816]\n","val_label_wise_accuracy: [0.65625 0.6875  0.84375 0.6875  0.71875]\n","val_binary_accuracy: 0.6650000214576721\n","val_precision: 0.469072163105011\n","val_recall: 0.48148149251937866\n","val_label_wise_macro_f1: 0.4411400854587555\n","val_subset_accuracy: 0.15416666865348816\n","val_subset_precision: 0.4713541269302368\n","val_subset_recall: 0.5018055438995361\n","val_subset_f1: 0.4846835136413574\n","val_auc: 0.6612228155136108\n","val_prc_auc: 0.4786034822463989\n","\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 85ms/step - auc: 0.7964 - binary_accuracy: 0.7183 - label_wise_accuracy: 0.7201 - label_wise_f1_score: 0.5700 - label_wise_macro_f1: 0.5667 - loss: 0.5695 - prc_auc: 0.6960 - precision: 0.5448 - recall: 0.6402 - subset_accuracy: 0.1993 - subset_f1: 0.6331 - subset_precision: 0.5867 - subset_recall: 0.6906 - val_auc: 0.6612 - val_binary_accuracy: 0.6650 - val_label_wise_accuracy: 0.7188 - val_label_wise_f1_score: 0.4215 - val_label_wise_macro_f1: 0.4411 - val_loss: 0.6214 - val_prc_auc: 0.4786 - val_precision: 0.4691 - val_recall: 0.4815 - val_subset_accuracy: 0.1542 - val_subset_f1: 0.4847 - val_subset_precision: 0.4714 - val_subset_recall: 0.5018\n","Epoch 9/10\n","\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - auc: 0.7967 - binary_accuracy: 0.7184 - label_wise_accuracy: 0.7206 - label_wise_f1_score: 0.5723 - label_wise_macro_f1: 0.5689 - loss: 0.5612 - prc_auc: 0.6960 - precision: 0.5448 - recall: 0.6432 - subset_accuracy: 0.2019 - subset_f1: 0.6359 - subset_precision: 0.5893 - subset_recall: 0.6939\n","Epoch 9: Validation Metrics:\n","loss: 0.558708906173706\n","val_label_wise_f1_score: [0.47619042 0.49999994 0.7826087  0.16666663 0.18181816]\n","val_label_wise_accuracy: [0.65625 0.6875  0.84375 0.6875  0.71875]\n","val_binary_accuracy: 0.6658333539962769\n","val_precision: 0.47036081552505493\n","val_recall: 0.48280423879623413\n","val_label_wise_macro_f1: 0.443153440952301\n","val_subset_accuracy: 0.15833333134651184\n","val_subset_precision: 0.47343748807907104\n","val_subset_recall: 0.5056249499320984\n","val_subset_f1: 0.4876960515975952\n","val_auc: 0.6611797213554382\n","val_prc_auc: 0.47983023524284363\n","\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 85ms/step - auc: 0.7967 - binary_accuracy: 0.7184 - label_wise_accuracy: 0.7206 - label_wise_f1_score: 0.5713 - label_wise_macro_f1: 0.5689 - loss: 0.5612 - prc_auc: 0.6961 - precision: 0.5449 - recall: 0.6432 - subset_accuracy: 0.2020 - subset_f1: 0.6360 - subset_precision: 0.5893 - subset_recall: 0.6939 - val_auc: 0.6612 - val_binary_accuracy: 0.6658 - val_label_wise_accuracy: 0.7188 - val_label_wise_f1_score: 0.4215 - val_label_wise_macro_f1: 0.4432 - val_loss: 0.6193 - val_prc_auc: 0.4798 - val_precision: 0.4704 - val_recall: 0.4828 - val_subset_accuracy: 0.1583 - val_subset_f1: 0.4877 - val_subset_precision: 0.4734 - val_subset_recall: 0.5056\n","Epoch 10/10\n","\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - auc: 0.7968 - binary_accuracy: 0.7201 - label_wise_accuracy: 0.7220 - label_wise_f1_score: 0.5737 - label_wise_macro_f1: 0.5703 - loss: 0.5541 - prc_auc: 0.6961 - precision: 0.5473 - recall: 0.6435 - subset_accuracy: 0.2062 - subset_f1: 0.6390 - subset_precision: 0.5940 - subset_recall: 0.6947\n","Epoch 10: Validation Metrics:\n","loss: 0.5517735481262207\n","val_label_wise_f1_score: [0.47619042 0.49999994 0.7826087  0.16666663 0.18181816]\n","val_label_wise_accuracy: [0.65625 0.6875  0.84375 0.6875  0.71875]\n","val_binary_accuracy: 0.6649999618530273\n","val_precision: 0.4692307710647583\n","val_recall: 0.4841269850730896\n","val_label_wise_macro_f1: 0.44289031624794006\n","val_subset_accuracy: 0.15833333134651184\n","val_subset_precision: 0.4741319417953491\n","val_subset_recall: 0.5066666007041931\n","val_subset_f1: 0.48856955766677856\n","val_auc: 0.6610590219497681\n","val_prc_auc: 0.47950279712677\n","\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 85ms/step - auc: 0.7968 - binary_accuracy: 0.7201 - label_wise_accuracy: 0.7220 - label_wise_f1_score: 0.5726 - label_wise_macro_f1: 0.5703 - loss: 0.5540 - prc_auc: 0.6961 - precision: 0.5473 - recall: 0.6435 - subset_accuracy: 0.2062 - subset_f1: 0.6390 - subset_precision: 0.5940 - subset_recall: 0.6946 - val_auc: 0.6611 - val_binary_accuracy: 0.6650 - val_label_wise_accuracy: 0.7188 - val_label_wise_f1_score: 0.4215 - val_label_wise_macro_f1: 0.4429 - val_loss: 0.6180 - val_prc_auc: 0.4795 - val_precision: 0.4692 - val_recall: 0.4841 - val_subset_accuracy: 0.1583 - val_subset_f1: 0.4886 - val_subset_precision: 0.4741 - val_subset_recall: 0.5067\n","Model saved to /content/drive/Othercomputers/ASUS Main/MLCP/05_MODELS/01_Custom_Models/custom_model_20250208_171423.weights.h5\n","Using the trained model\n","\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 77ms/step - auc: 0.6679 - binary_accuracy: 0.6590 - label_wise_accuracy: 0.6649 - label_wise_f1_score: 0.4405 - label_wise_macro_f1: 0.4407 - loss: 0.6157 - prc_auc: 0.5097 - precision: 0.4733 - recall: 0.4709 - subset_accuracy: 0.1387 - subset_f1: 0.4873 - subset_precision: 0.4853 - subset_recall: 0.4931\n","Evaluation Metrics:\n","Loss: 0.6178571581840515\n","Label F1 Scores: [0.76923066 0.6666666  0.49999994 0.3636363  0.19999997]\n","Label Accuracies: [0.8125  0.75    0.6875  0.78125 0.75   ]\n","Accuracy: 0.6624999046325684\n","Precision: 0.47161126136779785\n","Recall: 0.475012868642807\n","F1 Score: 0.4389772415161133\n","Subset Accuracy: 0.14638157188892365\n","Subset Precision: 0.4891721308231354\n","Subset Recall: 0.49864307045936584\n","Subset F1: 0.4917735159397125\n","AUC: 0.6639601588249207\n","PRC AUC: 0.49689382314682007\n"]}]},{"cell_type":"code","source":["import os\n","\n","# local application/library specific imports\n","from app_src import SentenceTransformerWrapper\n","from app_config import AppConfig\n","\n","# define configuration proxy\n","configProxy = AppConfig()\n","CONFIG = configProxy.return_config()\n","\n","from app_src import DecisionTreeEvaluator\n","\n","decisionTreeEvaluator = DecisionTreeEvaluator()\n","decisionTreeEvaluator.benchmark_model(\n","    encoder_batch_size=32,\n","    number_of_tags=5,\n","    transformer_name='microsoft/mpnet-base',\n","    model_path= os.path.join(CONFIG[\"MODEL_SAVE_PATH_ROOT\"], 'custom_model_20250203_225321.weights.h5'),\n","    transformer_model_path= os.path.join(CONFIG[\"TRANSFORMER_SAVE_PATH_ROOT\"], 'transformer_model_20250203_225321')\n","    )"],"metadata":{"id":"QNraNQLMG2Ga","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738627541119,"user_tz":-120,"elapsed":495627,"user":{"displayName":"Dinu Ion George","userId":"14034259797438922081"}},"outputId":"5c208d8d-1b71-4a42-84e8-676813dfe553"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n","Dask dataframe query planning is disabled because dask-expr is not installed.\n","\n","You can install it with `pip install dask[dataframe]` or `conda install dask`.\n","This will raise in a future version.\n","\n","  warnings.warn(msg, FutureWarning)\n","/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","All model checkpoint layers were used when initializing TFMPNetModel.\n","\n","All the layers of TFMPNetModel were initialized from the model checkpoint at /content/drive/Othercomputers/ASUS Main/MLCP/05_MODELS/02_Transformer_Models/transformer_model_20250203_225321.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMPNetModel for predictions without further training.\n","/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adamw', because it has 2 variables whereas the saved optimizer has 6 variables. \n","  saveable.load_own_variables(weights_store.get(inner_path))\n"]},{"output_type":"stream","name":"stdout","text":["Loaded transformer model from: /content/drive/Othercomputers/ASUS Main/MLCP/05_MODELS/02_Transformer_Models/transformer_model_20250203_225321\n","Using GPU\n"]},{"output_type":"stream","name":"stderr","text":["Encoding problem statements: 100%|██████████| 158/158 [03:28<00:00,  1.32s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Using GPU\n"]},{"output_type":"stream","name":"stderr","text":["Encoding problem statements: 100%|██████████| 44/44 [00:57<00:00,  1.32s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Benchmarking estimator model: LogisticRegression\n","Benchmarking estimator model: KNeighborsClassifier\n","Benchmarking estimator model: DecisionTreeClassifier\n","Benchmarking estimator model: GaussianNB\n","Benchmarking estimator model: RandomForestClassifier\n","Benchmarking estimator model: XGBClassifier\n","Benchmarking estimator model: LGBMClassifier\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Info] Number of positive: 2241, number of negative: 2784\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025515 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 195840\n","[LightGBM] [Info] Number of data points in the train set: 5025, number of used features: 768\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445970 -> initscore=-0.216967\n","[LightGBM] [Info] Start training from score -0.216967\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Info] Number of positive: 1979, number of negative: 3046\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015391 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 195842\n","[LightGBM] [Info] Number of data points in the train set: 5025, number of used features: 769\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.393831 -> initscore=-0.431238\n","[LightGBM] [Info] Start training from score -0.431238\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Info] Number of positive: 1883, number of negative: 3142\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014327 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 195844\n","[LightGBM] [Info] Number of data points in the train set: 5025, number of used features: 770\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374726 -> initscore=-0.511993\n","[LightGBM] [Info] Start training from score -0.511993\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Info] Number of positive: 1849, number of negative: 3176\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014668 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 195846\n","[LightGBM] [Info] Number of data points in the train set: 5025, number of used features: 771\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.367960 -> initscore=-0.540978\n","[LightGBM] [Info] Start training from score -0.540978\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Info] Number of positive: 1677, number of negative: 3348\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022198 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 195848\n","[LightGBM] [Info] Number of data points in the train set: 5025, number of used features: 772\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.333731 -> initscore=-0.691357\n","[LightGBM] [Info] Start training from score -0.691357\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Benchmarking estimator model: SVC\n"]}]},{"cell_type":"code","source":["import os\n","\n","# local application/library specific imports\n","from app_src import SentenceTransformerWrapper\n","from app_config import AppConfig\n","\n","# define configuration proxy\n","configProxy = AppConfig()\n","CONFIG = configProxy.return_config()\n","\n","from app_src import DecisionTreeEvaluator\n","\n","decisionTreeEvaluator = DecisionTreeEvaluator()\n","decisionTreeEvaluator.benchmark_model(\n","    encoder_batch_size=32,\n","    number_of_tags=5,\n","    transformer_name='sentence-transformers/all-mpnet-base-v2'\n","    )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["a76d43f7e247423d8bd0267409172ae1","8ef25127aea24b65aed075efbf782bd9","4905559eaa5b40779fa2d579308ddeb3","ec09a2d650af46328f1be4ad121ed09f","42f3658a4ec241fc93c783ee84caa9cf","aa274429fd264ada9935779eaa1ca354","faaa9d56430f4cd1854b1775bc3172ba","baa06b0ec2d34d02b113f8503fddc216","d441459d1c884b52b9772285b2fd17f0","9c4cd35bd3c64007a9744c871f60f686","b1ae715e675d45c2957a160069ac7d08","76c7ede6ff5e462a8312821bad1a8202","4dc6ae2f526f4653a54c0c07381768db","c42874edb92e43cca983af83dc96420b","4bdb5352724c40eb9d493cb1efd743d1","d4da583e676f4d08a7e4356e6c19f394","6812d035d701491cb605814f86bcbb3e","4549d9c099ab48e88afcbb1d8f4c789d","224fb6448d974adc8c89a7e5d92893f2","ca45baf4c67c4a679a7300e9fb6e18d3","ae478ac2bfec4d3694f2550a6e7d28e8","1969c0b6db9c4f47b22607038a7b1a79","37822c1dea7c4066891fe9dc2bfad9de","0607fc3d5ee2416ea523bd5cce0e8c80","4b42397cb6fa4f13a52fcdfc4e7d7095","115fb8c0adf84e5687cdfecaddc7fc88","b469b0c304d84cbe9467cc76204428c5","15ff464abd8f4c98bcacb672569a8dc6","824bdcb736db49b0b01e101f3d4bbe32","dbca1f857e064aeeabac2f8eac9c9ba7","ae0bdf64c6fe474ba4b361a10d3406ab","5a2a9545b11644b6a7c683c3baa92760","54739fd4f13d4b128a24de6e70c503f3","cd6805a6b4bc490bb3dabb61504ab8e6","fed591da0d304c0f8b320ff464c5666c","2c6bd7e97ea2414aac33c456b046e873","6dd438058253463ab15a65ad8f33192f","1872502f5bbf42d1827f773b9bcd37f6","eee4ebb41d904ad7a3084540c6b2f33a","229551aa84fd4c7787adcc867eabba53","a75a543c4a414a9e98646af48b6bbe80","c77a74539b4a4ca7a56aae18c674362e","67b0ded56d4342698ab40156896dbef3","da87376fe355463386e1bcae2d6146d4","f394cc47b76c424ea50f39894d33a95e","ae139a3cd6614bdb8b13997dfa0926c7","e3067982ccdd4f4b9e25d084782b196b","b2026bb57acb43ccabb5254e5a74b4d6","5c535e651f1c4adcbaa60791742af937","25c1221c444c4c6e81dc25e8ab06143f","359a25604e64401bba227185ae1c76c6","e2efb55c88a44d24941853097ec4a60d","bcb1e5270d0b4d76ad75e7f33f15895a","18c24b0b760a4b5e8dfcbace68fd7469","f9642ec2fd524b17a5a1ce5f4f5c1ee1","64e547717c224c02981da5a4ff0462a4","810f6376fd9e4b4589f1d76bdbbba698","65b9b5ea95334da8a0519f61e9dd4c8b","3a0217664365498e81c1375d11696089","a763decf210b4bb19269b66251f8ff6f","36965743912a4bbc83a05b3c55b9e544","fcb2b70818dd4586bc146ccf99f08869","b654486cb19443ec88cba4f74f18da7b","ece2b16bff0d42e1a080df7ff743a59d","42f47dbea5d44dff9749627569b344e3","afd7b77b1b5e41b9aa344ef3d4af6eee"]},"id":"P780gFGDJfn_","executionInfo":{"status":"ok","timestamp":1738962694527,"user_tz":-120,"elapsed":570286,"user":{"displayName":"Dinu Ion George","userId":"14034259797438922081"}},"outputId":"d4957257-75fd-4827-f2ea-e6d532f9d1be"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n","Dask dataframe query planning is disabled because dask-expr is not installed.\n","\n","You can install it with `pip install dask[dataframe]` or `conda install dask`.\n","This will raise in a future version.\n","\n","  warnings.warn(msg, FutureWarning)\n","/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a76d43f7e247423d8bd0267409172ae1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76c7ede6ff5e462a8312821bad1a8202"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37822c1dea7c4066891fe9dc2bfad9de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd6805a6b4bc490bb3dabb61504ab8e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f394cc47b76c424ea50f39894d33a95e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64e547717c224c02981da5a4ff0462a4"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFMPNetModel: ['embeddings.position_ids']\n","- This IS expected if you are initializing TFMPNetModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFMPNetModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFMPNetModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMPNetModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["Loaded transformer model: sentence-transformers/all-mpnet-base-v2\n","Using GPU\n"]},{"output_type":"stream","name":"stderr","text":["Encoding problem statements: 100%|██████████| 279/279 [01:04<00:00,  4.29it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Using GPU\n"]},{"output_type":"stream","name":"stderr","text":["Encoding problem statements: 100%|██████████| 39/39 [00:08<00:00,  4.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Benchmarking estimator model: LogisticRegression\n","Benchmarking estimator model: KNeighborsClassifier\n","Benchmarking estimator model: DecisionTreeClassifier\n","Benchmarking estimator model: GaussianNB\n","Benchmarking estimator model: RandomForestClassifier\n","Benchmarking estimator model: XGBClassifier\n","Benchmarking estimator model: LGBMClassifier\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Info] Number of positive: 3400, number of negative: 5516\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048537 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 195840\n","[LightGBM] [Info] Number of data points in the train set: 8916, number of used features: 768\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.381337 -> initscore=-0.483878\n","[LightGBM] [Info] Start training from score -0.483878\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Info] Number of positive: 3334, number of negative: 5582\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024986 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 195842\n","[LightGBM] [Info] Number of data points in the train set: 8916, number of used features: 769\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.373934 -> initscore=-0.515374\n","[LightGBM] [Info] Start training from score -0.515374\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Info] Number of positive: 2998, number of negative: 5918\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022292 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 195844\n","[LightGBM] [Info] Number of data points in the train set: 8916, number of used features: 770\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.336249 -> initscore=-0.680053\n","[LightGBM] [Info] Start training from score -0.680053\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Info] Number of positive: 2390, number of negative: 6526\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022028 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 195846\n","[LightGBM] [Info] Number of data points in the train set: 8916, number of used features: 771\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.268057 -> initscore=-1.004501\n","[LightGBM] [Info] Start training from score -1.004501\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Info] Number of positive: 2024, number of negative: 6892\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020946 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 195848\n","[LightGBM] [Info] Number of data points in the train set: 8916, number of used features: 772\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.227008 -> initscore=-1.225286\n","[LightGBM] [Info] Start training from score -1.225286\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Benchmarking estimator model: SVC\n"]}]},{"cell_type":"code","source":["import os\n","\n","# local application/library specific imports\n","from app_config import AppConfig\n","\n","# define configuration proxy\n","configProxy = AppConfig()\n","CONFIG = configProxy.return_config()\n","\n","from app_src import OneVsAllDecisionTreeEvaluator\n","\n","decisionTreeEvaluator = OneVsAllDecisionTreeEvaluator()\n","decisionTreeEvaluator.benchmark_model(\n","    encoder_batch_size=32,\n","    number_of_tags=5,\n","    transformer_name='sentence-transformers/all-mpnet-base-v2'\n","    )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MNyOaKubFYPY","executionInfo":{"status":"ok","timestamp":1738956382290,"user_tz":-120,"elapsed":230748,"user":{"displayName":"Dinu Ion George","userId":"14034259797438922081"}},"outputId":"8b8ad66c-8d94-4bca-cf4c-ba4905b43071"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFMPNetModel: ['embeddings.position_ids']\n","- This IS expected if you are initializing TFMPNetModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFMPNetModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFMPNetModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMPNetModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["Loaded transformer model: sentence-transformers/all-mpnet-base-v2\n","Using GPU\n"]},{"output_type":"stream","name":"stderr","text":["Encoding problem statements: 100%|██████████| 140/140 [00:30<00:00,  4.63it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Using GPU\n"]},{"output_type":"stream","name":"stderr","text":["Encoding problem statements: 100%|██████████| 39/39 [00:08<00:00,  4.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Benchmarking estimator model: LogisticRegression\n","Benchmarking estimator model: KNeighborsClassifier\n","Benchmarking estimator model: DecisionTreeClassifier\n","Benchmarking estimator model: GaussianNB\n","Benchmarking estimator model: RandomForestClassifier\n","Benchmarking estimator model: XGBClassifier\n","Benchmarking estimator model: LGBMClassifier\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Info] Number of positive: 1700, number of negative: 2758\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014652 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 195840\n","[LightGBM] [Info] Number of data points in the train set: 4458, number of used features: 768\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.381337 -> initscore=-0.483878\n","[LightGBM] [Info] Start training from score -0.483878\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Info] Number of positive: 1667, number of negative: 2791\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012859 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 195840\n","[LightGBM] [Info] Number of data points in the train set: 4458, number of used features: 768\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.373934 -> initscore=-0.515374\n","[LightGBM] [Info] Start training from score -0.515374\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Info] Number of positive: 1499, number of negative: 2959\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021165 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 195840\n","[LightGBM] [Info] Number of data points in the train set: 4458, number of used features: 768\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.336249 -> initscore=-0.680053\n","[LightGBM] [Info] Start training from score -0.680053\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Info] Number of positive: 1195, number of negative: 3263\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012665 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 195840\n","[LightGBM] [Info] Number of data points in the train set: 4458, number of used features: 768\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.268057 -> initscore=-1.004501\n","[LightGBM] [Info] Start training from score -1.004501\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Info] Number of positive: 1012, number of negative: 3446\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012527 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 195840\n","[LightGBM] [Info] Number of data points in the train set: 4458, number of used features: 768\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.227008 -> initscore=-1.225286\n","[LightGBM] [Info] Start training from score -1.225286\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Benchmarking estimator model: SVC\n"]}]},{"cell_type":"code","source":["# local application/library specific imports\n","from app_src import OneVsAllTransformerEvaluator\n","from app_config import AppConfig\n","\n","# define configuration proxy\n","configProxy = AppConfig()\n","CONFIG = configProxy.return_config()\n","\n","transformer_evaluator = OneVsAllTransformerEvaluator()\n","transformer_evaluator.evaluate_models(\n","    epochs=1,\n","    batch_size=32,\n","    number_of_tags=5,\n","    train_model=True,\n","    threshold=0.5\n","  )\n","\n"],"metadata":{"id":"PHgqiOcBdQh_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738956594129,"user_tz":-120,"elapsed":211836,"user":{"displayName":"Dinu Ion George","userId":"14034259797438922081"}},"outputId":"e2435a46-4a1b-4fc4-b144-318752f8cbf4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training and evaluating model: sentence-transformers/all-mpnet-base-v2\n","Starting training for label: 0\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFMPNetModel: ['embeddings.position_ids']\n","- This IS expected if you are initializing TFMPNetModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFMPNetModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFMPNetModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMPNetModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - auc: 0.4729 - binary_accuracy: 0.5651 - f1: 0.2521 - loss: 0.6864 - prc_auc: 0.3631 - precision: 0.3538 - recall: 0.1969\n","Epoch 1: Validation Metrics:\n","loss: 0.686181902885437\n","val_binary_accuracy: 0.5854166746139526\n","val_precision: 0.40625\n","val_recall: 0.13903743028640747\n","val_auc: 0.46750378608703613\n","val_prc_auc: 0.38557296991348267\n","\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 117ms/step - auc: 0.4729 - binary_accuracy: 0.5651 - f1: 0.2519 - loss: 0.6864 - prc_auc: 0.3631 - precision: 0.3538 - recall: 0.1966 - val_auc: 0.4675 - val_binary_accuracy: 0.5854 - val_f1: 0.2072 - val_loss: 0.6849 - val_prc_auc: 0.3856 - val_precision: 0.4062 - val_recall: 0.1390\n","Starting training for label: 1\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFMPNetModel: ['embeddings.position_ids']\n","- This IS expected if you are initializing TFMPNetModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFMPNetModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFMPNetModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMPNetModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - auc: 0.5482 - binary_accuracy: 0.4909 - f1: 0.4986 - loss: 0.6955 - prc_auc: 0.4098 - precision: 0.3935 - recall: 0.6811\n","Epoch 1: Validation Metrics:\n","loss: 0.6955739855766296\n","val_binary_accuracy: 0.5395833253860474\n","val_precision: 0.4117647111415863\n","val_recall: 0.5474860072135925\n","val_auc: 0.5642551183700562\n","val_prc_auc: 0.4129665791988373\n","\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 112ms/step - auc: 0.5481 - binary_accuracy: 0.4909 - f1: 0.4985 - loss: 0.6955 - prc_auc: 0.4097 - precision: 0.3935 - recall: 0.6807 - val_auc: 0.5643 - val_binary_accuracy: 0.5396 - val_f1: 0.4700 - val_loss: 0.6895 - val_prc_auc: 0.4130 - val_precision: 0.4118 - val_recall: 0.5475\n","Starting training for label: 2\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFMPNetModel: ['embeddings.position_ids']\n","- This IS expected if you are initializing TFMPNetModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFMPNetModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFMPNetModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMPNetModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - auc: 0.5426 - binary_accuracy: 0.5497 - f1: 0.4158 - loss: 0.6886 - prc_auc: 0.3782 - precision: 0.3669 - recall: 0.4815\n","Epoch 1: Validation Metrics:\n","loss: 0.6873651742935181\n","val_binary_accuracy: 0.5791666507720947\n","val_precision: 0.39436620473861694\n","val_recall: 0.3255814015865326\n","val_auc: 0.5228216648101807\n","val_prc_auc: 0.38145366311073303\n","\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 113ms/step - auc: 0.5425 - binary_accuracy: 0.5498 - f1: 0.4157 - loss: 0.6886 - prc_auc: 0.3782 - precision: 0.3669 - recall: 0.4811 - val_auc: 0.5228 - val_binary_accuracy: 0.5792 - val_f1: 0.3567 - val_loss: 0.6860 - val_prc_auc: 0.3815 - val_precision: 0.3944 - val_recall: 0.3256\n","Starting training for label: 3\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFMPNetModel: ['embeddings.position_ids']\n","- This IS expected if you are initializing TFMPNetModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFMPNetModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFMPNetModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMPNetModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - auc: 0.4647 - binary_accuracy: 0.4365 - f1: 0.3634 - loss: 0.7060 - prc_auc: 0.2438 - precision: 0.2610 - recall: 0.6001\n","Epoch 1: Validation Metrics:\n","loss: 0.7018375992774963\n","val_binary_accuracy: 0.48750001192092896\n","val_precision: 0.22466960549354553\n","val_recall: 0.42148759961128235\n","val_auc: 0.4521167576313019\n","val_prc_auc: 0.21609055995941162\n","\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 113ms/step - auc: 0.4647 - binary_accuracy: 0.4367 - f1: 0.3632 - loss: 0.7060 - prc_auc: 0.2438 - precision: 0.2609 - recall: 0.5996 - val_auc: 0.4521 - val_binary_accuracy: 0.4875 - val_f1: 0.2931 - val_loss: 0.6973 - val_prc_auc: 0.2161 - val_precision: 0.2247 - val_recall: 0.4215\n","Starting training for label: 4\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFMPNetModel: ['embeddings.position_ids']\n","- This IS expected if you are initializing TFMPNetModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFMPNetModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFMPNetModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMPNetModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - auc: 0.4654 - binary_accuracy: 0.6132 - f1: 0.2177 - loss: 0.6776 - prc_auc: 0.2102 - precision: 0.2005 - recall: 0.2400\n","Epoch 1: Validation Metrics:\n","loss: 0.6737685799598694\n","val_binary_accuracy: 0.6833333373069763\n","val_precision: 0.11267605423927307\n","val_recall: 0.0824742242693901\n","val_auc: 0.46710723638534546\n","val_prc_auc: 0.177982896566391\n","\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 112ms/step - auc: 0.4654 - binary_accuracy: 0.6133 - f1: 0.2176 - loss: 0.6775 - prc_auc: 0.2101 - precision: 0.2005 - recall: 0.2397 - val_auc: 0.4671 - val_binary_accuracy: 0.6833 - val_f1: 0.0952 - val_loss: 0.6616 - val_prc_auc: 0.1780 - val_precision: 0.1127 - val_recall: 0.0825\n","\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 249ms/step\n","\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 203ms/step\n","\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 205ms/step\n","\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 204ms/step\n","\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 205ms/step\n","[[0 1 1 0 0]\n"," [0 0 1 0 0]\n"," [0 0 1 0 0]\n"," ...\n"," [0 1 0 1 0]\n"," [0 1 0 0 0]\n"," [0 1 0 1 0]]\n","[[0 0 1 0 0]\n"," [0 1 1 0 0]\n"," [0 1 1 0 0]\n"," ...\n"," [0 0 0 1 1]\n"," [0 0 1 0 0]\n"," [1 1 1 0 0]]\n","test_tags_np shape: (1239, 5)\n","predictions shape: (1239, 5)\n"]}]},{"cell_type":"code","source":["# local application/library specific imports\n","from app_src import NLITransformerWrapper\n","from app_config import AppConfig\n","\n","# define configuration proxy\n","configProxy = AppConfig()\n","CONFIG = configProxy.return_config()\n","\n","nli_transformer_wrapper = NLITransformerWrapper('sentence-transformers/all-mpnet-base-v2')\n","nli_transformer_wrapper.train_model(\n","    train_dataset_path=CONFIG[f'NLI_TRAINING_DATASET_PATH'],\n","    epochs=5,\n","    batch_size=16,\n","    transformer_model_path=os.path.join(CONFIG[\"TRANSFORMER_SAVE_PATH_ROOT\"], 'transformer_model_20250208_171427')\n","  )\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JRO_Vzk2MtcV","executionInfo":{"status":"ok","timestamp":1739037459832,"user_tz":-120,"elapsed":826419,"user":{"displayName":"Dinu Ion George","userId":"14034259797438922081"}},"outputId":"a0b534d2-5d00-40a6-e07f-5be461426146"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFMPNetModel.\n","\n","All the layers of TFMPNetModel were initialized from the model checkpoint at /content/drive/Othercomputers/ASUS Main/MLCP/05_MODELS/02_Transformer_Models/transformer_model_20250208_171427.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMPNetModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","Anchor Embeddings Shape: (16, 768)\n","Positive Embeddings Shape: (16, 768)\n","Negative Embeddings Shape: (16, 768)\n","Loss Value: Tensor(\"Mean:0\", shape=(), dtype=float32)\n","[<tf.Variable 'tfmp_net_model_5/mpnet/encoder/relative_attention_bias/embeddings:0' shape=(32, 12) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/pooler/dense/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/pooler/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/embeddings/word_embeddings/weight:0' shape=(30527, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/embeddings/position_embeddings/embeddings:0' shape=(514, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/embeddings/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/embeddings/LayerNorm/beta:0' shape=(768,) dtype=float32>]\n","Gradients: [<tensorflow.python.framework.indexed_slices.IndexedSlices object at 0x7f3b041c0d90>, <tf.Tensor 'AddN_268:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_269:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_270:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_271:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_272:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_273:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_274:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_275:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_276:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_277:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_278:0' shape=(768, 3072) dtype=float32>, <tf.Tensor 'AddN_279:0' shape=(3072,) dtype=float32>, <tf.Tensor 'AddN_280:0' shape=(3072, 768) dtype=float32>, <tf.Tensor 'AddN_281:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_282:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_283:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_284:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_285:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_286:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_287:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_288:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_289:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_290:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_291:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_292:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_293:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_294:0' shape=(768, 3072) dtype=float32>, <tf.Tensor 'AddN_295:0' shape=(3072,) dtype=float32>, <tf.Tensor 'AddN_296:0' shape=(3072, 768) dtype=float32>, <tf.Tensor 'AddN_297:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_298:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_299:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_300:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_301:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_302:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_303:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_304:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_305:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_306:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_307:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_308:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_309:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_310:0' shape=(768, 3072) dtype=float32>, <tf.Tensor 'AddN_311:0' shape=(3072,) dtype=float32>, <tf.Tensor 'AddN_312:0' shape=(3072, 768) dtype=float32>, <tf.Tensor 'AddN_313:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_314:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_315:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_316:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_317:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_318:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_319:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_320:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_321:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_322:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_323:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_324:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_325:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_326:0' shape=(768, 3072) dtype=float32>, <tf.Tensor 'AddN_327:0' shape=(3072,) dtype=float32>, <tf.Tensor 'AddN_328:0' shape=(3072, 768) dtype=float32>, <tf.Tensor 'AddN_329:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_330:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_331:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_332:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_333:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_334:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_335:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_336:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_337:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_338:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_339:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_340:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_341:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_342:0' shape=(768, 3072) dtype=float32>, <tf.Tensor 'AddN_343:0' shape=(3072,) dtype=float32>, <tf.Tensor 'AddN_344:0' shape=(3072, 768) dtype=float32>, <tf.Tensor 'AddN_345:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_346:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_347:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_348:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_349:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_350:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_351:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_352:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_353:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_354:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_355:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_356:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_357:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_358:0' shape=(768, 3072) dtype=float32>, <tf.Tensor 'AddN_359:0' shape=(3072,) dtype=float32>, <tf.Tensor 'AddN_360:0' shape=(3072, 768) dtype=float32>, <tf.Tensor 'AddN_361:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_362:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_363:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_364:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_365:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_366:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_367:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_368:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_369:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_370:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_371:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_372:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_373:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_374:0' shape=(768, 3072) dtype=float32>, <tf.Tensor 'AddN_375:0' shape=(3072,) dtype=float32>, <tf.Tensor 'AddN_376:0' shape=(3072, 768) dtype=float32>, <tf.Tensor 'AddN_377:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_378:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_379:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_380:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_381:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_382:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_383:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_384:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_385:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_386:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_387:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_388:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_389:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_390:0' shape=(768, 3072) dtype=float32>, <tf.Tensor 'AddN_391:0' shape=(3072,) dtype=float32>, <tf.Tensor 'AddN_392:0' shape=(3072, 768) dtype=float32>, <tf.Tensor 'AddN_393:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_394:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_395:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_396:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_397:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_398:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_399:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_400:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_401:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_402:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_403:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_404:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_405:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_406:0' shape=(768, 3072) dtype=float32>, <tf.Tensor 'AddN_407:0' shape=(3072,) dtype=float32>, <tf.Tensor 'AddN_408:0' shape=(3072, 768) dtype=float32>, <tf.Tensor 'AddN_409:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_410:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_411:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_412:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_413:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_414:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_415:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_416:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_417:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_418:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_419:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_420:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_421:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_422:0' shape=(768, 3072) dtype=float32>, <tf.Tensor 'AddN_423:0' shape=(3072,) dtype=float32>, <tf.Tensor 'AddN_424:0' shape=(3072, 768) dtype=float32>, <tf.Tensor 'AddN_425:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_426:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_427:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_428:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_429:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_430:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_431:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_432:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_433:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_434:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_435:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_436:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_437:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_438:0' shape=(768, 3072) dtype=float32>, <tf.Tensor 'AddN_439:0' shape=(3072,) dtype=float32>, <tf.Tensor 'AddN_440:0' shape=(3072, 768) dtype=float32>, <tf.Tensor 'AddN_441:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_442:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_443:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_444:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_445:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_446:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_447:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_448:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_449:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_450:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_451:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_452:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_453:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_454:0' shape=(768, 3072) dtype=float32>, <tf.Tensor 'AddN_455:0' shape=(3072,) dtype=float32>, <tf.Tensor 'AddN_456:0' shape=(3072, 768) dtype=float32>, <tf.Tensor 'AddN_457:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_458:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_459:0' shape=(768,) dtype=float32>, None, None, <tensorflow.python.framework.indexed_slices.IndexedSlices object at 0x7f3b7de0ce50>, <tensorflow.python.framework.indexed_slices.IndexedSlices object at 0x7f3b2493a150>, <tf.Tensor 'AddN_460:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_461:0' shape=(768,) dtype=float32>]\n","Trainable Variables: [<tf.Variable 'tfmp_net_model_5/mpnet/encoder/relative_attention_bias/embeddings:0' shape=(32, 12) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/pooler/dense/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/pooler/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/embeddings/word_embeddings/weight:0' shape=(30527, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/embeddings/position_embeddings/embeddings:0' shape=(514, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/embeddings/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/embeddings/LayerNorm/beta:0' shape=(768,) dtype=float32>]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['tfmp_net_model_5/mpnet/pooler/dense/kernel:0', 'tfmp_net_model_5/mpnet/pooler/dense/bias:0'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Anchor Embeddings Shape: (16, 768)\n","Positive Embeddings Shape: (16, 768)\n","Negative Embeddings Shape: (16, 768)\n","Loss Value: Tensor(\"Mean:0\", shape=(), dtype=float32)\n","[<tf.Variable 'tfmp_net_model_5/mpnet/encoder/relative_attention_bias/embeddings:0' shape=(32, 12) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/pooler/dense/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/pooler/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/embeddings/word_embeddings/weight:0' shape=(30527, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/embeddings/position_embeddings/embeddings:0' shape=(514, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/embeddings/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/embeddings/LayerNorm/beta:0' shape=(768,) dtype=float32>]\n","Gradients: [<tensorflow.python.framework.indexed_slices.IndexedSlices object at 0x7f3bf06f0d90>, <tf.Tensor 'AddN_268:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_269:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_270:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_271:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_272:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_273:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_274:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_275:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_276:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_277:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_278:0' shape=(768, 3072) dtype=float32>, <tf.Tensor 'AddN_279:0' shape=(3072,) dtype=float32>, <tf.Tensor 'AddN_280:0' shape=(3072, 768) dtype=float32>, <tf.Tensor 'AddN_281:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_282:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_283:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_284:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_285:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_286:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_287:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_288:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_289:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_290:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_291:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_292:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_293:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_294:0' shape=(768, 3072) dtype=float32>, <tf.Tensor 'AddN_295:0' shape=(3072,) dtype=float32>, <tf.Tensor 'AddN_296:0' shape=(3072, 768) dtype=float32>, <tf.Tensor 'AddN_297:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_298:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_299:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_300:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_301:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_302:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_303:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_304:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_305:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_306:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_307:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_308:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_309:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_310:0' shape=(768, 3072) dtype=float32>, <tf.Tensor 'AddN_311:0' shape=(3072,) dtype=float32>, <tf.Tensor 'AddN_312:0' shape=(3072, 768) dtype=float32>, <tf.Tensor 'AddN_313:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_314:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_315:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_316:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_317:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_318:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_319:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_320:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_321:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_322:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_323:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_324:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_325:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_326:0' shape=(768, 3072) dtype=float32>, <tf.Tensor 'AddN_327:0' shape=(3072,) dtype=float32>, <tf.Tensor 'AddN_328:0' shape=(3072, 768) dtype=float32>, <tf.Tensor 'AddN_329:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_330:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_331:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_332:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_333:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_334:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_335:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_336:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_337:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_338:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_339:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_340:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_341:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_342:0' shape=(768, 3072) dtype=float32>, <tf.Tensor 'AddN_343:0' shape=(3072,) dtype=float32>, <tf.Tensor 'AddN_344:0' shape=(3072, 768) dtype=float32>, <tf.Tensor 'AddN_345:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_346:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_347:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_348:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_349:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_350:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_351:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_352:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_353:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_354:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_355:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_356:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_357:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_358:0' shape=(768, 3072) dtype=float32>, <tf.Tensor 'AddN_359:0' shape=(3072,) dtype=float32>, <tf.Tensor 'AddN_360:0' shape=(3072, 768) dtype=float32>, <tf.Tensor 'AddN_361:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_362:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_363:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_364:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_365:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_366:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_367:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_368:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_369:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_370:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_371:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_372:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_373:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_374:0' shape=(768, 3072) dtype=float32>, <tf.Tensor 'AddN_375:0' shape=(3072,) dtype=float32>, <tf.Tensor 'AddN_376:0' shape=(3072, 768) dtype=float32>, <tf.Tensor 'AddN_377:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_378:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_379:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_380:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_381:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_382:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_383:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_384:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_385:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_386:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_387:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_388:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_389:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_390:0' shape=(768, 3072) dtype=float32>, <tf.Tensor 'AddN_391:0' shape=(3072,) dtype=float32>, <tf.Tensor 'AddN_392:0' shape=(3072, 768) dtype=float32>, <tf.Tensor 'AddN_393:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_394:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_395:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_396:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_397:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_398:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_399:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_400:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_401:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_402:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_403:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_404:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_405:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_406:0' shape=(768, 3072) dtype=float32>, <tf.Tensor 'AddN_407:0' shape=(3072,) dtype=float32>, <tf.Tensor 'AddN_408:0' shape=(3072, 768) dtype=float32>, <tf.Tensor 'AddN_409:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_410:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_411:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_412:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_413:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_414:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_415:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_416:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_417:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_418:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_419:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_420:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_421:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_422:0' shape=(768, 3072) dtype=float32>, <tf.Tensor 'AddN_423:0' shape=(3072,) dtype=float32>, <tf.Tensor 'AddN_424:0' shape=(3072, 768) dtype=float32>, <tf.Tensor 'AddN_425:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_426:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_427:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_428:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_429:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_430:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_431:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_432:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_433:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_434:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_435:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_436:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_437:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_438:0' shape=(768, 3072) dtype=float32>, <tf.Tensor 'AddN_439:0' shape=(3072,) dtype=float32>, <tf.Tensor 'AddN_440:0' shape=(3072, 768) dtype=float32>, <tf.Tensor 'AddN_441:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_442:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_443:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_444:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_445:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_446:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_447:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_448:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_449:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_450:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_451:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_452:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_453:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_454:0' shape=(768, 3072) dtype=float32>, <tf.Tensor 'AddN_455:0' shape=(3072,) dtype=float32>, <tf.Tensor 'AddN_456:0' shape=(3072, 768) dtype=float32>, <tf.Tensor 'AddN_457:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_458:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_459:0' shape=(768,) dtype=float32>, None, None, <tensorflow.python.framework.indexed_slices.IndexedSlices object at 0x7f3b2458b1d0>, <tensorflow.python.framework.indexed_slices.IndexedSlices object at 0x7f3b16d52dd0>, <tf.Tensor 'AddN_460:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_461:0' shape=(768,) dtype=float32>]\n","Trainable Variables: [<tf.Variable 'tfmp_net_model_5/mpnet/encoder/relative_attention_bias/embeddings:0' shape=(32, 12) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._0/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._1/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._2/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._3/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._4/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._5/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._6/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._7/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._8/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._9/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._10/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/encoder/layer_._11/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/pooler/dense/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/pooler/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/embeddings/word_embeddings/weight:0' shape=(30527, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/embeddings/position_embeddings/embeddings:0' shape=(514, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/embeddings/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_5/mpnet/embeddings/LayerNorm/beta:0' shape=(768,) dtype=float32>]\n","\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 496ms/step - loss: 0.0255\n","Epoch 2/5\n","\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 496ms/step - loss: 0.0232\n","Epoch 3/5\n","\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 496ms/step - loss: 0.0183\n","Epoch 4/5\n","\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 496ms/step - loss: 0.0163\n","Epoch 5/5\n","\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 496ms/step - loss: 0.0131\n","Transformer model saved to /content/drive/Othercomputers/ASUS Main/MLCP/05_MODELS/02_Transformer_Models/transformer_model_20250208_171427\n"]}]},{"cell_type":"code","source":["import os\n","\n","# local application/library specific imports\n","from app_src import SentenceTransformerWrapper\n","from app_config import AppConfig\n","\n","# define configuration proxy\n","configProxy = AppConfig()\n","CONFIG = configProxy.return_config()\n","\n","from app_src import DecisionTreeEvaluator\n","\n","decisionTreeEvaluator = DecisionTreeEvaluator()\n","decisionTreeEvaluator.benchmark_model(\n","    encoder_batch_size=32,\n","    number_of_tags=5,\n","    transformer_name='sentence-transformers/all-mpnet-base-v2',\n","    transformer_model_path=os.path.join(CONFIG[\"TRANSFORMER_SAVE_PATH_ROOT\"], 'transformer_model_20250208_171427_20_epochs')\n","    )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZxccKR9-LaVM","outputId":"f5ea978d-5c3e-42aa-821e-d033897f456a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFMPNetModel.\n","\n","All the layers of TFMPNetModel were initialized from the model checkpoint at /content/drive/Othercomputers/ASUS Main/MLCP/05_MODELS/02_Transformer_Models/transformer_model_20250208_171427_20_epochs.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMPNetModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["Loaded transformer model from: /content/drive/Othercomputers/ASUS Main/MLCP/05_MODELS/02_Transformer_Models/transformer_model_20250208_171427_20_epochs\n","Using GPU\n"]},{"output_type":"stream","name":"stderr","text":["Encoding problem statements:  12%|█▏        | 17/140 [00:05<00:27,  4.52it/s]"]}]},{"cell_type":"code","source":["# local application/library specific imports\n","from app_src import OneVsAllTransformerEvaluator\n","from app_config import AppConfig\n","\n","# define configuration proxy\n","configProxy = AppConfig()\n","CONFIG = configProxy.return_config()\n","\n","transformer_evaluator = OneVsAllTransformerEvaluator()\n","transformer_evaluator.evaluate_models(\n","    epochs=15,\n","    batch_size=32,\n","    number_of_tags=5,\n","    train_model=True,\n","    threshold=0.5,\n","    transformer_model_path=os.path.join(CONFIG[\"TRANSFORMER_SAVE_PATH_ROOT\"], 'transformer_model_20250208_154459_1_epochs')\n","  )\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["ba398e7612f94d05bf7e8ab6bba737a9","16ae265f83cf49359cf247b140876d43","3122e14fb24642359a3b428a1713e243","df0f42e9ef944cbc88a9259550b140e1","248c5df3318b4fb9b8796274b3349919","0ea051a59d0d4612b36d4e85b60f6700","2f05d6fda3f548f3a4423764e0b347b1","18705f011fe149819410f429e212a580","aff81471b4ab4e6bbfcd271dfa1987c3","a0b079a8b4e04eeba17e22b47dbd697a","21ac481d907444239b10afc2e45b6a40","0c9c57424f47461b93c1a19f53195da0","e6cae83c079e464985e19cfefc812293","589bd6a7ac904448ab6dcfbe2f7c80a7","412556a73a984907897d680a3dd18115","c50648adaaad4ee3a80866f60a720b16","040060ab909340c3b51d4cf6a8a61700","c95f124715fd482db88bd5241ea957ea","456027c9130b4732a5a0858591af9376","01e636c9ad4d4942b9688f5aa6900004","0c4985118b8144f59546413169200453","51cc0c6348ab4394858bea3d192ecc53","248725129d6d498db9eeac1d6635dfba","396d3176dd7e4ebfaeec9567105e0688","bb59ae35548e408eb864e77c4a600c4f","030e31fb9dc44971892989c83fdb3573","9f3307e969bb4f2d812a99b9fea4f662","633e162edc224366ab877efc695d1dd8","9c11ef29a48b441499a369e298874184","1f7e0eaca8eb47fa9240c82ed620f8d6","a47f2d2dedb5484dbf657d55807569cf","37c7b58ab0dd41d9846237489975bd49","cfc07547e2614469ac2ea65c112575e3","3288dbb820d446c182178be2591407fb","75f3389b5cb945b89f05a3d831b5ee14","15f8b80d0a8b4d898cb34b6b7e953b0e","82f69a30662b41aaa3fbfea3ca86ca3e","48dd7586eac1408bbacc44f6893c1626","c6aa8b0aa94f431f8105ca62a1aebf1b","5754014ae09944dea10fb5c0f5c3739d","ac5debef42e141a5a4ba62aca46793a7","4e2c8efcc0e34a8cb4f44d67dade470b","1dea0db52330436bb2db163fe768b17d","775bb7897bd64ceab13b09bec7e46bc0"]},"id":"E67PUmrfEy0H","executionInfo":{"status":"ok","timestamp":1739125837309,"user_tz":-120,"elapsed":725836,"user":{"displayName":"Dinu Ion George","userId":"14034259797438922081"}},"outputId":"af2f189c-bf0b-4468-a378-2bfacd4edff7"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n","Dask dataframe query planning is disabled because dask-expr is not installed.\n","\n","You can install it with `pip install dask[dataframe]` or `conda install dask`.\n","This will raise in a future version.\n","\n","  warnings.warn(msg, FutureWarning)\n"]},{"output_type":"stream","name":"stdout","text":["Training and evaluating model: sentence-transformers/all-mpnet-base-v2\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba398e7612f94d05bf7e8ab6bba737a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c9c57424f47461b93c1a19f53195da0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"248725129d6d498db9eeac1d6635dfba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3288dbb820d446c182178be2591407fb"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Starting training for label: 0\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFMPNetModel.\n","\n","All the layers of TFMPNetModel were initialized from the model checkpoint at /content/drive/Othercomputers/ASUS Main/MLCP/05_MODELS/02_Transformer_Models/transformer_model_20250208_154459_1_epochs.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMPNetModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/15\n","\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - auc: 0.4403 - binary_accuracy: 0.4460 - f1: 0.1468 - loss: 0.7008 - prc_auc: 0.4607 - precision: 0.3553 - recall: 0.0932\n","Epoch 1: Validation Metrics:\n","loss: 0.699674665927887\n","val_binary_accuracy: 0.4838709533214569\n","val_precision: 0.22123894095420837\n","val_recall: 0.12953367829322815\n","val_auc: 0.40918105840682983\n","val_prc_auc: 0.3282744586467743\n","\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 293ms/step - auc: 0.4403 - binary_accuracy: 0.4460 - f1: 0.1470 - loss: 0.7008 - prc_auc: 0.4606 - precision: 0.3551 - recall: 0.0933 - val_auc: 0.4092 - val_binary_accuracy: 0.4839 - val_f1: 0.1634 - val_loss: 0.6927 - val_prc_auc: 0.3283 - val_precision: 0.2212 - val_recall: 0.1295\n","Epoch 2/15\n","\u001b[1m106/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.4553 - binary_accuracy: 0.4509 - f1: 0.2524 - loss: 0.6980 - prc_auc: 0.4755 - precision: 0.4198 - recall: 0.1809\n","Epoch 2: Validation Metrics:\n","loss: 0.6975067257881165\n","val_binary_accuracy: 0.4657258093357086\n","val_precision: 0.26923078298568726\n","val_recall: 0.21761657297611237\n","val_auc: 0.4202021360397339\n","val_prc_auc: 0.3442111015319824\n","\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 83ms/step - auc: 0.4552 - binary_accuracy: 0.4509 - f1: 0.2525 - loss: 0.6980 - prc_auc: 0.4751 - precision: 0.4194 - recall: 0.1811 - val_auc: 0.4202 - val_binary_accuracy: 0.4657 - val_f1: 0.2407 - val_loss: 0.6940 - val_prc_auc: 0.3442 - val_precision: 0.2692 - val_recall: 0.2176\n","Epoch 3/15\n","\u001b[1m106/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.4739 - binary_accuracy: 0.4665 - f1: 0.3590 - loss: 0.6960 - prc_auc: 0.4934 - precision: 0.4690 - recall: 0.2916\n","Epoch 3: Validation Metrics:\n","loss: 0.6958339810371399\n","val_binary_accuracy: 0.4375\n","val_precision: 0.2806122303009033\n","val_recall: 0.2849740982055664\n","val_auc: 0.43466031551361084\n","val_prc_auc: 0.35619378089904785\n","\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 82ms/step - auc: 0.4738 - binary_accuracy: 0.4666 - f1: 0.3592 - loss: 0.6960 - prc_auc: 0.4930 - precision: 0.4687 - recall: 0.2920 - val_auc: 0.4347 - val_binary_accuracy: 0.4375 - val_f1: 0.2828 - val_loss: 0.6949 - val_prc_auc: 0.3562 - val_precision: 0.2806 - val_recall: 0.2850\n","Epoch 4/15\n","\u001b[1m106/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.4935 - binary_accuracy: 0.4818 - f1: 0.4449 - loss: 0.6943 - prc_auc: 0.5078 - precision: 0.4947 - recall: 0.4051\n","Epoch 4: Validation Metrics:\n","loss: 0.6944711208343506\n","val_binary_accuracy: 0.4536290466785431\n","val_precision: 0.33043476939201355\n","val_recall: 0.393782377243042\n","val_auc: 0.46025922894477844\n","val_prc_auc: 0.3714736998081207\n","\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 82ms/step - auc: 0.4934 - binary_accuracy: 0.4818 - f1: 0.4449 - loss: 0.6943 - prc_auc: 0.5075 - precision: 0.4944 - recall: 0.4054 - val_auc: 0.4603 - val_binary_accuracy: 0.4536 - val_f1: 0.3593 - val_loss: 0.6954 - val_prc_auc: 0.3715 - val_precision: 0.3304 - val_recall: 0.3938\n","Epoch 5/15\n","\u001b[1m106/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.5131 - binary_accuracy: 0.5110 - f1: 0.5096 - loss: 0.6929 - prc_auc: 0.5252 - precision: 0.5255 - recall: 0.4957\n","Epoch 5: Validation Metrics:\n","loss: 0.693306028842926\n","val_binary_accuracy: 0.4959677457809448\n","val_precision: 0.38823530077934265\n","val_recall: 0.5129533410072327\n","val_auc: 0.4842678904533386\n","val_prc_auc: 0.38500311970710754\n","\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 83ms/step - auc: 0.5130 - binary_accuracy: 0.5109 - f1: 0.5096 - loss: 0.6930 - prc_auc: 0.5248 - precision: 0.5251 - recall: 0.4959 - val_auc: 0.4843 - val_binary_accuracy: 0.4960 - val_f1: 0.4420 - val_loss: 0.6956 - val_prc_auc: 0.3850 - val_precision: 0.3882 - val_recall: 0.5130\n","Epoch 6/15\n","\u001b[1m106/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.5334 - binary_accuracy: 0.5226 - f1: 0.5482 - loss: 0.6918 - prc_auc: 0.5393 - precision: 0.5331 - recall: 0.5649\n","Epoch 6: Validation Metrics:\n","loss: 0.6922717094421387\n","val_binary_accuracy: 0.5141128897666931\n","val_precision: 0.40977442264556885\n","val_recall: 0.5647668242454529\n","val_auc: 0.5077207088470459\n","val_prc_auc: 0.39704054594039917\n","\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 83ms/step - auc: 0.5333 - binary_accuracy: 0.5225 - f1: 0.5481 - loss: 0.6918 - prc_auc: 0.5390 - precision: 0.5328 - recall: 0.5650 - val_auc: 0.5077 - val_binary_accuracy: 0.5141 - val_f1: 0.4749 - val_loss: 0.6955 - val_prc_auc: 0.3970 - val_precision: 0.4098 - val_recall: 0.5648\n","Epoch 7/15\n","\u001b[1m106/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.5488 - binary_accuracy: 0.5355 - f1: 0.5739 - loss: 0.6907 - prc_auc: 0.5487 - precision: 0.5424 - recall: 0.6099\n","Epoch 7: Validation Metrics:\n","loss: 0.6913281083106995\n","val_binary_accuracy: 0.5141128897666931\n","val_precision: 0.4124087691307068\n","val_recall: 0.5854922533035278\n","val_auc: 0.5217684507369995\n","val_prc_auc: 0.4098568558692932\n","\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 83ms/step - auc: 0.5487 - binary_accuracy: 0.5354 - f1: 0.5737 - loss: 0.6907 - prc_auc: 0.5483 - precision: 0.5420 - recall: 0.6098 - val_auc: 0.5218 - val_binary_accuracy: 0.5141 - val_f1: 0.4839 - val_loss: 0.6953 - val_prc_auc: 0.4099 - val_precision: 0.4124 - val_recall: 0.5855\n","Epoch 8/15\n","\u001b[1m106/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.5616 - binary_accuracy: 0.5479 - f1: 0.5915 - loss: 0.6898 - prc_auc: 0.5564 - precision: 0.5516 - recall: 0.6380\n","Epoch 8: Validation Metrics:\n","loss: 0.6904510855674744\n","val_binary_accuracy: 0.5262096524238586\n","val_precision: 0.4227941036224365\n","val_recall: 0.5958549380302429\n","val_auc: 0.5333281755447388\n","val_prc_auc: 0.4154067039489746\n","\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 83ms/step - auc: 0.5615 - binary_accuracy: 0.5478 - f1: 0.5913 - loss: 0.6898 - prc_auc: 0.5560 - precision: 0.5513 - recall: 0.6379 - val_auc: 0.5333 - val_binary_accuracy: 0.5262 - val_f1: 0.4946 - val_loss: 0.6949 - val_prc_auc: 0.4154 - val_precision: 0.4228 - val_recall: 0.5959\n","Epoch 9/15\n","\u001b[1m106/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.5728 - binary_accuracy: 0.5622 - f1: 0.6093 - loss: 0.6889 - prc_auc: 0.5627 - precision: 0.5622 - recall: 0.6653\n","Epoch 9: Validation Metrics:\n","loss: 0.6896253228187561\n","val_binary_accuracy: 0.538306474685669\n","val_precision: 0.43478259444236755\n","val_recall: 0.621761679649353\n","val_auc: 0.5399288535118103\n","val_prc_auc: 0.4152604639530182\n","\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 83ms/step - auc: 0.5727 - binary_accuracy: 0.5621 - f1: 0.6090 - loss: 0.6889 - prc_auc: 0.5623 - precision: 0.5619 - recall: 0.6652 - val_auc: 0.5399 - val_binary_accuracy: 0.5383 - val_f1: 0.5117 - val_loss: 0.6944 - val_prc_auc: 0.4153 - val_precision: 0.4348 - val_recall: 0.6218\n","Epoch 10/15\n","\u001b[1m106/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.5774 - binary_accuracy: 0.5743 - f1: 0.6224 - loss: 0.6881 - prc_auc: 0.5679 - precision: 0.5712 - recall: 0.6840\n","Epoch 10: Validation Metrics:\n","loss: 0.6888416409492493\n","val_binary_accuracy: 0.5564516186714172\n","val_precision: 0.4509090781211853\n","val_recall: 0.6424870491027832\n","val_auc: 0.5526342988014221\n","val_prc_auc: 0.4236639440059662\n","\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 83ms/step - auc: 0.5774 - binary_accuracy: 0.5741 - f1: 0.6221 - loss: 0.6881 - prc_auc: 0.5675 - precision: 0.5708 - recall: 0.6837 - val_auc: 0.5526 - val_binary_accuracy: 0.5565 - val_f1: 0.5299 - val_loss: 0.6939 - val_prc_auc: 0.4237 - val_precision: 0.4509 - val_recall: 0.6425\n","Epoch 11/15\n","\u001b[1m106/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.5838 - binary_accuracy: 0.5785 - f1: 0.6286 - loss: 0.6874 - prc_auc: 0.5697 - precision: 0.5736 - recall: 0.6956\n","Epoch 11: Validation Metrics:\n","loss: 0.688092052936554\n","val_binary_accuracy: 0.5483871102333069\n","val_precision: 0.44483986496925354\n","val_recall: 0.6476684212684631\n","val_auc: 0.5586962699890137\n","val_prc_auc: 0.42414963245391846\n","\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 82ms/step - auc: 0.5838 - binary_accuracy: 0.5782 - f1: 0.6283 - loss: 0.6874 - prc_auc: 0.5693 - precision: 0.5732 - recall: 0.6954 - val_auc: 0.5587 - val_binary_accuracy: 0.5484 - val_f1: 0.5274 - val_loss: 0.6934 - val_prc_auc: 0.4241 - val_precision: 0.4448 - val_recall: 0.6477\n","Epoch 12/15\n","\u001b[1m106/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.5883 - binary_accuracy: 0.5801 - f1: 0.6318 - loss: 0.6866 - prc_auc: 0.5749 - precision: 0.5744 - recall: 0.7023\n","Epoch 12: Validation Metrics:\n","loss: 0.6873722076416016\n","val_binary_accuracy: 0.5463709831237793\n","val_precision: 0.44285714626312256\n","val_recall: 0.6424870491027832\n","val_auc: 0.5665709376335144\n","val_prc_auc: 0.43145617842674255\n","\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 82ms/step - auc: 0.5882 - binary_accuracy: 0.5799 - f1: 0.6315 - loss: 0.6867 - prc_auc: 0.5745 - precision: 0.5740 - recall: 0.7021 - val_auc: 0.5666 - val_binary_accuracy: 0.5464 - val_f1: 0.5243 - val_loss: 0.6928 - val_prc_auc: 0.4315 - val_precision: 0.4429 - val_recall: 0.6425\n","Epoch 13/15\n","\u001b[1m106/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.5922 - binary_accuracy: 0.5798 - f1: 0.6313 - loss: 0.6859 - prc_auc: 0.5764 - precision: 0.5742 - recall: 0.7013\n","Epoch 13: Validation Metrics:\n","loss: 0.6866781115531921\n","val_binary_accuracy: 0.538306474685669\n","val_precision: 0.43661972880363464\n","val_recall: 0.6424870491027832\n","val_auc: 0.5674772262573242\n","val_prc_auc: 0.4313955307006836\n","\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 82ms/step - auc: 0.5922 - binary_accuracy: 0.5796 - f1: 0.6310 - loss: 0.6860 - prc_auc: 0.5759 - precision: 0.5738 - recall: 0.7012 - val_auc: 0.5675 - val_binary_accuracy: 0.5383 - val_f1: 0.5199 - val_loss: 0.6922 - val_prc_auc: 0.4314 - val_precision: 0.4366 - val_recall: 0.6425\n","Starting training for label: 1\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFMPNetModel.\n","\n","All the layers of TFMPNetModel were initialized from the model checkpoint at /content/drive/Othercomputers/ASUS Main/MLCP/05_MODELS/02_Transformer_Models/transformer_model_20250208_154459_1_epochs.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMPNetModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/15\n","\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - auc: 0.3915 - binary_accuracy: 0.5075 - f1: 0.0022 - loss: 0.7010 - prc_auc: 0.4233 - precision: 0.3241 - recall: 0.0011\n","Epoch 1: Validation Metrics:\n","loss: 0.6994696855545044\n","val_binary_accuracy: 0.6270161271095276\n","val_precision: 0.0\n","val_recall: 0.0\n","val_auc: 0.562127411365509\n","val_prc_auc: 0.4149266481399536\n","\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 256ms/step - auc: 0.3920 - binary_accuracy: 0.5074 - f1: 0.0022 - loss: 0.7010 - prc_auc: 0.4237 - precision: 0.3274 - recall: 0.0011 - val_auc: 0.5621 - val_binary_accuracy: 0.6270 - val_f1: 0.0000e+00 - val_loss: 0.6744 - val_prc_auc: 0.4149 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n","Epoch 2/15\n","\u001b[1m104/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.5919 - binary_accuracy: 0.5078 - f1: 0.0079 - loss: 0.6907 - prc_auc: 0.5768 - precision: 0.4400 - recall: 0.0040\n","Epoch 2: Validation Metrics:\n","loss: 0.6895043849945068\n","val_binary_accuracy: 0.6471773982048035\n","val_precision: 0.8125\n","val_recall: 0.07027027010917664\n","val_auc: 0.6608759760856628\n","val_prc_auc: 0.5658126473426819\n","\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 83ms/step - auc: 0.5925 - binary_accuracy: 0.5077 - f1: 0.0081 - loss: 0.6907 - prc_auc: 0.5776 - precision: 0.4450 - recall: 0.0041 - val_auc: 0.6609 - val_binary_accuracy: 0.6472 - val_f1: 0.1294 - val_loss: 0.6700 - val_prc_auc: 0.5658 - val_precision: 0.8125 - val_recall: 0.0703\n","Epoch 3/15\n","\u001b[1m104/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.7018 - binary_accuracy: 0.5530 - f1: 0.1975 - loss: 0.6812 - prc_auc: 0.6909 - precision: 0.8061 - recall: 0.1141\n","Epoch 3: Validation Metrics:\n","loss: 0.680346667766571\n","val_binary_accuracy: 0.7036290168762207\n","val_precision: 0.7021276354789734\n","val_recall: 0.3567567467689514\n","val_auc: 0.6904666423797607\n","val_prc_auc: 0.6122245192527771\n","\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 83ms/step - auc: 0.7019 - binary_accuracy: 0.5534 - f1: 0.1994 - loss: 0.6812 - prc_auc: 0.6912 - precision: 0.8066 - recall: 0.1153 - val_auc: 0.6905 - val_binary_accuracy: 0.7036 - val_f1: 0.4731 - val_loss: 0.6658 - val_prc_auc: 0.6122 - val_precision: 0.7021 - val_recall: 0.3568\n","Epoch 4/15\n","\u001b[1m104/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.7407 - binary_accuracy: 0.6373 - f1: 0.4963 - loss: 0.6724 - prc_auc: 0.7298 - precision: 0.7785 - recall: 0.3652\n","Epoch 4: Validation Metrics:\n","loss: 0.6719467043876648\n","val_binary_accuracy: 0.6955645084381104\n","val_precision: 0.6197183132171631\n","val_recall: 0.47567567229270935\n","val_auc: 0.7002173066139221\n","val_prc_auc: 0.6301008462905884\n","\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 83ms/step - auc: 0.7406 - binary_accuracy: 0.6374 - f1: 0.4969 - loss: 0.6724 - prc_auc: 0.7299 - precision: 0.7786 - recall: 0.3659 - val_auc: 0.7002 - val_binary_accuracy: 0.6956 - val_f1: 0.5382 - val_loss: 0.6617 - val_prc_auc: 0.6301 - val_precision: 0.6197 - val_recall: 0.4757\n","Epoch 5/15\n","\u001b[1m104/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.7580 - binary_accuracy: 0.6788 - f1: 0.6048 - loss: 0.6642 - prc_auc: 0.7450 - precision: 0.7678 - recall: 0.5010\n","Epoch 5: Validation Metrics:\n","loss: 0.664222002029419\n","val_binary_accuracy: 0.6935483813285828\n","val_precision: 0.6012269854545593\n","val_recall: 0.5297297239303589\n","val_auc: 0.7051968574523926\n","val_prc_auc: 0.6364639401435852\n","\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 83ms/step - auc: 0.7578 - binary_accuracy: 0.6787 - f1: 0.6048 - loss: 0.6642 - prc_auc: 0.7450 - precision: 0.7678 - recall: 0.5010 - val_auc: 0.7052 - val_binary_accuracy: 0.6935 - val_f1: 0.5632 - val_loss: 0.6577 - val_prc_auc: 0.6365 - val_precision: 0.6012 - val_recall: 0.5297\n","Epoch 6/15\n","\u001b[1m104/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.7674 - binary_accuracy: 0.7007 - f1: 0.6512 - loss: 0.6567 - prc_auc: 0.7554 - precision: 0.7666 - recall: 0.5695\n","Epoch 6: Validation Metrics:\n","loss: 0.6571022868156433\n","val_binary_accuracy: 0.6875\n","val_precision: 0.5892857313156128\n","val_recall: 0.5351351499557495\n","val_auc: 0.7092639207839966\n","val_prc_auc: 0.6372381448745728\n","\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 83ms/step - auc: 0.7671 - binary_accuracy: 0.7005 - f1: 0.6509 - loss: 0.6567 - prc_auc: 0.7554 - precision: 0.7664 - recall: 0.5692 - val_auc: 0.7093 - val_binary_accuracy: 0.6875 - val_f1: 0.5609 - val_loss: 0.6539 - val_prc_auc: 0.6372 - val_precision: 0.5893 - val_recall: 0.5351\n","Epoch 7/15\n","\u001b[1m104/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.7725 - binary_accuracy: 0.7103 - f1: 0.6722 - loss: 0.6497 - prc_auc: 0.7602 - precision: 0.7618 - recall: 0.6055\n","Epoch 7: Validation Metrics:\n","loss: 0.6505306363105774\n","val_binary_accuracy: 0.6935483813285828\n","val_precision: 0.5953757166862488\n","val_recall: 0.5567567348480225\n","val_auc: 0.7095854878425598\n","val_prc_auc: 0.6369649171829224\n","\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 83ms/step - auc: 0.7722 - binary_accuracy: 0.7100 - f1: 0.6719 - loss: 0.6497 - prc_auc: 0.7601 - precision: 0.7616 - recall: 0.6051 - val_auc: 0.7096 - val_binary_accuracy: 0.6935 - val_f1: 0.5754 - val_loss: 0.6503 - val_prc_auc: 0.6370 - val_precision: 0.5954 - val_recall: 0.5568\n","Epoch 8/15\n","\u001b[1m104/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.7758 - binary_accuracy: 0.7166 - f1: 0.6852 - loss: 0.6431 - prc_auc: 0.7642 - precision: 0.7581 - recall: 0.6288\n","Epoch 8: Validation Metrics:\n","loss: 0.6444594264030457\n","val_binary_accuracy: 0.6854838728904724\n","val_precision: 0.580110490322113\n","val_recall: 0.5675675868988037\n","val_auc: 0.7108107805252075\n","val_prc_auc: 0.6352725028991699\n","\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 83ms/step - auc: 0.7755 - binary_accuracy: 0.7163 - f1: 0.6849 - loss: 0.6432 - prc_auc: 0.7641 - precision: 0.7579 - recall: 0.6283 - val_auc: 0.7108 - val_binary_accuracy: 0.6855 - val_f1: 0.5738 - val_loss: 0.6470 - val_prc_auc: 0.6353 - val_precision: 0.5801 - val_recall: 0.5676\n","Epoch 9/15\n","\u001b[1m104/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.7786 - binary_accuracy: 0.7188 - f1: 0.6907 - loss: 0.6371 - prc_auc: 0.7668 - precision: 0.7550 - recall: 0.6401\n","Epoch 9: Validation Metrics:\n","loss: 0.6388477683067322\n","val_binary_accuracy: 0.6733871102333069\n","val_precision: 0.5614973306655884\n","val_recall: 0.5675675868988037\n","val_auc: 0.7116712331771851\n","val_prc_auc: 0.6382310390472412\n","\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 83ms/step - auc: 0.7783 - binary_accuracy: 0.7185 - f1: 0.6904 - loss: 0.6371 - prc_auc: 0.7667 - precision: 0.7548 - recall: 0.6396 - val_auc: 0.7117 - val_binary_accuracy: 0.6734 - val_f1: 0.5645 - val_loss: 0.6438 - val_prc_auc: 0.6382 - val_precision: 0.5615 - val_recall: 0.5676\n","Epoch 10/15\n","\u001b[1m104/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.7797 - binary_accuracy: 0.7198 - f1: 0.6945 - loss: 0.6315 - prc_auc: 0.7677 - precision: 0.7511 - recall: 0.6492\n","Epoch 10: Validation Metrics:\n","loss: 0.6336600184440613\n","val_binary_accuracy: 0.6754032373428345\n","val_precision: 0.561855673789978\n","val_recall: 0.5891891717910767\n","val_auc: 0.7133136987686157\n","val_prc_auc: 0.6406967043876648\n","\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 83ms/step - auc: 0.7794 - binary_accuracy: 0.7195 - f1: 0.6941 - loss: 0.6315 - prc_auc: 0.7677 - precision: 0.7510 - recall: 0.6487 - val_auc: 0.7133 - val_binary_accuracy: 0.6754 - val_f1: 0.5752 - val_loss: 0.6409 - val_prc_auc: 0.6407 - val_precision: 0.5619 - val_recall: 0.5892\n","Starting training for label: 2\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFMPNetModel.\n","\n","All the layers of TFMPNetModel were initialized from the model checkpoint at /content/drive/Othercomputers/ASUS Main/MLCP/05_MODELS/02_Transformer_Models/transformer_model_20250208_154459_1_epochs.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMPNetModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/15\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - auc: 0.5469 - binary_accuracy: 0.4962 - f1: 0.6307 - loss: 0.6941 - prc_auc: 0.5159 - precision: 0.4851 - recall: 0.9017\n","Epoch 1: Validation Metrics:\n","loss: 0.6917527318000793\n","val_binary_accuracy: 0.4072580635547638\n","val_precision: 0.3606557250022888\n","val_recall: 0.8799999952316284\n","val_auc: 0.5642812252044678\n","val_prc_auc: 0.40767723321914673\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 254ms/step - auc: 0.5468 - binary_accuracy: 0.4964 - f1: 0.6309 - loss: 0.6941 - prc_auc: 0.5161 - precision: 0.4854 - recall: 0.9017 - val_auc: 0.5643 - val_binary_accuracy: 0.4073 - val_f1: 0.5116 - val_loss: 0.7020 - val_prc_auc: 0.4077 - val_precision: 0.3607 - val_recall: 0.8800\n","Epoch 2/15\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.5646 - binary_accuracy: 0.5155 - f1: 0.6333 - loss: 0.6922 - prc_auc: 0.5248 - precision: 0.4960 - recall: 0.8769\n","Epoch 2: Validation Metrics:\n","loss: 0.6903735995292664\n","val_binary_accuracy: 0.4495967626571655\n","val_precision: 0.37435898184776306\n","val_recall: 0.8342857360839844\n","val_auc: 0.5830262899398804\n","val_prc_auc: 0.4168289303779602\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 85ms/step - auc: 0.5646 - binary_accuracy: 0.5157 - f1: 0.6335 - loss: 0.6922 - prc_auc: 0.5251 - precision: 0.4962 - recall: 0.8768 - val_auc: 0.5830 - val_binary_accuracy: 0.4496 - val_f1: 0.5168 - val_loss: 0.6989 - val_prc_auc: 0.4168 - val_precision: 0.3744 - val_recall: 0.8343\n","Epoch 3/15\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.5804 - binary_accuracy: 0.5380 - f1: 0.6346 - loss: 0.6906 - prc_auc: 0.5354 - precision: 0.5099 - recall: 0.8408\n","Epoch 3: Validation Metrics:\n","loss: 0.6891207098960876\n","val_binary_accuracy: 0.4858871102333069\n","val_precision: 0.3882681429386139\n","val_recall: 0.7942857146263123\n","val_auc: 0.5891499519348145\n","val_prc_auc: 0.4217577278614044\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 85ms/step - auc: 0.5803 - binary_accuracy: 0.5382 - f1: 0.6348 - loss: 0.6906 - prc_auc: 0.5356 - precision: 0.5102 - recall: 0.8407 - val_auc: 0.5891 - val_binary_accuracy: 0.4859 - val_f1: 0.5216 - val_loss: 0.6963 - val_prc_auc: 0.4218 - val_precision: 0.3883 - val_recall: 0.7943\n","Epoch 4/15\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.5890 - binary_accuracy: 0.5630 - f1: 0.6394 - loss: 0.6892 - prc_auc: 0.5408 - precision: 0.5276 - recall: 0.8120\n","Epoch 4: Validation Metrics:\n","loss: 0.6879667043685913\n","val_binary_accuracy: 0.5120967626571655\n","val_precision: 0.4000000059604645\n","val_recall: 0.7657142877578735\n","val_auc: 0.6004361510276794\n","val_prc_auc: 0.42408934235572815\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 85ms/step - auc: 0.5890 - binary_accuracy: 0.5630 - f1: 0.6394 - loss: 0.6891 - prc_auc: 0.5411 - precision: 0.5278 - recall: 0.8118 - val_auc: 0.6004 - val_binary_accuracy: 0.5121 - val_f1: 0.5255 - val_loss: 0.6941 - val_prc_auc: 0.4241 - val_precision: 0.4000 - val_recall: 0.7657\n","Epoch 5/15\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.5950 - binary_accuracy: 0.5671 - f1: 0.6345 - loss: 0.6878 - prc_auc: 0.5439 - precision: 0.5317 - recall: 0.7875\n","Epoch 5: Validation Metrics:\n","loss: 0.6868970394134521\n","val_binary_accuracy: 0.5342742204666138\n","val_precision: 0.41304346919059753\n","val_recall: 0.7599999904632568\n","val_auc: 0.6015398502349854\n","val_prc_auc: 0.42429065704345703\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 85ms/step - auc: 0.5950 - binary_accuracy: 0.5672 - f1: 0.6346 - loss: 0.6878 - prc_auc: 0.5442 - precision: 0.5319 - recall: 0.7874 - val_auc: 0.6015 - val_binary_accuracy: 0.5343 - val_f1: 0.5352 - val_loss: 0.6922 - val_prc_auc: 0.4243 - val_precision: 0.4130 - val_recall: 0.7600\n","Epoch 6/15\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.6002 - binary_accuracy: 0.5716 - f1: 0.6292 - loss: 0.6866 - prc_auc: 0.5457 - precision: 0.5364 - recall: 0.7617\n","Epoch 6: Validation Metrics:\n","loss: 0.6858984231948853\n","val_binary_accuracy: 0.5524193644523621\n","val_precision: 0.4244372844696045\n","val_recall: 0.7542856931686401\n","val_auc: 0.6059101223945618\n","val_prc_auc: 0.4266301393508911\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 85ms/step - auc: 0.6001 - binary_accuracy: 0.5717 - f1: 0.6293 - loss: 0.6866 - prc_auc: 0.5459 - precision: 0.5367 - recall: 0.7616 - val_auc: 0.6059 - val_binary_accuracy: 0.5524 - val_f1: 0.5432 - val_loss: 0.6905 - val_prc_auc: 0.4266 - val_precision: 0.4244 - val_recall: 0.7543\n","Epoch 7/15\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.6045 - binary_accuracy: 0.5692 - f1: 0.6236 - loss: 0.6855 - prc_auc: 0.5484 - precision: 0.5352 - recall: 0.7479\n","Epoch 7: Validation Metrics:\n","loss: 0.6849609613418579\n","val_binary_accuracy: 0.5584677457809448\n","val_precision: 0.42810457944869995\n","val_recall: 0.7485714554786682\n","val_auc: 0.6073964834213257\n","val_prc_auc: 0.42773300409317017\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 85ms/step - auc: 0.6044 - binary_accuracy: 0.5693 - f1: 0.6238 - loss: 0.6854 - prc_auc: 0.5486 - precision: 0.5354 - recall: 0.7479 - val_auc: 0.6074 - val_binary_accuracy: 0.5585 - val_f1: 0.5447 - val_loss: 0.6890 - val_prc_auc: 0.4277 - val_precision: 0.4281 - val_recall: 0.7486\n","Epoch 8/15\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.6086 - binary_accuracy: 0.5717 - f1: 0.6221 - loss: 0.6844 - prc_auc: 0.5524 - precision: 0.5378 - recall: 0.7387\n","Epoch 8: Validation Metrics:\n","loss: 0.6840765476226807\n","val_binary_accuracy: 0.5604838728904724\n","val_precision: 0.4285714328289032\n","val_recall: 0.7371428608894348\n","val_auc: 0.6092656850814819\n","val_prc_auc: 0.42656099796295166\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 85ms/step - auc: 0.6085 - binary_accuracy: 0.5718 - f1: 0.6222 - loss: 0.6844 - prc_auc: 0.5526 - precision: 0.5381 - recall: 0.7387 - val_auc: 0.6093 - val_binary_accuracy: 0.5605 - val_f1: 0.5420 - val_loss: 0.6877 - val_prc_auc: 0.4266 - val_precision: 0.4286 - val_recall: 0.7371\n","Epoch 9/15\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.6110 - binary_accuracy: 0.5738 - f1: 0.6226 - loss: 0.6834 - prc_auc: 0.5573 - precision: 0.5397 - recall: 0.7368\n","Epoch 9: Validation Metrics:\n","loss: 0.6832392811775208\n","val_binary_accuracy: 0.5544354915618896\n","val_precision: 0.4233333468437195\n","val_recall: 0.7257142663002014\n","val_auc: 0.6087583303451538\n","val_prc_auc: 0.4241024851799011\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 85ms/step - auc: 0.6109 - binary_accuracy: 0.5739 - f1: 0.6228 - loss: 0.6834 - prc_auc: 0.5575 - precision: 0.5400 - recall: 0.7367 - val_auc: 0.6088 - val_binary_accuracy: 0.5544 - val_f1: 0.5347 - val_loss: 0.6865 - val_prc_auc: 0.4241 - val_precision: 0.4233 - val_recall: 0.7257\n","Epoch 10/15\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.6143 - binary_accuracy: 0.5717 - f1: 0.6197 - loss: 0.6825 - prc_auc: 0.5597 - precision: 0.5382 - recall: 0.7313\n","Epoch 10: Validation Metrics:\n","loss: 0.682443380355835\n","val_binary_accuracy: 0.5625\n","val_precision: 0.4285714328289032\n","val_recall: 0.7200000286102295\n","val_auc: 0.6141433119773865\n","val_prc_auc: 0.4330539405345917\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 85ms/step - auc: 0.6142 - binary_accuracy: 0.5718 - f1: 0.6199 - loss: 0.6825 - prc_auc: 0.5599 - precision: 0.5385 - recall: 0.7314 - val_auc: 0.6141 - val_binary_accuracy: 0.5625 - val_f1: 0.5373 - val_loss: 0.6854 - val_prc_auc: 0.4331 - val_precision: 0.4286 - val_recall: 0.7200\n","Starting training for label: 3\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFMPNetModel.\n","\n","All the layers of TFMPNetModel were initialized from the model checkpoint at /content/drive/Othercomputers/ASUS Main/MLCP/05_MODELS/02_Transformer_Models/transformer_model_20250208_154459_1_epochs.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMPNetModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/15\n","\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - auc: 0.4752 - binary_accuracy: 0.4823 - f1: 0.5280 - loss: 0.6958 - prc_auc: 0.4840 - precision: 0.4802 - recall: 0.5867\n","Epoch 1: Validation Metrics:\n","loss: 0.6959530115127563\n","val_binary_accuracy: 0.47379031777381897\n","val_precision: 0.2261904776096344\n","val_recall: 0.46341463923454285\n","val_auc: 0.43391314148902893\n","val_prc_auc: 0.22166088223457336\n","\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 285ms/step - auc: 0.4751 - binary_accuracy: 0.4823 - f1: 0.5279 - loss: 0.6958 - prc_auc: 0.4839 - precision: 0.4803 - recall: 0.5864 - val_auc: 0.4339 - val_binary_accuracy: 0.4738 - val_f1: 0.3040 - val_loss: 0.6990 - val_prc_auc: 0.2217 - val_precision: 0.2262 - val_recall: 0.4634\n","Epoch 2/15\n","\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.4823 - binary_accuracy: 0.4931 - f1: 0.5196 - loss: 0.6953 - prc_auc: 0.4924 - precision: 0.4884 - recall: 0.5551\n","Epoch 2: Validation Metrics:\n","loss: 0.6955283880233765\n","val_binary_accuracy: 0.4879032373428345\n","val_precision: 0.22594141960144043\n","val_recall: 0.4390243887901306\n","val_auc: 0.4442664682865143\n","val_prc_auc: 0.22555677592754364\n","\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 88ms/step - auc: 0.4822 - binary_accuracy: 0.4931 - f1: 0.5195 - loss: 0.6953 - prc_auc: 0.4923 - precision: 0.4884 - recall: 0.5548 - val_auc: 0.4443 - val_binary_accuracy: 0.4879 - val_f1: 0.2983 - val_loss: 0.6976 - val_prc_auc: 0.2256 - val_precision: 0.2259 - val_recall: 0.4390\n","Epoch 3/15\n","\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.4867 - binary_accuracy: 0.4926 - f1: 0.5066 - loss: 0.6949 - prc_auc: 0.4928 - precision: 0.4874 - recall: 0.5274\n","Epoch 3: Validation Metrics:\n","loss: 0.6951418519020081\n","val_binary_accuracy: 0.5120967626571655\n","val_precision: 0.24017466604709625\n","val_recall: 0.4471544623374939\n","val_auc: 0.44654417037963867\n","val_prc_auc: 0.22545450925827026\n","\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 88ms/step - auc: 0.4866 - binary_accuracy: 0.4925 - f1: 0.5065 - loss: 0.6949 - prc_auc: 0.4927 - precision: 0.4875 - recall: 0.5272 - val_auc: 0.4465 - val_binary_accuracy: 0.5121 - val_f1: 0.3125 - val_loss: 0.6965 - val_prc_auc: 0.2255 - val_precision: 0.2402 - val_recall: 0.4472\n","Epoch 4/15\n","\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.4902 - binary_accuracy: 0.4971 - f1: 0.5013 - loss: 0.6945 - prc_auc: 0.4941 - precision: 0.4913 - recall: 0.5118\n","Epoch 4: Validation Metrics:\n","loss: 0.6947680115699768\n","val_binary_accuracy: 0.524193525314331\n","val_precision: 0.24434389173984528\n","val_recall: 0.4390243887901306\n","val_auc: 0.4531812071800232\n","val_prc_auc: 0.22621914744377136\n","\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 88ms/step - auc: 0.4901 - binary_accuracy: 0.4970 - f1: 0.5012 - loss: 0.6945 - prc_auc: 0.4940 - precision: 0.4913 - recall: 0.5117 - val_auc: 0.4532 - val_binary_accuracy: 0.5242 - val_f1: 0.3140 - val_loss: 0.6955 - val_prc_auc: 0.2262 - val_precision: 0.2443 - val_recall: 0.4390\n","Epoch 5/15\n","\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.4933 - binary_accuracy: 0.5044 - f1: 0.5006 - loss: 0.6941 - prc_auc: 0.4970 - precision: 0.4984 - recall: 0.5030\n","Epoch 5: Validation Metrics:\n","loss: 0.6944043040275574\n","val_binary_accuracy: 0.5262096524238586\n","val_precision: 0.2431192696094513\n","val_recall: 0.43089431524276733\n","val_auc: 0.4644172787666321\n","val_prc_auc: 0.23000501096248627\n","\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 88ms/step - auc: 0.4932 - binary_accuracy: 0.5043 - f1: 0.5005 - loss: 0.6941 - prc_auc: 0.4969 - precision: 0.4984 - recall: 0.5028 - val_auc: 0.4644 - val_binary_accuracy: 0.5262 - val_f1: 0.3109 - val_loss: 0.6946 - val_prc_auc: 0.2300 - val_precision: 0.2431 - val_recall: 0.4309\n","Epoch 6/15\n","\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.4995 - binary_accuracy: 0.5104 - f1: 0.4991 - loss: 0.6938 - prc_auc: 0.4994 - precision: 0.5045 - recall: 0.4939\n","Epoch 6: Validation Metrics:\n","loss: 0.6940493583679199\n","val_binary_accuracy: 0.5342742204666138\n","val_precision: 0.24528302252292633\n","val_recall: 0.42276424169540405\n","val_auc: 0.471163272857666\n","val_prc_auc: 0.23407498002052307\n","\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 88ms/step - auc: 0.4994 - binary_accuracy: 0.5103 - f1: 0.4990 - loss: 0.6938 - prc_auc: 0.4993 - precision: 0.5045 - recall: 0.4938 - val_auc: 0.4712 - val_binary_accuracy: 0.5343 - val_f1: 0.3104 - val_loss: 0.6938 - val_prc_auc: 0.2341 - val_precision: 0.2453 - val_recall: 0.4228\n","Epoch 7/15\n","\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.5038 - binary_accuracy: 0.5106 - f1: 0.4943 - loss: 0.6934 - prc_auc: 0.5023 - precision: 0.5050 - recall: 0.4842\n","Epoch 7: Validation Metrics:\n","loss: 0.6937012672424316\n","val_binary_accuracy: 0.5403226017951965\n","val_precision: 0.24637681245803833\n","val_recall: 0.4146341383457184\n","val_auc: 0.47812727093696594\n","val_prc_auc: 0.23573297262191772\n","\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 88ms/step - auc: 0.5038 - binary_accuracy: 0.5105 - f1: 0.4942 - loss: 0.6934 - prc_auc: 0.5022 - precision: 0.5050 - recall: 0.4840 - val_auc: 0.4781 - val_binary_accuracy: 0.5403 - val_f1: 0.3091 - val_loss: 0.6932 - val_prc_auc: 0.2357 - val_precision: 0.2464 - val_recall: 0.4146\n","Starting training for label: 4\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFMPNetModel.\n","\n","All the layers of TFMPNetModel were initialized from the model checkpoint at /content/drive/Othercomputers/ASUS Main/MLCP/05_MODELS/02_Transformer_Models/transformer_model_20250208_154459_1_epochs.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMPNetModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/15\n","\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - auc: 0.5351 - binary_accuracy: 0.5114 - f1: 0.0381 - loss: 0.6938 - prc_auc: 0.5453 - precision: 0.6457 - recall: 0.0197\n","Epoch 1: Validation Metrics:\n","loss: 0.6923713684082031\n","val_binary_accuracy: 0.7983871102333069\n","val_precision: 0.4545454680919647\n","val_recall: 0.05050504952669144\n","val_auc: 0.5286110043525696\n","val_prc_auc: 0.2721653878688812\n","\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 318ms/step - auc: 0.5356 - binary_accuracy: 0.5114 - f1: 0.0385 - loss: 0.6937 - prc_auc: 0.5457 - precision: 0.6479 - recall: 0.0199 - val_auc: 0.5286 - val_binary_accuracy: 0.7984 - val_f1: 0.0909 - val_loss: 0.6488 - val_prc_auc: 0.2722 - val_precision: 0.4545 - val_recall: 0.0505\n","Epoch 2/15\n","\u001b[1m63/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - auc: 0.5714 - binary_accuracy: 0.5300 - f1: 0.1346 - loss: 0.6903 - prc_auc: 0.5849 - precision: 0.7512 - recall: 0.0740\n","Epoch 2: Validation Metrics:\n","loss: 0.6887969970703125\n","val_binary_accuracy: 0.8084677457809448\n","val_precision: 0.5833333134651184\n","val_recall: 0.14141413569450378\n","val_auc: 0.5387247800827026\n","val_prc_auc: 0.2921207547187805\n","\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 90ms/step - auc: 0.5722 - binary_accuracy: 0.5300 - f1: 0.1351 - loss: 0.6903 - prc_auc: 0.5856 - precision: 0.7521 - recall: 0.0744 - val_auc: 0.5387 - val_binary_accuracy: 0.8085 - val_f1: 0.2276 - val_loss: 0.6517 - val_prc_auc: 0.2921 - val_precision: 0.5833 - val_recall: 0.1414\n","Epoch 3/15\n","\u001b[1m63/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.5967 - binary_accuracy: 0.5518 - f1: 0.2421 - loss: 0.6872 - prc_auc: 0.6127 - precision: 0.7342 - recall: 0.1452\n","Epoch 3: Validation Metrics:\n","loss: 0.6855561137199402\n","val_binary_accuracy: 0.7822580933570862\n","val_precision: 0.40425533056259155\n","val_recall: 0.19191919267177582\n","val_auc: 0.5462051630020142\n","val_prc_auc: 0.3075822591781616\n","\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 90ms/step - auc: 0.5974 - binary_accuracy: 0.5520 - f1: 0.2432 - loss: 0.6871 - prc_auc: 0.6133 - precision: 0.7350 - recall: 0.1460 - val_auc: 0.5462 - val_binary_accuracy: 0.7823 - val_f1: 0.2603 - val_loss: 0.6539 - val_prc_auc: 0.3076 - val_precision: 0.4043 - val_recall: 0.1919\n","Epoch 4/15\n","\u001b[1m63/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - auc: 0.6113 - binary_accuracy: 0.5743 - f1: 0.3503 - loss: 0.6843 - prc_auc: 0.6337 - precision: 0.7146 - recall: 0.2329\n","Epoch 4: Validation Metrics:\n","loss: 0.6825794577598572\n","val_binary_accuracy: 0.7399193644523621\n","val_precision: 0.3076923191547394\n","val_recall: 0.24242424964904785\n","val_auc: 0.5563570261001587\n","val_prc_auc: 0.3208298087120056\n","\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 90ms/step - auc: 0.6119 - binary_accuracy: 0.5746 - f1: 0.3514 - loss: 0.6843 - prc_auc: 0.6343 - precision: 0.7149 - recall: 0.2338 - val_auc: 0.5564 - val_binary_accuracy: 0.7399 - val_f1: 0.2712 - val_loss: 0.6557 - val_prc_auc: 0.3208 - val_precision: 0.3077 - val_recall: 0.2424\n","Epoch 5/15\n","\u001b[1m63/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - auc: 0.6241 - binary_accuracy: 0.5896 - f1: 0.4270 - loss: 0.6817 - prc_auc: 0.6493 - precision: 0.6880 - recall: 0.3106\n","Epoch 5: Validation Metrics:\n","loss: 0.6798308491706848\n","val_binary_accuracy: 0.7036290168762207\n","val_precision: 0.2857142984867096\n","val_recall: 0.3232323229312897\n","val_auc: 0.5641935467720032\n","val_prc_auc: 0.3346615433692932\n","\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 90ms/step - auc: 0.6246 - binary_accuracy: 0.5898 - f1: 0.4280 - loss: 0.6816 - prc_auc: 0.6498 - precision: 0.6882 - recall: 0.3116 - val_auc: 0.5642 - val_binary_accuracy: 0.7036 - val_f1: 0.3033 - val_loss: 0.6571 - val_prc_auc: 0.3347 - val_precision: 0.2857 - val_recall: 0.3232\n","Epoch 6/15\n","\u001b[1m63/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - auc: 0.6320 - binary_accuracy: 0.6091 - f1: 0.4949 - loss: 0.6792 - prc_auc: 0.6586 - precision: 0.6845 - recall: 0.3885\n","Epoch 6: Validation Metrics:\n","loss: 0.6772817969322205\n","val_binary_accuracy: 0.6854838728904724\n","val_precision: 0.27906978130340576\n","val_recall: 0.3636363744735718\n","val_auc: 0.569651186466217\n","val_prc_auc: 0.3400520086288452\n","\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 91ms/step - auc: 0.6325 - binary_accuracy: 0.6091 - f1: 0.4956 - loss: 0.6792 - prc_auc: 0.6591 - precision: 0.6843 - recall: 0.3895 - val_auc: 0.5697 - val_binary_accuracy: 0.6855 - val_f1: 0.3158 - val_loss: 0.6581 - val_prc_auc: 0.3401 - val_precision: 0.2791 - val_recall: 0.3636\n","Epoch 7/15\n","\u001b[1m63/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.6400 - binary_accuracy: 0.6140 - f1: 0.5366 - loss: 0.6770 - prc_auc: 0.6676 - precision: 0.6593 - recall: 0.4533\n","Epoch 7: Validation Metrics:\n","loss: 0.6749086976051331\n","val_binary_accuracy: 0.6592742204666138\n","val_precision: 0.2569444477558136\n","val_recall: 0.3737373650074005\n","val_auc: 0.5730987191200256\n","val_prc_auc: 0.3423757553100586\n","\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 90ms/step - auc: 0.6404 - binary_accuracy: 0.6140 - f1: 0.5372 - loss: 0.6769 - prc_auc: 0.6679 - precision: 0.6593 - recall: 0.4541 - val_auc: 0.5731 - val_binary_accuracy: 0.6593 - val_f1: 0.3045 - val_loss: 0.6589 - val_prc_auc: 0.3424 - val_precision: 0.2569 - val_recall: 0.3737\n","Epoch 8/15\n","\u001b[1m63/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.6452 - binary_accuracy: 0.6157 - f1: 0.5631 - loss: 0.6748 - prc_auc: 0.6737 - precision: 0.6416 - recall: 0.5022\n","Epoch 8: Validation Metrics:\n","loss: 0.6726934909820557\n","val_binary_accuracy: 0.6431451439857483\n","val_precision: 0.2621951103210449\n","val_recall: 0.4343434274196625\n","val_auc: 0.5765844583511353\n","val_prc_auc: 0.3464933931827545\n","\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 90ms/step - auc: 0.6456 - binary_accuracy: 0.6157 - f1: 0.5635 - loss: 0.6748 - prc_auc: 0.6740 - precision: 0.6417 - recall: 0.5029 - val_auc: 0.5766 - val_binary_accuracy: 0.6431 - val_f1: 0.3270 - val_loss: 0.6593 - val_prc_auc: 0.3465 - val_precision: 0.2622 - val_recall: 0.4343\n","Epoch 9/15\n","\u001b[1m63/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.6495 - binary_accuracy: 0.6228 - f1: 0.5887 - loss: 0.6728 - prc_auc: 0.6782 - precision: 0.6382 - recall: 0.5466\n","Epoch 9: Validation Metrics:\n","loss: 0.6706206202507019\n","val_binary_accuracy: 0.6270161271095276\n","val_precision: 0.2556818127632141\n","val_recall: 0.4545454680919647\n","val_auc: 0.5794596076011658\n","val_prc_auc: 0.3487308621406555\n","\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 90ms/step - auc: 0.6498 - binary_accuracy: 0.6227 - f1: 0.5889 - loss: 0.6728 - prc_auc: 0.6785 - precision: 0.6383 - recall: 0.5469 - val_auc: 0.5795 - val_binary_accuracy: 0.6270 - val_f1: 0.3273 - val_loss: 0.6596 - val_prc_auc: 0.3487 - val_precision: 0.2557 - val_recall: 0.4545\n","Epoch 10/15\n","\u001b[1m63/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.6523 - binary_accuracy: 0.6137 - f1: 0.5917 - loss: 0.6710 - prc_auc: 0.6826 - precision: 0.6196 - recall: 0.5666\n","Epoch 10: Validation Metrics:\n","loss: 0.6686776876449585\n","val_binary_accuracy: 0.6149193644523621\n","val_precision: 0.25\n","val_recall: 0.46464645862579346\n","val_auc: 0.582207441329956\n","val_prc_auc: 0.35193711519241333\n","\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 90ms/step - auc: 0.6526 - binary_accuracy: 0.6138 - f1: 0.5919 - loss: 0.6709 - prc_auc: 0.6829 - precision: 0.6199 - recall: 0.5668 - val_auc: 0.5822 - val_binary_accuracy: 0.6149 - val_f1: 0.3251 - val_loss: 0.6597 - val_prc_auc: 0.3519 - val_precision: 0.2500 - val_recall: 0.4646\n","Epoch 11/15\n","\u001b[1m63/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.6548 - binary_accuracy: 0.6083 - f1: 0.5937 - loss: 0.6692 - prc_auc: 0.6856 - precision: 0.6092 - recall: 0.5794\n","Epoch 11: Validation Metrics:\n","loss: 0.6668539047241211\n","val_binary_accuracy: 0.6129032373428345\n","val_precision: 0.2513369023799896\n","val_recall: 0.4747474789619446\n","val_auc: 0.5852606892585754\n","val_prc_auc: 0.35349908471107483\n","\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 90ms/step - auc: 0.6552 - binary_accuracy: 0.6085 - f1: 0.5940 - loss: 0.6691 - prc_auc: 0.6859 - precision: 0.6096 - recall: 0.5796 - val_auc: 0.5853 - val_binary_accuracy: 0.6129 - val_f1: 0.3287 - val_loss: 0.6597 - val_prc_auc: 0.3535 - val_precision: 0.2513 - val_recall: 0.4747\n","Epoch 12/15\n","\u001b[1m63/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.6574 - binary_accuracy: 0.6053 - f1: 0.5981 - loss: 0.6676 - prc_auc: 0.6875 - precision: 0.6022 - recall: 0.5945\n","Epoch 12: Validation Metrics:\n","loss: 0.6651396155357361\n","val_binary_accuracy: 0.5967742204666138\n","val_precision: 0.2410256415605545\n","val_recall: 0.4747474789619446\n","val_auc: 0.5864056348800659\n","val_prc_auc: 0.3577408194541931\n","\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 90ms/step - auc: 0.6577 - binary_accuracy: 0.6055 - f1: 0.5984 - loss: 0.6675 - prc_auc: 0.6878 - precision: 0.6026 - recall: 0.5946 - val_auc: 0.5864 - val_binary_accuracy: 0.5968 - val_f1: 0.3197 - val_loss: 0.6595 - val_prc_auc: 0.3577 - val_precision: 0.2410 - val_recall: 0.4747\n","Epoch 13/15\n","\u001b[1m63/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - auc: 0.6595 - binary_accuracy: 0.6126 - f1: 0.6100 - loss: 0.6660 - prc_auc: 0.6894 - precision: 0.6072 - recall: 0.6133\n","Epoch 13: Validation Metrics:\n","loss: 0.663527250289917\n","val_binary_accuracy: 0.5907257795333862\n","val_precision: 0.23999999463558197\n","val_recall: 0.4848484992980957\n","val_auc: 0.5862529873847961\n","val_prc_auc: 0.35428524017333984\n","\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 90ms/step - auc: 0.6599 - binary_accuracy: 0.6128 - f1: 0.6102 - loss: 0.6659 - prc_auc: 0.6897 - precision: 0.6075 - recall: 0.6133 - val_auc: 0.5863 - val_binary_accuracy: 0.5907 - val_f1: 0.3211 - val_loss: 0.6593 - val_prc_auc: 0.3543 - val_precision: 0.2400 - val_recall: 0.4848\n","Epoch 14/15\n","\u001b[1m63/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - auc: 0.6610 - binary_accuracy: 0.6117 - f1: 0.6124 - loss: 0.6645 - prc_auc: 0.6916 - precision: 0.6045 - recall: 0.6208\n","Epoch 14: Validation Metrics:\n","loss: 0.6620092391967773\n","val_binary_accuracy: 0.5887096524238586\n","val_precision: 0.24137930572032928\n","val_recall: 0.49494948983192444\n","val_auc: 0.5894079804420471\n","val_prc_auc: 0.3552165925502777\n","\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 90ms/step - auc: 0.6614 - binary_accuracy: 0.6118 - f1: 0.6125 - loss: 0.6645 - prc_auc: 0.6919 - precision: 0.6048 - recall: 0.6208 - val_auc: 0.5894 - val_binary_accuracy: 0.5887 - val_f1: 0.3245 - val_loss: 0.6590 - val_prc_auc: 0.3552 - val_precision: 0.2414 - val_recall: 0.4949\n","\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 273ms/step\n","\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 215ms/step\n","\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 214ms/step\n","\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 214ms/step\n","\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 214ms/step\n","[[1 0 1 0 0]\n"," [0 0 1 0 1]\n"," [0 1 1 0 0]\n"," ...\n"," [0 1 0 0 0]\n"," [1 0 1 0 1]\n"," [1 0 1 1 1]]\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.15"},"widgets":{"application/vnd.jupyter.widget-state+json":{"7ee763e22b064a0fbcf9b42ccf97b9ca":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_40fb4eea828a47eebde3deffb34eccaf","IPY_MODEL_6c28c29db2b54a68809be4927f2e6561","IPY_MODEL_156e7b3dc5c74377abf997088f5d5039"],"layout":"IPY_MODEL_143f3f72648947628be426536c54165a"}},"40fb4eea828a47eebde3deffb34eccaf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d465a2294ac346fcaeb1e40f905431ed","placeholder":"​","style":"IPY_MODEL_e8217faab1a64813989a48f3bc71980e","value":"model.safetensors: 100%"}},"6c28c29db2b54a68809be4927f2e6561":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_31789b2281b447e9a533802256b73e32","max":531998632,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ffa3a842b49943ba8fffe30c63b7e803","value":531998632}},"156e7b3dc5c74377abf997088f5d5039":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e0b5259abee4a00b2aed377427faebc","placeholder":"​","style":"IPY_MODEL_b1298a3565b24588878eab99a2f778d2","value":" 532M/532M [00:11&lt;00:00, 34.3MB/s]"}},"143f3f72648947628be426536c54165a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d465a2294ac346fcaeb1e40f905431ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e8217faab1a64813989a48f3bc71980e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"31789b2281b447e9a533802256b73e32":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ffa3a842b49943ba8fffe30c63b7e803":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0e0b5259abee4a00b2aed377427faebc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1298a3565b24588878eab99a2f778d2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a76d43f7e247423d8bd0267409172ae1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8ef25127aea24b65aed075efbf782bd9","IPY_MODEL_4905559eaa5b40779fa2d579308ddeb3","IPY_MODEL_ec09a2d650af46328f1be4ad121ed09f"],"layout":"IPY_MODEL_42f3658a4ec241fc93c783ee84caa9cf"}},"8ef25127aea24b65aed075efbf782bd9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aa274429fd264ada9935779eaa1ca354","placeholder":"​","style":"IPY_MODEL_faaa9d56430f4cd1854b1775bc3172ba","value":"tokenizer_config.json: 100%"}},"4905559eaa5b40779fa2d579308ddeb3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_baa06b0ec2d34d02b113f8503fddc216","max":363,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d441459d1c884b52b9772285b2fd17f0","value":363}},"ec09a2d650af46328f1be4ad121ed09f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9c4cd35bd3c64007a9744c871f60f686","placeholder":"​","style":"IPY_MODEL_b1ae715e675d45c2957a160069ac7d08","value":" 363/363 [00:00&lt;00:00, 31.3kB/s]"}},"42f3658a4ec241fc93c783ee84caa9cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aa274429fd264ada9935779eaa1ca354":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"faaa9d56430f4cd1854b1775bc3172ba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"baa06b0ec2d34d02b113f8503fddc216":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d441459d1c884b52b9772285b2fd17f0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9c4cd35bd3c64007a9744c871f60f686":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1ae715e675d45c2957a160069ac7d08":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"76c7ede6ff5e462a8312821bad1a8202":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4dc6ae2f526f4653a54c0c07381768db","IPY_MODEL_c42874edb92e43cca983af83dc96420b","IPY_MODEL_4bdb5352724c40eb9d493cb1efd743d1"],"layout":"IPY_MODEL_d4da583e676f4d08a7e4356e6c19f394"}},"4dc6ae2f526f4653a54c0c07381768db":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6812d035d701491cb605814f86bcbb3e","placeholder":"​","style":"IPY_MODEL_4549d9c099ab48e88afcbb1d8f4c789d","value":"vocab.txt: 100%"}},"c42874edb92e43cca983af83dc96420b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_224fb6448d974adc8c89a7e5d92893f2","max":231536,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ca45baf4c67c4a679a7300e9fb6e18d3","value":231536}},"4bdb5352724c40eb9d493cb1efd743d1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ae478ac2bfec4d3694f2550a6e7d28e8","placeholder":"​","style":"IPY_MODEL_1969c0b6db9c4f47b22607038a7b1a79","value":" 232k/232k [00:00&lt;00:00, 4.81MB/s]"}},"d4da583e676f4d08a7e4356e6c19f394":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6812d035d701491cb605814f86bcbb3e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4549d9c099ab48e88afcbb1d8f4c789d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"224fb6448d974adc8c89a7e5d92893f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca45baf4c67c4a679a7300e9fb6e18d3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ae478ac2bfec4d3694f2550a6e7d28e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1969c0b6db9c4f47b22607038a7b1a79":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"37822c1dea7c4066891fe9dc2bfad9de":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0607fc3d5ee2416ea523bd5cce0e8c80","IPY_MODEL_4b42397cb6fa4f13a52fcdfc4e7d7095","IPY_MODEL_115fb8c0adf84e5687cdfecaddc7fc88"],"layout":"IPY_MODEL_b469b0c304d84cbe9467cc76204428c5"}},"0607fc3d5ee2416ea523bd5cce0e8c80":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_15ff464abd8f4c98bcacb672569a8dc6","placeholder":"​","style":"IPY_MODEL_824bdcb736db49b0b01e101f3d4bbe32","value":"tokenizer.json: 100%"}},"4b42397cb6fa4f13a52fcdfc4e7d7095":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dbca1f857e064aeeabac2f8eac9c9ba7","max":466021,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ae0bdf64c6fe474ba4b361a10d3406ab","value":466021}},"115fb8c0adf84e5687cdfecaddc7fc88":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a2a9545b11644b6a7c683c3baa92760","placeholder":"​","style":"IPY_MODEL_54739fd4f13d4b128a24de6e70c503f3","value":" 466k/466k [00:00&lt;00:00, 10.7MB/s]"}},"b469b0c304d84cbe9467cc76204428c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15ff464abd8f4c98bcacb672569a8dc6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"824bdcb736db49b0b01e101f3d4bbe32":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dbca1f857e064aeeabac2f8eac9c9ba7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae0bdf64c6fe474ba4b361a10d3406ab":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5a2a9545b11644b6a7c683c3baa92760":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"54739fd4f13d4b128a24de6e70c503f3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cd6805a6b4bc490bb3dabb61504ab8e6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fed591da0d304c0f8b320ff464c5666c","IPY_MODEL_2c6bd7e97ea2414aac33c456b046e873","IPY_MODEL_6dd438058253463ab15a65ad8f33192f"],"layout":"IPY_MODEL_1872502f5bbf42d1827f773b9bcd37f6"}},"fed591da0d304c0f8b320ff464c5666c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eee4ebb41d904ad7a3084540c6b2f33a","placeholder":"​","style":"IPY_MODEL_229551aa84fd4c7787adcc867eabba53","value":"special_tokens_map.json: 100%"}},"2c6bd7e97ea2414aac33c456b046e873":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a75a543c4a414a9e98646af48b6bbe80","max":239,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c77a74539b4a4ca7a56aae18c674362e","value":239}},"6dd438058253463ab15a65ad8f33192f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_67b0ded56d4342698ab40156896dbef3","placeholder":"​","style":"IPY_MODEL_da87376fe355463386e1bcae2d6146d4","value":" 239/239 [00:00&lt;00:00, 21.9kB/s]"}},"1872502f5bbf42d1827f773b9bcd37f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eee4ebb41d904ad7a3084540c6b2f33a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"229551aa84fd4c7787adcc867eabba53":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a75a543c4a414a9e98646af48b6bbe80":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c77a74539b4a4ca7a56aae18c674362e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"67b0ded56d4342698ab40156896dbef3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"da87376fe355463386e1bcae2d6146d4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f394cc47b76c424ea50f39894d33a95e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ae139a3cd6614bdb8b13997dfa0926c7","IPY_MODEL_e3067982ccdd4f4b9e25d084782b196b","IPY_MODEL_b2026bb57acb43ccabb5254e5a74b4d6"],"layout":"IPY_MODEL_5c535e651f1c4adcbaa60791742af937"}},"ae139a3cd6614bdb8b13997dfa0926c7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_25c1221c444c4c6e81dc25e8ab06143f","placeholder":"​","style":"IPY_MODEL_359a25604e64401bba227185ae1c76c6","value":"config.json: 100%"}},"e3067982ccdd4f4b9e25d084782b196b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e2efb55c88a44d24941853097ec4a60d","max":571,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bcb1e5270d0b4d76ad75e7f33f15895a","value":571}},"b2026bb57acb43ccabb5254e5a74b4d6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_18c24b0b760a4b5e8dfcbace68fd7469","placeholder":"​","style":"IPY_MODEL_f9642ec2fd524b17a5a1ce5f4f5c1ee1","value":" 571/571 [00:00&lt;00:00, 52.1kB/s]"}},"5c535e651f1c4adcbaa60791742af937":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"25c1221c444c4c6e81dc25e8ab06143f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"359a25604e64401bba227185ae1c76c6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e2efb55c88a44d24941853097ec4a60d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bcb1e5270d0b4d76ad75e7f33f15895a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"18c24b0b760a4b5e8dfcbace68fd7469":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9642ec2fd524b17a5a1ce5f4f5c1ee1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"64e547717c224c02981da5a4ff0462a4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_810f6376fd9e4b4589f1d76bdbbba698","IPY_MODEL_65b9b5ea95334da8a0519f61e9dd4c8b","IPY_MODEL_3a0217664365498e81c1375d11696089"],"layout":"IPY_MODEL_a763decf210b4bb19269b66251f8ff6f"}},"810f6376fd9e4b4589f1d76bdbbba698":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_36965743912a4bbc83a05b3c55b9e544","placeholder":"​","style":"IPY_MODEL_fcb2b70818dd4586bc146ccf99f08869","value":"model.safetensors: 100%"}},"65b9b5ea95334da8a0519f61e9dd4c8b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b654486cb19443ec88cba4f74f18da7b","max":437971872,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ece2b16bff0d42e1a080df7ff743a59d","value":437971872}},"3a0217664365498e81c1375d11696089":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_42f47dbea5d44dff9749627569b344e3","placeholder":"​","style":"IPY_MODEL_afd7b77b1b5e41b9aa344ef3d4af6eee","value":" 438M/438M [00:01&lt;00:00, 238MB/s]"}},"a763decf210b4bb19269b66251f8ff6f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"36965743912a4bbc83a05b3c55b9e544":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fcb2b70818dd4586bc146ccf99f08869":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b654486cb19443ec88cba4f74f18da7b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ece2b16bff0d42e1a080df7ff743a59d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"42f47dbea5d44dff9749627569b344e3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"afd7b77b1b5e41b9aa344ef3d4af6eee":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ba398e7612f94d05bf7e8ab6bba737a9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_16ae265f83cf49359cf247b140876d43","IPY_MODEL_3122e14fb24642359a3b428a1713e243","IPY_MODEL_df0f42e9ef944cbc88a9259550b140e1"],"layout":"IPY_MODEL_248c5df3318b4fb9b8796274b3349919"}},"16ae265f83cf49359cf247b140876d43":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0ea051a59d0d4612b36d4e85b60f6700","placeholder":"​","style":"IPY_MODEL_2f05d6fda3f548f3a4423764e0b347b1","value":"tokenizer_config.json: 100%"}},"3122e14fb24642359a3b428a1713e243":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_18705f011fe149819410f429e212a580","max":363,"min":0,"orientation":"horizontal","style":"IPY_MODEL_aff81471b4ab4e6bbfcd271dfa1987c3","value":363}},"df0f42e9ef944cbc88a9259550b140e1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a0b079a8b4e04eeba17e22b47dbd697a","placeholder":"​","style":"IPY_MODEL_21ac481d907444239b10afc2e45b6a40","value":" 363/363 [00:00&lt;00:00, 31.9kB/s]"}},"248c5df3318b4fb9b8796274b3349919":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ea051a59d0d4612b36d4e85b60f6700":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f05d6fda3f548f3a4423764e0b347b1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"18705f011fe149819410f429e212a580":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aff81471b4ab4e6bbfcd271dfa1987c3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a0b079a8b4e04eeba17e22b47dbd697a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"21ac481d907444239b10afc2e45b6a40":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0c9c57424f47461b93c1a19f53195da0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e6cae83c079e464985e19cfefc812293","IPY_MODEL_589bd6a7ac904448ab6dcfbe2f7c80a7","IPY_MODEL_412556a73a984907897d680a3dd18115"],"layout":"IPY_MODEL_c50648adaaad4ee3a80866f60a720b16"}},"e6cae83c079e464985e19cfefc812293":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_040060ab909340c3b51d4cf6a8a61700","placeholder":"​","style":"IPY_MODEL_c95f124715fd482db88bd5241ea957ea","value":"vocab.txt: 100%"}},"589bd6a7ac904448ab6dcfbe2f7c80a7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_456027c9130b4732a5a0858591af9376","max":231536,"min":0,"orientation":"horizontal","style":"IPY_MODEL_01e636c9ad4d4942b9688f5aa6900004","value":231536}},"412556a73a984907897d680a3dd18115":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0c4985118b8144f59546413169200453","placeholder":"​","style":"IPY_MODEL_51cc0c6348ab4394858bea3d192ecc53","value":" 232k/232k [00:00&lt;00:00, 14.2MB/s]"}},"c50648adaaad4ee3a80866f60a720b16":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"040060ab909340c3b51d4cf6a8a61700":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c95f124715fd482db88bd5241ea957ea":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"456027c9130b4732a5a0858591af9376":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"01e636c9ad4d4942b9688f5aa6900004":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0c4985118b8144f59546413169200453":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"51cc0c6348ab4394858bea3d192ecc53":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"248725129d6d498db9eeac1d6635dfba":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_396d3176dd7e4ebfaeec9567105e0688","IPY_MODEL_bb59ae35548e408eb864e77c4a600c4f","IPY_MODEL_030e31fb9dc44971892989c83fdb3573"],"layout":"IPY_MODEL_9f3307e969bb4f2d812a99b9fea4f662"}},"396d3176dd7e4ebfaeec9567105e0688":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_633e162edc224366ab877efc695d1dd8","placeholder":"​","style":"IPY_MODEL_9c11ef29a48b441499a369e298874184","value":"tokenizer.json: 100%"}},"bb59ae35548e408eb864e77c4a600c4f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1f7e0eaca8eb47fa9240c82ed620f8d6","max":466021,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a47f2d2dedb5484dbf657d55807569cf","value":466021}},"030e31fb9dc44971892989c83fdb3573":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_37c7b58ab0dd41d9846237489975bd49","placeholder":"​","style":"IPY_MODEL_cfc07547e2614469ac2ea65c112575e3","value":" 466k/466k [00:00&lt;00:00, 2.17MB/s]"}},"9f3307e969bb4f2d812a99b9fea4f662":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"633e162edc224366ab877efc695d1dd8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c11ef29a48b441499a369e298874184":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1f7e0eaca8eb47fa9240c82ed620f8d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a47f2d2dedb5484dbf657d55807569cf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"37c7b58ab0dd41d9846237489975bd49":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cfc07547e2614469ac2ea65c112575e3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3288dbb820d446c182178be2591407fb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_75f3389b5cb945b89f05a3d831b5ee14","IPY_MODEL_15f8b80d0a8b4d898cb34b6b7e953b0e","IPY_MODEL_82f69a30662b41aaa3fbfea3ca86ca3e"],"layout":"IPY_MODEL_48dd7586eac1408bbacc44f6893c1626"}},"75f3389b5cb945b89f05a3d831b5ee14":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c6aa8b0aa94f431f8105ca62a1aebf1b","placeholder":"​","style":"IPY_MODEL_5754014ae09944dea10fb5c0f5c3739d","value":"special_tokens_map.json: 100%"}},"15f8b80d0a8b4d898cb34b6b7e953b0e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac5debef42e141a5a4ba62aca46793a7","max":239,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4e2c8efcc0e34a8cb4f44d67dade470b","value":239}},"82f69a30662b41aaa3fbfea3ca86ca3e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1dea0db52330436bb2db163fe768b17d","placeholder":"​","style":"IPY_MODEL_775bb7897bd64ceab13b09bec7e46bc0","value":" 239/239 [00:00&lt;00:00, 19.7kB/s]"}},"48dd7586eac1408bbacc44f6893c1626":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6aa8b0aa94f431f8105ca62a1aebf1b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5754014ae09944dea10fb5c0f5c3739d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ac5debef42e141a5a4ba62aca46793a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e2c8efcc0e34a8cb4f44d67dade470b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1dea0db52330436bb2db163fe768b17d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"775bb7897bd64ceab13b09bec7e46bc0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
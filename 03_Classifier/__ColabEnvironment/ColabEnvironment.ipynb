{"cells":[{"cell_type":"markdown","metadata":{"id":"Jx_pafU4toh5"},"source":["For google colab environment"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xe8Eih0hHc2-","executionInfo":{"status":"ok","timestamp":1739148441943,"user_tz":-120,"elapsed":38339,"user":{"displayName":"Dinu Ion George","userId":"14034259797438922081"}},"outputId":"64441ec6-f8e9-4f87-cdd9-aff921552ef3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YmNUmwcdHc3B","executionInfo":{"status":"ok","timestamp":1739148442756,"user_tz":-120,"elapsed":812,"user":{"displayName":"Dinu Ion George","userId":"14034259797438922081"}},"outputId":"b9892a9e-9861-43a8-e86d-e5751edda9a7"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/Othercomputers/ASUS Main/MLCP/03_Classifier/__ColabEnvironment\n"]}],"source":["cd \"/content/drive/Othercomputers/ASUS Main/MLCP/03_Classifier/__ColabEnvironment\""]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qIIAqc0VIDm4","executionInfo":{"status":"ok","timestamp":1739148445459,"user_tz":-120,"elapsed":2702,"user":{"displayName":"Dinu Ion George","userId":"14034259797438922081"}},"outputId":"dc7d9b07-2083-4933-9cd9-557c9c2e7d8e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting scikit-multilearn\n","  Downloading scikit_multilearn-0.2.0-py3-none-any.whl.metadata (6.0 kB)\n","Downloading scikit_multilearn-0.2.0-py3-none-any.whl (89 kB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/89.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.4/89.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: scikit-multilearn\n","Successfully installed scikit-multilearn-0.2.0\n"]}],"source":["# install dependencies\n","\n","# !pip install catboost\n","!pip install scikit-multilearn\n","# !pip install --upgrade tensorflow-addons\n","# !pip install sentence-transformers\n","# !pip install --upgrade tensorflow-addons\n","# !pip install dask[dataframe]\n"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T2lmJ49jGPJE","executionInfo":{"status":"ok","timestamp":1739148445474,"user_tz":-120,"elapsed":14,"user":{"displayName":"Dinu Ion George","userId":"14034259797438922081"}},"outputId":"49b7e1f9-36d7-4c9a-eb10-bdb603f5ba07"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/Othercomputers/ASUS Main/MLCP/03_Classifier\n"]}],"source":["import sys\n","import os\n","\n","# Add the parent directory of app_config to the Python path\n","sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n","print(os.path.abspath(os.path.join(os.getcwd(), '..')))\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":974,"status":"ok","timestamp":1739148446449,"user":{"displayName":"Dinu Ion George","userId":"14034259797438922081"},"user_tz":-120},"id":"7T7YDX_3GPJE","outputId":"e3c7678d-b30d-401b-9fae-617beca05988"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/Othercomputers/ASUS Main/MLCP\n"]}],"source":["from app_config import AppConfig\n","\n","# define configuration proxy\n","configProxy = AppConfig()\n","CONFIG = configProxy.return_config()\n","\n","print(CONFIG['BASE_DIR'])"]},{"cell_type":"markdown","metadata":{"id":"-d7Bz7antoh8"},"source":["Start code here"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":269},"executionInfo":{"elapsed":2129,"status":"error","timestamp":1739027432803,"user":{"displayName":"Dinu Ion George","userId":"14034259797438922081"},"user_tz":-120},"id":"vnQee3H_Hc3B","outputId":"93859f2d-fe9d-4e36-8869-ed550be4af9a"},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-31747437e9e0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mapp_src\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCustomEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert-base-uncased\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"This is a test\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"This is another test\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"This is a third test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/Othercomputers/ASUS Main/MLCP/03_Classifier/app_src/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mCustomEncoder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCustomEncoder\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mCustomEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mClassifierChainWrapper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClassifierChainWrapper\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mClassifierChainWrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mDecisionTreeEvaluator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDecisionTreeEvaluator\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mDecisionTreeEvaluator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/Othercomputers/ASUS Main/MLCP/03_Classifier/app_src/CustomEncoder.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# related third-party\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTFAutoModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Check the dependencies satisfy the minimal versions required.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdependency_versions_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m from .utils import (\n\u001b[1;32m     28\u001b[0m     \u001b[0mOptionalDependencyNotAvailable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/dependency_versions_check.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdependency_versions_table\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequire_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_version_core\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbackbone_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBackboneConfigMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBackboneMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mchat_template_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDocstringParsingException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeHintParsingException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_json_schema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconstants\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIMAGENET_DEFAULT_MEAN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMAGENET_DEFAULT_STD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMAGENET_STANDARD_MEAN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMAGENET_STANDARD_STD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m from .doc import (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/chat_template_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_jinja_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mjinja2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mjinja2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExtension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mjinja2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msandbox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImmutableSandboxedEnvironment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jinja2/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbccache\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFileSystemBytecodeCache\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mFileSystemBytecodeCache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbccache\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMemcachedBytecodeCache\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mMemcachedBytecodeCache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0menvironment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEnvironment\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mEnvironment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0menvironment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTemplate\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mTemplate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTemplateAssertionError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mTemplateAssertionError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jinja2/environment.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCodeType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmarkupsafe\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMarkup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_compile_bytecode\u001b[0;34m(data, name, bytecode_path, source_path)\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["from app_src import CustomEncoder\n","\n","encoder = CustomEncoder(\"bert-base-uncased\")\n","data = [\"This is a test\", \"This is another test\", \"This is a third test\"]\n","\n","encodded_sentence = encoder.encode_problem_statement(data)\n","print(encodded_sentence)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":332},"executionInfo":{"elapsed":23706,"status":"error","timestamp":1738784679685,"user":{"displayName":"Dinu Ion George","userId":"14034259797438922081"},"user_tz":-120},"id":"AL8NTNOKHc3C","outputId":"abb35947-c589-4a59-e7ac-fde63c1314b8"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n","Dask dataframe query planning is disabled because dask-expr is not installed.\n","\n","You can install it with `pip install dask[dataframe]` or `conda install dask`.\n","This will raise in a future version.\n","\n","  warnings.warn(msg, FutureWarning)\n"]},{"ename":"TypeError","evalue":"ClassifierChainWrapper.__init__() takes 2 positional arguments but 4 were given","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-97d0783496a7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mclassifierChainWrapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassifierChainWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bert-base-uncased\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mclassifierChainWrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmetrics_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifierChainWrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: ClassifierChainWrapper.__init__() takes 2 positional arguments but 4 were given"]}],"source":["from app_src import ClassifierChainWrapper\n","from sklearn.ensemble import RandomForestClassifier\n","\n","classifierChainWrapper = ClassifierChainWrapper(RandomForestClassifier(), \"bert-base-uncased\", 5)\n","classifierChainWrapper.fit()\n","metrics_results = classifierChainWrapper.predict()\n","\n","for metric_name, metric_value in metrics_results.items():\n","    print(f\"{metric_name}: {metric_value}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"elapsed":21,"status":"error","timestamp":1738784694006,"user":{"displayName":"Dinu Ion George","userId":"14034259797438922081"},"user_tz":-120},"id":"2V2DZ5O5wpXk","outputId":"d20aeebd-4329-42fc-84ea-6727ce8dad68"},"outputs":[{"ename":"AttributeError","evalue":"'DecisionTreeEvaluator' object has no attribute 'benchmark_models'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-f48c8ed72720>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdecisionTreeEvaluator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecisionTreeEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdecisionTreeEvaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbenchmark_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_tags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# decisionTreeEvaluator.benchmark_models(encoder_batch_size=32, number_of_tags=10)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# decisionTreeEvaluator.benchmark_models(encoder_batch_size=32, number_of_tags=15)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'DecisionTreeEvaluator' object has no attribute 'benchmark_models'"]}],"source":["from app_src import DecisionTreeEvaluator\n","\n","decisionTreeEvaluator = DecisionTreeEvaluator()\n","decisionTreeEvaluator.benchmark_models(encoder_batch_size=32, number_of_tags=5)\n","# decisionTreeEvaluator.benchmark_models(encoder_batch_size=32, number_of_tags=10)\n","# decisionTreeEvaluator.benchmark_models(encoder_batch_size=32, number_of_tags=15)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4154,"status":"ok","timestamp":1738441980041,"user":{"displayName":"Dinu Ion George","userId":"14034259797438922081"},"user_tz":-120},"id":"UMkaJZsOBid0","outputId":"607490d9-83ea-4484-8a63-e7bc11428d7b"},"outputs":[{"name":"stdout","output_type":"stream","text":["2.18.0\n","CUDA version: 12.5.1\n","CUDNN version: 9\n"]}],"source":["import tensorflow as tf\n","print(tf.__version__)\n","\n","build_info = tf.sysconfig.get_build_info()\n","print(\"CUDA version:\", build_info.get(\"cuda_version\", \"Unknown\"))\n","print(\"CUDNN version:\", build_info.get(\"cudnn_version\", \"Unknown\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":729,"referenced_widgets":["7ee763e22b064a0fbcf9b42ccf97b9ca","40fb4eea828a47eebde3deffb34eccaf","6c28c29db2b54a68809be4927f2e6561","156e7b3dc5c74377abf997088f5d5039","143f3f72648947628be426536c54165a","d465a2294ac346fcaeb1e40f905431ed","e8217faab1a64813989a48f3bc71980e","31789b2281b447e9a533802256b73e32","ffa3a842b49943ba8fffe30c63b7e803","0e0b5259abee4a00b2aed377427faebc","b1298a3565b24588878eab99a2f778d2"]},"executionInfo":{"elapsed":222505,"status":"ok","timestamp":1738622095096,"user":{"displayName":"Dinu Ion George","userId":"14034259797438922081"},"user_tz":-120},"id":"_DhP3q-Htoh9","outputId":"8783a6b1-64b3-4e8d-bf2b-f3990fc2ee04"},"outputs":[{"name":"stdout","output_type":"stream","text":["GPU is available\n","GPU details: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n","Dask dataframe query planning is disabled because dask-expr is not installed.\n","\n","You can install it with `pip install dask[dataframe]` or `conda install dask`.\n","This will raise in a future version.\n","\n","  warnings.warn(msg, FutureWarning)\n","/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7ee763e22b064a0fbcf9b42ccf97b9ca","version_major":2,"version_minor":0},"text/plain":["model.safetensors:  24%|##3       | 126M/532M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFMPNetModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing TFMPNetModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFMPNetModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights or buffers of the TF 2.0 model TFMPNetModel were not initialized from the PyTorch model and are newly initialized: ['mpnet.pooler.dense.weight', 'mpnet.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 962ms/step - auc: 0.5180 - binary_accuracy: 0.4868 - label_wise_accuracy: 0.5003 - label_wise_f1_score: 0.4110 - label_wise_macro_f1: 0.4369 - loss: 0.7062 - prc_auc: 0.3934 - precision: 0.3981 - recall: 0.6753 - subset_accuracy: 0.0170 - subset_f1: 0.4971 - subset_precision: 0.3978 - subset_recall: 0.6687\n","Epoch 1: Validation Metrics:\n","loss: 0.7000761032104492\n","val_label_wise_f1_score: [0.6046511  0.31999996 0.         0.6666666  0.        ]\n","val_label_wise_accuracy: [0.46875 0.46875 0.6875  0.53125 0.71875]\n","val_binary_accuracy: 0.5161765217781067\n","val_precision: 0.40509089827537537\n","val_recall: 0.5279620885848999\n","val_label_wise_macro_f1: 0.35385945439338684\n","val_subset_accuracy: 0.03492647036910057\n","val_subset_precision: 0.4010416865348816\n","val_subset_recall: 0.5241115093231201\n","val_subset_f1: 0.45356810092926025\n","val_auc: 0.5249963998794556\n","val_prc_auc: 0.40708646178245544\n","\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 1s/step - auc: 0.5181 - binary_accuracy: 0.4868 - label_wise_accuracy: 0.5008 - label_wise_f1_score: 0.4104 - label_wise_macro_f1: 0.4367 - loss: 0.7061 - prc_auc: 0.3935 - precision: 0.3981 - recall: 0.6750 - subset_accuracy: 0.0171 - subset_f1: 0.4970 - subset_precision: 0.3979 - subset_recall: 0.6683 - val_auc: 0.5250 - val_binary_accuracy: 0.5162 - val_label_wise_accuracy: 0.5750 - val_label_wise_f1_score: 0.3183 - val_label_wise_macro_f1: 0.3539 - val_loss: 0.6918 - val_prc_auc: 0.4071 - val_precision: 0.4051 - val_recall: 0.5280 - val_subset_accuracy: 0.0349 - val_subset_f1: 0.4536 - val_subset_precision: 0.4010 - val_subset_recall: 0.5241\n","Transformer model saved to /content/drive/Othercomputers/ASUS Main/MLCP/05_MODELS/02_Transformer_Models/transformer_model_20250203_223120\n","Model saved to /content/drive/Othercomputers/ASUS Main/MLCP/05_MODELS/01_Custom_Models/custom_model_20250203_223120.weights.h5\n"]}],"source":["import os\n","import tensorflow as tf\n","\n","# Check if GPU is available and print details\n","if tf.config.experimental.list_physical_devices('GPU'):\n","    print(\"GPU is available\")\n","    print(\"GPU details:\", tf.config.list_physical_devices('GPU'))\n","else:\n","    print(\"GPU not available, using CPU\")\n","\n","# # If GPU is available, try setting memory growth\n","# gpus = tf.config.list_physical_devices('GPU')\n","# if gpus:\n","#     try:\n","#         # Currently, memory growth needs to be the same across GPUs\n","#         for gpu in gpus:\n","#             tf.config.experimental.set_memory_growth(gpu, True)\n","#         logical_gpus = tf.config.list_logical_devices('GPU')\n","#         print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n","#     except RuntimeError as e:\n","#         # Memory growth must be set before GPUs have been initialized\n","#         print('Error')\n","#         print(e)\n","\n","# local application/library specific imports\n","from app_src import SentenceTransformerWrapper\n","from app_config import AppConfig\n","\n","# define configuration proxy\n","configProxy = AppConfig()\n","CONFIG = configProxy.return_config()\n","\n","NUMBER_OF_TAGS = 5\n","\n","sentenceTransformerWrapper = SentenceTransformerWrapper(\"microsoft/mpnet-base\", NUMBER_OF_TAGS)\n","sentenceTransformerWrapper.train_model(\n","    train_dataset_path=CONFIG[f'TOP_{NUMBER_OF_TAGS}_TRAINING_DATASET_PATH'],\n","    val_dataset_path=CONFIG[f'TOP_{NUMBER_OF_TAGS}_VALIDATION_DATASET_PATH'],\n","    epochs=1,\n","    batch_size=32,\n","    train_model=True\n","    )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":59070,"status":"ok","timestamp":1738622242973,"user":{"displayName":"Dinu Ion George","userId":"14034259797438922081"},"user_tz":-120},"id":"Pd4K7EOOtoh9","outputId":"d6499eee-9aa8-4b77-fa81-b902b942e2ee"},"outputs":[{"name":"stderr","output_type":"stream","text":["All model checkpoint layers were used when initializing TFMPNetModel.\n","\n","All the layers of TFMPNetModel were initialized from the model checkpoint at /content/drive/Othercomputers/ASUS Main/MLCP/05_MODELS/02_Transformer_Models/transformer_model_20250203_223120.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMPNetModel for predictions without further training.\n","/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adamw', because it has 2 variables whereas the saved optimizer has 6 variables. \n","  saveable.load_own_variables(weights_store.get(inner_path))\n"]},{"name":"stdout","output_type":"stream","text":["Model loaded from /content/drive/Othercomputers/ASUS Main/MLCP/05_MODELS/01_Custom_Models/custom_model_20250203_223120.weights.h5\n","\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 990ms/step - auc: 0.5208 - binary_accuracy: 0.5071 - label_wise_accuracy: 0.5067 - label_wise_f1_score: 0.3501 - label_wise_macro_f1: 0.3516 - loss: 0.6924 - prc_auc: 0.4088 - precision: 0.3994 - recall: 0.5128 - subset_accuracy: 0.0333 - subset_f1: 0.4498 - subset_precision: 0.4007 - subset_recall: 0.5159\n","Evaluation Metrics:\n","Loss: 0.6929304599761963\n","Label F1 Scores: [0.56410253 0.4705882  0.09999999 0.43243238 0.        ]\n","Label Accuracies: [0.46875 0.4375  0.4375  0.34375 0.75   ]\n","Accuracy: 0.5071220993995667\n","Precision: 0.3944847583770752\n","Recall: 0.5101351141929626\n","F1 Score: 0.3509281873703003\n","Subset Accuracy: 0.028343023732304573\n","Subset Precision: 0.3935804069042206\n","Subset Recall: 0.5103682279586792\n","Subset F1: 0.44326552748680115\n","AUC: 0.5183724164962769\n","PRC AUC: 0.4017588496208191\n"]},{"data":{"text/plain":["{'Loss': 0.6929304599761963,\n"," 'Label F1 Scores': array([0.56410253, 0.4705882 , 0.09999999, 0.43243238, 0.        ],\n","       dtype=float32),\n"," 'Label Accuracies': array([0.46875, 0.4375 , 0.4375 , 0.34375, 0.75   ], dtype=float32),\n"," 'Accuracy': 0.5071220993995667,\n"," 'Precision': 0.3944847583770752,\n"," 'Recall': 0.5101351141929626,\n"," 'F1 Score': 0.3509281873703003,\n"," 'Subset Accuracy': 0.028343023732304573,\n"," 'Subset Precision': 0.3935804069042206,\n"," 'Subset Recall': 0.5103682279586792,\n"," 'Subset F1': 0.44326552748680115,\n"," 'AUC': 0.5183724164962769,\n"," 'PRC AUC': 0.4017588496208191}"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","\n","# local application/library specific imports\n","from app_src import SentenceTransformerWrapper\n","from app_config import AppConfig\n","\n","# define configuration proxy\n","configProxy = AppConfig()\n","CONFIG = configProxy.return_config()\n","\n","NUMBER_OF_TAGS = 5\n","\n","sentenceTransformerWrapper = SentenceTransformerWrapper(\"microsoft/mpnet-base\", NUMBER_OF_TAGS)\n","\n","sentenceTransformerWrapper.benchmark_model(\n","    test_dataset_path=CONFIG[f'TOP_{NUMBER_OF_TAGS}_TESTING_DATASET_PATH'],\n","    batch_size=32,\n","    model_path= os.path.join(CONFIG[\"MODEL_SAVE_PATH_ROOT\"], 'custom_model_20250203_223120.weights.h5'),\n","    transformer_model_path= os.path.join(CONFIG[\"TRANSFORMER_SAVE_PATH_ROOT\"], 'transformer_model_20250203_223120')\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["0aca9bdca6b844d28e52f26c9c8d7c0b","3b9df8fc691f40fe98990c54561e2c74","6fc42a1b658b45b882fb43b7b6d8f232","e4f13c189dab4b3da8efd1a02af84723","8ddf1c28f5994768ac08be730fb5d4f9","38a505fae5844b9a99dc7240b56f1f35","e7479909d47f4b32a70cdd30a9469d7d","394054e7098d42d68f43d75f088d00f0","aa2a78105181497c80a88a6cd6272e7a","56ced83d439f4233974c0b732c4b8008","80172f1094b240ebb9474fd8d5ec92b6","5a58397a57e14097b5e26abd6bd2277f","d03f852c858b4a9b82b9edfed62aae95","f4bb3a71389b4b16b94077ba86f72fac","10bdce2086a5473ba4cb6ddcee7d16e4","133aa1b9c30c413c801aa5af370d7152","e4aca2b76727418c876b97d847884b6f","5dd7e4033d7d4a7687910dd2e7030454","197034abbdda48c48ca7327db1cc3187","3de58cc6746d4b5bb0e82579661a960b","346f0e433bfe4df4a4ce6b9f884a3c43","dde9c02f2afc4fbc9e2f6a8041de369c","629d0c97f04945dfa52d407952c3123d","ecaeca1869b54dd3816f8198d483adc4","af49943242a24b8f87ffdb4bc1265ab4","445c5becb50647a098e68e6bca012276","7a441379e706410388285282d411606c","82c4db0038a142ffa98f46da523d95f3","256df0b0bbc543caa79c397cbbfba153","d0944baaa43845f8a21712e9b942719d","3752e4d3b48a4f7d8897efc0c72b92bf","26f4c11f6dbc43b19112f4502a4546a7","42d356c365ca4c2ab154f640a5a50f15","ed78789852be40d4b2b16711594f3bc1","71ffb7c2f5e24755a283cd5294e40f7e","3e5076a0f2ca4e6cbc4fd25a35cb23dc","cb5b456f7378455e9fb95392b2c27b24","7d29ee8eb0d9461d87c32913f11ea7b5","c9f2cd35619145e5b043f9b8883bd639","34c364b6aba24f388bf8b35d3f87fa11","3199f9cac05842f1beff8c0e221fa4b9","f19b119822bb42c2aeddb790466c3d78","21560efa6dab4552925f64ef39b9617a","ec472bf3c24e4a489f6b4666c3e0a9e4"]},"executionInfo":{"elapsed":115275,"status":"ok","timestamp":1739126435824,"user":{"displayName":"Dinu Ion George","userId":"14034259797438922081"},"user_tz":-120},"id":"mCcdWL_XfxIG","outputId":"0e645e36-f579-4256-c263-c19824970fb0"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n","Dask dataframe query planning is disabled because dask-expr is not installed.\n","\n","You can install it with `pip install dask[dataframe]` or `conda install dask`.\n","This will raise in a future version.\n","\n","  warnings.warn(msg, FutureWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Training and evaluating model: sentence-transformers/all-mpnet-base-v2\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0aca9bdca6b844d28e52f26c9c8d7c0b","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5a58397a57e14097b5e26abd6bd2277f","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"629d0c97f04945dfa52d407952c3123d","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ed78789852be40d4b2b16711594f3bc1","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["All model checkpoint layers were used when initializing TFMPNetModel.\n","\n","All the layers of TFMPNetModel were initialized from the model checkpoint at /content/drive/Othercomputers/ASUS Main/MLCP/05_MODELS/02_Transformer_Models/transformer_model_20250208_155506_5_epochs.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMPNetModel for predictions without further training.\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/15\n","\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - auc: 0.5179 - binary_accuracy: 0.5164 - label_wise_accuracy: 0.5386 - label_wise_f1_score: 0.3872 - label_wise_macro_f1: 0.3739 - loss: 0.6923 - prc_auc: 0.3115 - precision: 0.3210 - recall: 0.4802 - subset_accuracy: 0.0275 - subset_f1: 0.3838 - subset_precision: 0.3213 - subset_recall: 0.4815\n","Epoch 1: Validation Metrics:\n","loss: 0.6857919692993164\n","val_label_wise_f1_score: [0.15384611 0.59999996 0.4705882  0.38888887 0.19999996]\n","val_label_wise_accuracy: [0.65625 0.625   0.71875 0.3125  0.75   ]\n","val_binary_accuracy: 0.5679166913032532\n","val_precision: 0.3525708317756653\n","val_recall: 0.4444444477558136\n","val_label_wise_macro_f1: 0.3491588532924652\n","val_subset_accuracy: 0.05000000074505806\n","val_subset_precision: 0.34062498807907104\n","val_subset_recall: 0.45309025049209595\n","val_subset_f1: 0.3877354562282562\n","val_auc: 0.5458770990371704\n","val_prc_auc: 0.3298894762992859\n","\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 142ms/step - auc: 0.5181 - binary_accuracy: 0.5165 - label_wise_accuracy: 0.5392 - label_wise_f1_score: 0.3871 - label_wise_macro_f1: 0.3740 - loss: 0.6923 - prc_auc: 0.3116 - precision: 0.3211 - recall: 0.4803 - subset_accuracy: 0.0276 - subset_f1: 0.3840 - subset_precision: 0.3215 - subset_recall: 0.4817 - val_auc: 0.5459 - val_binary_accuracy: 0.5679 - val_label_wise_accuracy: 0.6125 - val_label_wise_f1_score: 0.3627 - val_label_wise_macro_f1: 0.3492 - val_loss: 0.6779 - val_prc_auc: 0.3299 - val_precision: 0.3526 - val_recall: 0.4444 - val_subset_accuracy: 0.0500 - val_subset_f1: 0.3877 - val_subset_precision: 0.3406 - val_subset_recall: 0.4531\n","Epoch 2/15\n","\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - auc: 0.6020 - binary_accuracy: 0.6114 - label_wise_accuracy: 0.6218 - label_wise_f1_score: 0.4157 - label_wise_macro_f1: 0.4139 - loss: 0.6656 - prc_auc: 0.3753 - precision: 0.4049 - recall: 0.4991 - subset_accuracy: 0.0789 - subset_f1: 0.4538 - subset_precision: 0.4051 - subset_recall: 0.5214\n","Epoch 2: Validation Metrics:\n","loss: 0.6604307889938354\n","val_label_wise_f1_score: [0.         0.5454545  0.37499997 0.2941176  0.24999994]\n","val_label_wise_accuracy: [0.65625 0.6875  0.6875  0.25    0.8125 ]\n","val_binary_accuracy: 0.6016666293144226\n","val_precision: 0.37046632170677185\n","val_recall: 0.37830686569213867\n","val_label_wise_macro_f1: 0.3300512731075287\n","val_subset_accuracy: 0.08958332985639572\n","val_subset_precision: 0.34930554032325745\n","val_subset_recall: 0.39145833253860474\n","val_subset_f1: 0.3679051101207733\n","val_auc: 0.586551308631897\n","val_prc_auc: 0.39564943313598633\n","\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 83ms/step - auc: 0.6021 - binary_accuracy: 0.6115 - label_wise_accuracy: 0.6217 - label_wise_f1_score: 0.4148 - label_wise_macro_f1: 0.4139 - loss: 0.6655 - prc_auc: 0.3754 - precision: 0.4050 - recall: 0.4991 - subset_accuracy: 0.0790 - subset_f1: 0.4538 - subset_precision: 0.4052 - subset_recall: 0.5213 - val_auc: 0.5866 - val_binary_accuracy: 0.6017 - val_label_wise_accuracy: 0.6187 - val_label_wise_f1_score: 0.2929 - val_label_wise_macro_f1: 0.3301 - val_loss: 0.6586 - val_prc_auc: 0.3956 - val_precision: 0.3705 - val_recall: 0.3783 - val_subset_accuracy: 0.0896 - val_subset_f1: 0.3679 - val_subset_precision: 0.3493 - val_subset_recall: 0.3915\n","Epoch 3/15\n","\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - auc: 0.6645 - binary_accuracy: 0.6653 - label_wise_accuracy: 0.6795 - label_wise_f1_score: 0.4009 - label_wise_macro_f1: 0.4035 - loss: 0.6429 - prc_auc: 0.5126 - precision: 0.4665 - recall: 0.4334 - subset_accuracy: 0.1353 - subset_f1: 0.4544 - subset_precision: 0.4478 - subset_recall: 0.4662\n","Epoch 3: Validation Metrics:\n","loss: 0.6390367746353149\n","val_label_wise_f1_score: [0.         0.5714285  0.26666662 0.1333333  0.24999994]\n","val_label_wise_accuracy: [0.75    0.71875 0.65625 0.59375 0.8125 ]\n","val_binary_accuracy: 0.6641666889190674\n","val_precision: 0.4431818127632141\n","val_recall: 0.2579365074634552\n","val_label_wise_macro_f1: 0.2764054834842682\n","val_subset_accuracy: 0.11041666567325592\n","val_subset_precision: 0.33090272545814514\n","val_subset_recall: 0.2759374976158142\n","val_subset_f1: 0.29964175820350647\n","val_auc: 0.6114357709884644\n","val_prc_auc: 0.4431091845035553\n","\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 83ms/step - auc: 0.6646 - binary_accuracy: 0.6654 - label_wise_accuracy: 0.6797 - label_wise_f1_score: 0.3998 - label_wise_macro_f1: 0.4035 - loss: 0.6429 - prc_auc: 0.5127 - precision: 0.4667 - recall: 0.4332 - subset_accuracy: 0.1354 - subset_f1: 0.4544 - subset_precision: 0.4479 - subset_recall: 0.4660 - val_auc: 0.6114 - val_binary_accuracy: 0.6642 - val_label_wise_accuracy: 0.7063 - val_label_wise_f1_score: 0.2443 - val_label_wise_macro_f1: 0.2764 - val_loss: 0.6428 - val_prc_auc: 0.4431 - val_precision: 0.4432 - val_recall: 0.2579 - val_subset_accuracy: 0.1104 - val_subset_f1: 0.2996 - val_subset_precision: 0.3309 - val_subset_recall: 0.2759\n","Epoch 4/15\n","\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - auc: 0.6934 - binary_accuracy: 0.7199 - label_wise_accuracy: 0.7245 - label_wise_f1_score: 0.3512 - label_wise_macro_f1: 0.3570 - loss: 0.6238 - prc_auc: 0.5710 - precision: 0.6028 - recall: 0.3252 - subset_accuracy: 0.1793 - subset_f1: 0.3937 - subset_precision: 0.4346 - subset_recall: 0.3627\n","Epoch 4: Validation Metrics:\n","loss: 0.6210464239120483\n","val_label_wise_f1_score: [0.         0.5555555  0.37499997 0.         0.24999994]\n","val_label_wise_accuracy: [0.75    0.75    0.6875  0.78125 0.8125 ]\n","val_binary_accuracy: 0.7004167437553406\n","val_precision: 0.5649122595787048\n","val_recall: 0.21296297013759613\n","val_label_wise_macro_f1: 0.23744307458400726\n","val_subset_accuracy: 0.13333334028720856\n","val_subset_precision: 0.3053818941116333\n","val_subset_recall: 0.2323611080646515\n","val_subset_f1: 0.2630937695503235\n","val_auc: 0.6282019019126892\n","val_prc_auc: 0.46002691984176636\n","\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 84ms/step - auc: 0.6935 - binary_accuracy: 0.7200 - label_wise_accuracy: 0.7247 - label_wise_f1_score: 0.3503 - label_wise_macro_f1: 0.3570 - loss: 0.6238 - prc_auc: 0.5710 - precision: 0.6030 - recall: 0.3251 - subset_accuracy: 0.1793 - subset_f1: 0.3937 - subset_precision: 0.4345 - subset_recall: 0.3626 - val_auc: 0.6282 - val_binary_accuracy: 0.7004 - val_label_wise_accuracy: 0.7563 - val_label_wise_f1_score: 0.2361 - val_label_wise_macro_f1: 0.2374 - val_loss: 0.6300 - val_prc_auc: 0.4600 - val_precision: 0.5649 - val_recall: 0.2130 - val_subset_accuracy: 0.1333 - val_subset_f1: 0.2631 - val_subset_precision: 0.3054 - val_subset_recall: 0.2324\n","Model saved to /content/drive/Othercomputers/ASUS Main/MLCP/05_MODELS/01_Custom_Models/custom_model_20250209_183902.weights.h5\n","Using the trained model\n","\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 76ms/step - auc: 0.5418 - binary_accuracy: 0.5614 - label_wise_accuracy: 0.5625 - label_wise_f1_score: 0.3607 - label_wise_macro_f1: 0.3620 - loss: 0.6789 - prc_auc: 0.3347 - precision: 0.3570 - recall: 0.4432 - subset_accuracy: 0.0667 - subset_f1: 0.3977 - subset_precision: 0.3486 - subset_recall: 0.4664\n","Evaluation Metrics:\n","Loss: 0.6780438423156738\n","Label F1 Scores: [0.         0.56249994 0.30769226 0.3589743  0.39999998]\n","Label Accuracies: [0.46875 0.5625  0.71875 0.21875 0.8125 ]\n","Accuracy: 0.5626645088195801\n","Precision: 0.35477346181869507\n","Recall: 0.4518289566040039\n","F1 Score: 0.3615792691707611\n","Subset Accuracy: 0.05674342066049576\n","Subset Precision: 0.34731361269950867\n","Subset Recall: 0.47086068987846375\n","Subset F1: 0.3982698321342468\n","AUC: 0.5455693006515503\n","PRC AUC: 0.3318568468093872\n"]}],"source":["# local application/library specific imports\n","from app_src import TransformerEvaluator\n","from app_config import AppConfig\n","\n","# define configuration proxy\n","configProxy = AppConfig()\n","CONFIG = configProxy.return_config()\n","\n","transformer_evaluator = TransformerEvaluator()\n","transformer_evaluator.evaluate_models(\n","    epochs=15,\n","    batch_size=32,\n","    number_of_tags=5,\n","    train_model=True,\n","    threshold=0.5,\n","    transformer_model_path=os.path.join(CONFIG[\"TRANSFORMER_SAVE_PATH_ROOT\"], 'transformer_model_20250208_155506_5_epochs')\n","  )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":495627,"status":"ok","timestamp":1738627541119,"user":{"displayName":"Dinu Ion George","userId":"14034259797438922081"},"user_tz":-120},"id":"QNraNQLMG2Ga","outputId":"5c208d8d-1b71-4a42-84e8-676813dfe553"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n","Dask dataframe query planning is disabled because dask-expr is not installed.\n","\n","You can install it with `pip install dask[dataframe]` or `conda install dask`.\n","This will raise in a future version.\n","\n","  warnings.warn(msg, FutureWarning)\n","/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","All model checkpoint layers were used when initializing TFMPNetModel.\n","\n","All the layers of TFMPNetModel were initialized from the model checkpoint at /content/drive/Othercomputers/ASUS Main/MLCP/05_MODELS/02_Transformer_Models/transformer_model_20250203_225321.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMPNetModel for predictions without further training.\n","/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adamw', because it has 2 variables whereas the saved optimizer has 6 variables. \n","  saveable.load_own_variables(weights_store.get(inner_path))\n"]},{"name":"stdout","output_type":"stream","text":["Loaded transformer model from: /content/drive/Othercomputers/ASUS Main/MLCP/05_MODELS/02_Transformer_Models/transformer_model_20250203_225321\n","Using GPU\n"]},{"name":"stderr","output_type":"stream","text":["Encoding problem statements: 100%|██████████| 158/158 [03:28<00:00,  1.32s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Using GPU\n"]},{"name":"stderr","output_type":"stream","text":["Encoding problem statements: 100%|██████████| 44/44 [00:57<00:00,  1.32s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Benchmarking estimator model: LogisticRegression\n","Benchmarking estimator model: KNeighborsClassifier\n","Benchmarking estimator model: DecisionTreeClassifier\n","Benchmarking estimator model: GaussianNB\n","Benchmarking estimator model: RandomForestClassifier\n","Benchmarking estimator model: XGBClassifier\n","Benchmarking estimator model: LGBMClassifier\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 2241, number of negative: 2784\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025515 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 195840\n","[LightGBM] [Info] Number of data points in the train set: 5025, number of used features: 768\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445970 -> initscore=-0.216967\n","[LightGBM] [Info] Start training from score -0.216967\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 1979, number of negative: 3046\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015391 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 195842\n","[LightGBM] [Info] Number of data points in the train set: 5025, number of used features: 769\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.393831 -> initscore=-0.431238\n","[LightGBM] [Info] Start training from score -0.431238\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 1883, number of negative: 3142\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014327 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 195844\n","[LightGBM] [Info] Number of data points in the train set: 5025, number of used features: 770\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374726 -> initscore=-0.511993\n","[LightGBM] [Info] Start training from score -0.511993\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 1849, number of negative: 3176\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014668 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 195846\n","[LightGBM] [Info] Number of data points in the train set: 5025, number of used features: 771\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.367960 -> initscore=-0.540978\n","[LightGBM] [Info] Start training from score -0.540978\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 1677, number of negative: 3348\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022198 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 195848\n","[LightGBM] [Info] Number of data points in the train set: 5025, number of used features: 772\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.333731 -> initscore=-0.691357\n","[LightGBM] [Info] Start training from score -0.691357\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Benchmarking estimator model: SVC\n"]}],"source":["import os\n","\n","# local application/library specific imports\n","from app_src import SentenceTransformerWrapper\n","from app_config import AppConfig\n","\n","# define configuration proxy\n","configProxy = AppConfig()\n","CONFIG = configProxy.return_config()\n","\n","from app_src import DecisionTreeEvaluator\n","\n","decisionTreeEvaluator = DecisionTreeEvaluator()\n","decisionTreeEvaluator.benchmark_model(\n","    encoder_batch_size=32,\n","    number_of_tags=5,\n","    transformer_name='microsoft/mpnet-base',\n","    model_path= os.path.join(CONFIG[\"MODEL_SAVE_PATH_ROOT\"], 'custom_model_20250203_225321.weights.h5'),\n","    transformer_model_path= os.path.join(CONFIG[\"TRANSFORMER_SAVE_PATH_ROOT\"], 'transformer_model_20250203_225321')\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["a76d43f7e247423d8bd0267409172ae1","8ef25127aea24b65aed075efbf782bd9","4905559eaa5b40779fa2d579308ddeb3","ec09a2d650af46328f1be4ad121ed09f","42f3658a4ec241fc93c783ee84caa9cf","aa274429fd264ada9935779eaa1ca354","faaa9d56430f4cd1854b1775bc3172ba","baa06b0ec2d34d02b113f8503fddc216","d441459d1c884b52b9772285b2fd17f0","9c4cd35bd3c64007a9744c871f60f686","b1ae715e675d45c2957a160069ac7d08","76c7ede6ff5e462a8312821bad1a8202","4dc6ae2f526f4653a54c0c07381768db","c42874edb92e43cca983af83dc96420b","4bdb5352724c40eb9d493cb1efd743d1","d4da583e676f4d08a7e4356e6c19f394","6812d035d701491cb605814f86bcbb3e","4549d9c099ab48e88afcbb1d8f4c789d","224fb6448d974adc8c89a7e5d92893f2","ca45baf4c67c4a679a7300e9fb6e18d3","ae478ac2bfec4d3694f2550a6e7d28e8","1969c0b6db9c4f47b22607038a7b1a79","37822c1dea7c4066891fe9dc2bfad9de","0607fc3d5ee2416ea523bd5cce0e8c80","4b42397cb6fa4f13a52fcdfc4e7d7095","115fb8c0adf84e5687cdfecaddc7fc88","b469b0c304d84cbe9467cc76204428c5","15ff464abd8f4c98bcacb672569a8dc6","824bdcb736db49b0b01e101f3d4bbe32","dbca1f857e064aeeabac2f8eac9c9ba7","ae0bdf64c6fe474ba4b361a10d3406ab","5a2a9545b11644b6a7c683c3baa92760","54739fd4f13d4b128a24de6e70c503f3","cd6805a6b4bc490bb3dabb61504ab8e6","fed591da0d304c0f8b320ff464c5666c","2c6bd7e97ea2414aac33c456b046e873","6dd438058253463ab15a65ad8f33192f","1872502f5bbf42d1827f773b9bcd37f6","eee4ebb41d904ad7a3084540c6b2f33a","229551aa84fd4c7787adcc867eabba53","a75a543c4a414a9e98646af48b6bbe80","c77a74539b4a4ca7a56aae18c674362e","67b0ded56d4342698ab40156896dbef3","da87376fe355463386e1bcae2d6146d4","f394cc47b76c424ea50f39894d33a95e","ae139a3cd6614bdb8b13997dfa0926c7","e3067982ccdd4f4b9e25d084782b196b","b2026bb57acb43ccabb5254e5a74b4d6","5c535e651f1c4adcbaa60791742af937","25c1221c444c4c6e81dc25e8ab06143f","359a25604e64401bba227185ae1c76c6","e2efb55c88a44d24941853097ec4a60d","bcb1e5270d0b4d76ad75e7f33f15895a","18c24b0b760a4b5e8dfcbace68fd7469","f9642ec2fd524b17a5a1ce5f4f5c1ee1","64e547717c224c02981da5a4ff0462a4","810f6376fd9e4b4589f1d76bdbbba698","65b9b5ea95334da8a0519f61e9dd4c8b","3a0217664365498e81c1375d11696089","a763decf210b4bb19269b66251f8ff6f","36965743912a4bbc83a05b3c55b9e544","fcb2b70818dd4586bc146ccf99f08869","b654486cb19443ec88cba4f74f18da7b","ece2b16bff0d42e1a080df7ff743a59d","42f47dbea5d44dff9749627569b344e3","afd7b77b1b5e41b9aa344ef3d4af6eee"]},"executionInfo":{"elapsed":570286,"status":"ok","timestamp":1738962694527,"user":{"displayName":"Dinu Ion George","userId":"14034259797438922081"},"user_tz":-120},"id":"P780gFGDJfn_","outputId":"d4957257-75fd-4827-f2ea-e6d532f9d1be"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n","Dask dataframe query planning is disabled because dask-expr is not installed.\n","\n","You can install it with `pip install dask[dataframe]` or `conda install dask`.\n","This will raise in a future version.\n","\n","  warnings.warn(msg, FutureWarning)\n","/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a76d43f7e247423d8bd0267409172ae1","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"76c7ede6ff5e462a8312821bad1a8202","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"37822c1dea7c4066891fe9dc2bfad9de","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cd6805a6b4bc490bb3dabb61504ab8e6","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f394cc47b76c424ea50f39894d33a95e","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"64e547717c224c02981da5a4ff0462a4","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFMPNetModel: ['embeddings.position_ids']\n","- This IS expected if you are initializing TFMPNetModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFMPNetModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFMPNetModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMPNetModel for predictions without further training.\n"]},{"name":"stdout","output_type":"stream","text":["Loaded transformer model: sentence-transformers/all-mpnet-base-v2\n","Using GPU\n"]},{"name":"stderr","output_type":"stream","text":["Encoding problem statements: 100%|██████████| 279/279 [01:04<00:00,  4.29it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Using GPU\n"]},{"name":"stderr","output_type":"stream","text":["Encoding problem statements: 100%|██████████| 39/39 [00:08<00:00,  4.53it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Benchmarking estimator model: LogisticRegression\n","Benchmarking estimator model: KNeighborsClassifier\n","Benchmarking estimator model: DecisionTreeClassifier\n","Benchmarking estimator model: GaussianNB\n","Benchmarking estimator model: RandomForestClassifier\n","Benchmarking estimator model: XGBClassifier\n","Benchmarking estimator model: LGBMClassifier\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 3400, number of negative: 5516\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048537 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 195840\n","[LightGBM] [Info] Number of data points in the train set: 8916, number of used features: 768\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.381337 -> initscore=-0.483878\n","[LightGBM] [Info] Start training from score -0.483878\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 3334, number of negative: 5582\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024986 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 195842\n","[LightGBM] [Info] Number of data points in the train set: 8916, number of used features: 769\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.373934 -> initscore=-0.515374\n","[LightGBM] [Info] Start training from score -0.515374\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 2998, number of negative: 5918\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022292 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 195844\n","[LightGBM] [Info] Number of data points in the train set: 8916, number of used features: 770\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.336249 -> initscore=-0.680053\n","[LightGBM] [Info] Start training from score -0.680053\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 2390, number of negative: 6526\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022028 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 195846\n","[LightGBM] [Info] Number of data points in the train set: 8916, number of used features: 771\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.268057 -> initscore=-1.004501\n","[LightGBM] [Info] Start training from score -1.004501\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 2024, number of negative: 6892\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020946 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 195848\n","[LightGBM] [Info] Number of data points in the train set: 8916, number of used features: 772\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.227008 -> initscore=-1.225286\n","[LightGBM] [Info] Start training from score -1.225286\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Benchmarking estimator model: SVC\n"]}],"source":["import os\n","\n","# local application/library specific imports\n","from app_src import SentenceTransformerWrapper\n","from app_config import AppConfig\n","\n","# define configuration proxy\n","configProxy = AppConfig()\n","CONFIG = configProxy.return_config()\n","\n","from app_src import DecisionTreeEvaluator\n","\n","decisionTreeEvaluator = DecisionTreeEvaluator()\n","decisionTreeEvaluator.benchmark_model(\n","    encoder_batch_size=32,\n","    number_of_tags=5,\n","    transformer_name='sentence-transformers/all-mpnet-base-v2'\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":230748,"status":"ok","timestamp":1738956382290,"user":{"displayName":"Dinu Ion George","userId":"14034259797438922081"},"user_tz":-120},"id":"MNyOaKubFYPY","outputId":"8b8ad66c-8d94-4bca-cf4c-ba4905b43071"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFMPNetModel: ['embeddings.position_ids']\n","- This IS expected if you are initializing TFMPNetModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFMPNetModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFMPNetModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMPNetModel for predictions without further training.\n"]},{"name":"stdout","output_type":"stream","text":["Loaded transformer model: sentence-transformers/all-mpnet-base-v2\n","Using GPU\n"]},{"name":"stderr","output_type":"stream","text":["Encoding problem statements: 100%|██████████| 140/140 [00:30<00:00,  4.63it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Using GPU\n"]},{"name":"stderr","output_type":"stream","text":["Encoding problem statements: 100%|██████████| 39/39 [00:08<00:00,  4.68it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Benchmarking estimator model: LogisticRegression\n","Benchmarking estimator model: KNeighborsClassifier\n","Benchmarking estimator model: DecisionTreeClassifier\n","Benchmarking estimator model: GaussianNB\n","Benchmarking estimator model: RandomForestClassifier\n","Benchmarking estimator model: XGBClassifier\n","Benchmarking estimator model: LGBMClassifier\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 1700, number of negative: 2758\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014652 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 195840\n","[LightGBM] [Info] Number of data points in the train set: 4458, number of used features: 768\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.381337 -> initscore=-0.483878\n","[LightGBM] [Info] Start training from score -0.483878\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 1667, number of negative: 2791\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012859 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 195840\n","[LightGBM] [Info] Number of data points in the train set: 4458, number of used features: 768\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.373934 -> initscore=-0.515374\n","[LightGBM] [Info] Start training from score -0.515374\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 1499, number of negative: 2959\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021165 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 195840\n","[LightGBM] [Info] Number of data points in the train set: 4458, number of used features: 768\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.336249 -> initscore=-0.680053\n","[LightGBM] [Info] Start training from score -0.680053\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 1195, number of negative: 3263\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012665 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 195840\n","[LightGBM] [Info] Number of data points in the train set: 4458, number of used features: 768\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.268057 -> initscore=-1.004501\n","[LightGBM] [Info] Start training from score -1.004501\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 1012, number of negative: 3446\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012527 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 195840\n","[LightGBM] [Info] Number of data points in the train set: 4458, number of used features: 768\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.227008 -> initscore=-1.225286\n","[LightGBM] [Info] Start training from score -1.225286\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Benchmarking estimator model: SVC\n"]}],"source":["import os\n","\n","# local application/library specific imports\n","from app_config import AppConfig\n","\n","# define configuration proxy\n","configProxy = AppConfig()\n","CONFIG = configProxy.return_config()\n","\n","from app_src import OneVsAllDecisionTreeEvaluator\n","\n","decisionTreeEvaluator = OneVsAllDecisionTreeEvaluator()\n","decisionTreeEvaluator.benchmark_model(\n","    encoder_batch_size=32,\n","    number_of_tags=5,\n","    transformer_name='sentence-transformers/all-mpnet-base-v2'\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":211836,"status":"ok","timestamp":1738956594129,"user":{"displayName":"Dinu Ion George","userId":"14034259797438922081"},"user_tz":-120},"id":"PHgqiOcBdQh_","outputId":"e2435a46-4a1b-4fc4-b144-318752f8cbf4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training and evaluating model: sentence-transformers/all-mpnet-base-v2\n","Starting training for label: 0\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFMPNetModel: ['embeddings.position_ids']\n","- This IS expected if you are initializing TFMPNetModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFMPNetModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFMPNetModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMPNetModel for predictions without further training.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - auc: 0.4729 - binary_accuracy: 0.5651 - f1: 0.2521 - loss: 0.6864 - prc_auc: 0.3631 - precision: 0.3538 - recall: 0.1969\n","Epoch 1: Validation Metrics:\n","loss: 0.686181902885437\n","val_binary_accuracy: 0.5854166746139526\n","val_precision: 0.40625\n","val_recall: 0.13903743028640747\n","val_auc: 0.46750378608703613\n","val_prc_auc: 0.38557296991348267\n","\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 117ms/step - auc: 0.4729 - binary_accuracy: 0.5651 - f1: 0.2519 - loss: 0.6864 - prc_auc: 0.3631 - precision: 0.3538 - recall: 0.1966 - val_auc: 0.4675 - val_binary_accuracy: 0.5854 - val_f1: 0.2072 - val_loss: 0.6849 - val_prc_auc: 0.3856 - val_precision: 0.4062 - val_recall: 0.1390\n","Starting training for label: 1\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFMPNetModel: ['embeddings.position_ids']\n","- This IS expected if you are initializing TFMPNetModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFMPNetModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFMPNetModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMPNetModel for predictions without further training.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - auc: 0.5482 - binary_accuracy: 0.4909 - f1: 0.4986 - loss: 0.6955 - prc_auc: 0.4098 - precision: 0.3935 - recall: 0.6811\n","Epoch 1: Validation Metrics:\n","loss: 0.6955739855766296\n","val_binary_accuracy: 0.5395833253860474\n","val_precision: 0.4117647111415863\n","val_recall: 0.5474860072135925\n","val_auc: 0.5642551183700562\n","val_prc_auc: 0.4129665791988373\n","\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 112ms/step - auc: 0.5481 - binary_accuracy: 0.4909 - f1: 0.4985 - loss: 0.6955 - prc_auc: 0.4097 - precision: 0.3935 - recall: 0.6807 - val_auc: 0.5643 - val_binary_accuracy: 0.5396 - val_f1: 0.4700 - val_loss: 0.6895 - val_prc_auc: 0.4130 - val_precision: 0.4118 - val_recall: 0.5475\n","Starting training for label: 2\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFMPNetModel: ['embeddings.position_ids']\n","- This IS expected if you are initializing TFMPNetModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFMPNetModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFMPNetModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMPNetModel for predictions without further training.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - auc: 0.5426 - binary_accuracy: 0.5497 - f1: 0.4158 - loss: 0.6886 - prc_auc: 0.3782 - precision: 0.3669 - recall: 0.4815\n","Epoch 1: Validation Metrics:\n","loss: 0.6873651742935181\n","val_binary_accuracy: 0.5791666507720947\n","val_precision: 0.39436620473861694\n","val_recall: 0.3255814015865326\n","val_auc: 0.5228216648101807\n","val_prc_auc: 0.38145366311073303\n","\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 113ms/step - auc: 0.5425 - binary_accuracy: 0.5498 - f1: 0.4157 - loss: 0.6886 - prc_auc: 0.3782 - precision: 0.3669 - recall: 0.4811 - val_auc: 0.5228 - val_binary_accuracy: 0.5792 - val_f1: 0.3567 - val_loss: 0.6860 - val_prc_auc: 0.3815 - val_precision: 0.3944 - val_recall: 0.3256\n","Starting training for label: 3\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFMPNetModel: ['embeddings.position_ids']\n","- This IS expected if you are initializing TFMPNetModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFMPNetModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFMPNetModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMPNetModel for predictions without further training.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - auc: 0.4647 - binary_accuracy: 0.4365 - f1: 0.3634 - loss: 0.7060 - prc_auc: 0.2438 - precision: 0.2610 - recall: 0.6001\n","Epoch 1: Validation Metrics:\n","loss: 0.7018375992774963\n","val_binary_accuracy: 0.48750001192092896\n","val_precision: 0.22466960549354553\n","val_recall: 0.42148759961128235\n","val_auc: 0.4521167576313019\n","val_prc_auc: 0.21609055995941162\n","\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 113ms/step - auc: 0.4647 - binary_accuracy: 0.4367 - f1: 0.3632 - loss: 0.7060 - prc_auc: 0.2438 - precision: 0.2609 - recall: 0.5996 - val_auc: 0.4521 - val_binary_accuracy: 0.4875 - val_f1: 0.2931 - val_loss: 0.6973 - val_prc_auc: 0.2161 - val_precision: 0.2247 - val_recall: 0.4215\n","Starting training for label: 4\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFMPNetModel: ['embeddings.position_ids']\n","- This IS expected if you are initializing TFMPNetModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFMPNetModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFMPNetModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMPNetModel for predictions without further training.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - auc: 0.4654 - binary_accuracy: 0.6132 - f1: 0.2177 - loss: 0.6776 - prc_auc: 0.2102 - precision: 0.2005 - recall: 0.2400\n","Epoch 1: Validation Metrics:\n","loss: 0.6737685799598694\n","val_binary_accuracy: 0.6833333373069763\n","val_precision: 0.11267605423927307\n","val_recall: 0.0824742242693901\n","val_auc: 0.46710723638534546\n","val_prc_auc: 0.177982896566391\n","\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 112ms/step - auc: 0.4654 - binary_accuracy: 0.6133 - f1: 0.2176 - loss: 0.6775 - prc_auc: 0.2101 - precision: 0.2005 - recall: 0.2397 - val_auc: 0.4671 - val_binary_accuracy: 0.6833 - val_f1: 0.0952 - val_loss: 0.6616 - val_prc_auc: 0.1780 - val_precision: 0.1127 - val_recall: 0.0825\n","\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 249ms/step\n","\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 203ms/step\n","\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 205ms/step\n","\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 204ms/step\n","\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 205ms/step\n","[[0 1 1 0 0]\n"," [0 0 1 0 0]\n"," [0 0 1 0 0]\n"," ...\n"," [0 1 0 1 0]\n"," [0 1 0 0 0]\n"," [0 1 0 1 0]]\n","[[0 0 1 0 0]\n"," [0 1 1 0 0]\n"," [0 1 1 0 0]\n"," ...\n"," [0 0 0 1 1]\n"," [0 0 1 0 0]\n"," [1 1 1 0 0]]\n","test_tags_np shape: (1239, 5)\n","predictions shape: (1239, 5)\n"]}],"source":["# local application/library specific imports\n","from app_src import OneVsAllTransformerEvaluator\n","from app_config import AppConfig\n","\n","# define configuration proxy\n","configProxy = AppConfig()\n","CONFIG = configProxy.return_config()\n","\n","transformer_evaluator = OneVsAllTransformerEvaluator()\n","transformer_evaluator.evaluate_models(\n","    epochs=1,\n","    batch_size=32,\n","    number_of_tags=5,\n","    train_model=True,\n","    threshold=0.5\n","  )\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2804517,"status":"ok","timestamp":1739147305862,"user":{"displayName":"Dinu Ion George","userId":"14034259797438922081"},"user_tz":-120},"id":"JRO_Vzk2MtcV","outputId":"5d314b69-575f-4841-a235-0c1170ca0978"},"outputs":[{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFMPNetModel.\n","\n","All the layers of TFMPNetModel were initialized from the model checkpoint at /content/drive/Othercomputers/ASUS Main/MLCP/05_MODELS/02_Transformer_Models/transformer_model_20250209_231314_1_epoch_full_nli.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMPNetModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/4\n","Anchor Embeddings Shape: (16, 768)\n","Positive Embeddings Shape: (16, 768)\n","Negative Embeddings Shape: (16, 768)\n","Loss Value: Tensor(\"Mean:0\", shape=(), dtype=float32)\n","[<tf.Variable 'tfmp_net_model_1/mpnet/encoder/relative_attention_bias/embeddings:0' shape=(32, 12) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/pooler/dense/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/pooler/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/embeddings/word_embeddings/weight:0' shape=(30527, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/embeddings/position_embeddings/embeddings:0' shape=(514, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/embeddings/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/embeddings/LayerNorm/beta:0' shape=(768,) dtype=float32>]\n","Gradients: [<tensorflow.python.framework.indexed_slices.IndexedSlices object at 0x7dcfbff8bd90>, <tf.Tensor 'AddN_268:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_269:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_270:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_271:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_272:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_273:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_274:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_275:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_276:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_277:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_278:0' shape=(768, 3072) dtype=float32>, <tf.Tensor 'AddN_279:0' shape=(3072,) dtype=float32>, <tf.Tensor 'AddN_280:0' shape=(3072, 768) dtype=float32>, <tf.Tensor 'AddN_281:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_282:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_283:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_284:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_285:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_286:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_287:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_288:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_289:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_290:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_291:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_292:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_293:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_294:0' shape=(768, 3072) dtype=float32>, <tf.Tensor 'AddN_295:0' shape=(3072,) dtype=float32>, <tf.Tensor 'AddN_296:0' shape=(3072, 768) dtype=float32>, <tf.Tensor 'AddN_297:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_298:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_299:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_300:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_301:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_302:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_303:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_304:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_305:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_306:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_307:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_308:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_309:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_310:0' shape=(768, 3072) dtype=float32>, <tf.Tensor 'AddN_311:0' shape=(3072,) dtype=float32>, <tf.Tensor 'AddN_312:0' shape=(3072, 768) dtype=float32>, <tf.Tensor 'AddN_313:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_314:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_315:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_316:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_317:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_318:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_319:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_320:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_321:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_322:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_323:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_324:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_325:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_326:0' shape=(768, 3072) dtype=float32>, <tf.Tensor 'AddN_327:0' shape=(3072,) dtype=float32>, <tf.Tensor 'AddN_328:0' shape=(3072, 768) dtype=float32>, <tf.Tensor 'AddN_329:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_330:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_331:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_332:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_333:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_334:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_335:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_336:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_337:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_338:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_339:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_340:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_341:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_342:0' shape=(768, 3072) dtype=float32>, <tf.Tensor 'AddN_343:0' shape=(3072,) dtype=float32>, <tf.Tensor 'AddN_344:0' shape=(3072, 768) dtype=float32>, <tf.Tensor 'AddN_345:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_346:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_347:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_348:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_349:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_350:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_351:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_352:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_353:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_354:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_355:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_356:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_357:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_358:0' shape=(768, 3072) dtype=float32>, <tf.Tensor 'AddN_359:0' shape=(3072,) dtype=float32>, <tf.Tensor 'AddN_360:0' shape=(3072, 768) dtype=float32>, <tf.Tensor 'AddN_361:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_362:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_363:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_364:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_365:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_366:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_367:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_368:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_369:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_370:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_371:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_372:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_373:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_374:0' shape=(768, 3072) dtype=float32>, <tf.Tensor 'AddN_375:0' shape=(3072,) dtype=float32>, <tf.Tensor 'AddN_376:0' shape=(3072, 768) dtype=float32>, <tf.Tensor 'AddN_377:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_378:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_379:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_380:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_381:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_382:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_383:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_384:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_385:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_386:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_387:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_388:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_389:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_390:0' shape=(768, 3072) dtype=float32>, <tf.Tensor 'AddN_391:0' shape=(3072,) dtype=float32>, <tf.Tensor 'AddN_392:0' shape=(3072, 768) dtype=float32>, <tf.Tensor 'AddN_393:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_394:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_395:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_396:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_397:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_398:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_399:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_400:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_401:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_402:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_403:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_404:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_405:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_406:0' shape=(768, 3072) dtype=float32>, <tf.Tensor 'AddN_407:0' shape=(3072,) dtype=float32>, <tf.Tensor 'AddN_408:0' shape=(3072, 768) dtype=float32>, <tf.Tensor 'AddN_409:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_410:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_411:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_412:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_413:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_414:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_415:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_416:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_417:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_418:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_419:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_420:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_421:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_422:0' shape=(768, 3072) dtype=float32>, <tf.Tensor 'AddN_423:0' shape=(3072,) dtype=float32>, <tf.Tensor 'AddN_424:0' shape=(3072, 768) dtype=float32>, <tf.Tensor 'AddN_425:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_426:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_427:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_428:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_429:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_430:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_431:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_432:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_433:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_434:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_435:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_436:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_437:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_438:0' shape=(768, 3072) dtype=float32>, <tf.Tensor 'AddN_439:0' shape=(3072,) dtype=float32>, <tf.Tensor 'AddN_440:0' shape=(3072, 768) dtype=float32>, <tf.Tensor 'AddN_441:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_442:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_443:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_444:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_445:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_446:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_447:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_448:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_449:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_450:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_451:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_452:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_453:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_454:0' shape=(768, 3072) dtype=float32>, <tf.Tensor 'AddN_455:0' shape=(3072,) dtype=float32>, <tf.Tensor 'AddN_456:0' shape=(3072, 768) dtype=float32>, <tf.Tensor 'AddN_457:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_458:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_459:0' shape=(768,) dtype=float32>, None, None, <tensorflow.python.framework.indexed_slices.IndexedSlices object at 0x7dcdd5064d10>, <tensorflow.python.framework.indexed_slices.IndexedSlices object at 0x7dcfbb35f550>, <tf.Tensor 'AddN_460:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_461:0' shape=(768,) dtype=float32>]\n","Trainable Variables: [<tf.Variable 'tfmp_net_model_1/mpnet/encoder/relative_attention_bias/embeddings:0' shape=(32, 12) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/pooler/dense/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/pooler/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/embeddings/word_embeddings/weight:0' shape=(30527, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/embeddings/position_embeddings/embeddings:0' shape=(514, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/embeddings/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/embeddings/LayerNorm/beta:0' shape=(768,) dtype=float32>]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['tfmp_net_model_1/mpnet/pooler/dense/kernel:0', 'tfmp_net_model_1/mpnet/pooler/dense/bias:0'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Anchor Embeddings Shape: (16, 768)\n","Positive Embeddings Shape: (16, 768)\n","Negative Embeddings Shape: (16, 768)\n","Loss Value: Tensor(\"Mean:0\", shape=(), dtype=float32)\n","[<tf.Variable 'tfmp_net_model_1/mpnet/encoder/relative_attention_bias/embeddings:0' shape=(32, 12) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/pooler/dense/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/pooler/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/embeddings/word_embeddings/weight:0' shape=(30527, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/embeddings/position_embeddings/embeddings:0' shape=(514, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/embeddings/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/embeddings/LayerNorm/beta:0' shape=(768,) dtype=float32>]\n","Gradients: [<tensorflow.python.framework.indexed_slices.IndexedSlices object at 0x7dceb04d3ed0>, <tf.Tensor 'AddN_268:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_269:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_270:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_271:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_272:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_273:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_274:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_275:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_276:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_277:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_278:0' shape=(768, 3072) dtype=float32>, <tf.Tensor 'AddN_279:0' shape=(3072,) dtype=float32>, <tf.Tensor 'AddN_280:0' shape=(3072, 768) dtype=float32>, <tf.Tensor 'AddN_281:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_282:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_283:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_284:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_285:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_286:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_287:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_288:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_289:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_290:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_291:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_292:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_293:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_294:0' shape=(768, 3072) dtype=float32>, <tf.Tensor 'AddN_295:0' shape=(3072,) dtype=float32>, <tf.Tensor 'AddN_296:0' shape=(3072, 768) dtype=float32>, <tf.Tensor 'AddN_297:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_298:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_299:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_300:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_301:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_302:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_303:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_304:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_305:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_306:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_307:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_308:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_309:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_310:0' shape=(768, 3072) dtype=float32>, <tf.Tensor 'AddN_311:0' shape=(3072,) dtype=float32>, <tf.Tensor 'AddN_312:0' shape=(3072, 768) dtype=float32>, <tf.Tensor 'AddN_313:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_314:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_315:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_316:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_317:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_318:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_319:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_320:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_321:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_322:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_323:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_324:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_325:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_326:0' shape=(768, 3072) dtype=float32>, <tf.Tensor 'AddN_327:0' shape=(3072,) dtype=float32>, <tf.Tensor 'AddN_328:0' shape=(3072, 768) dtype=float32>, <tf.Tensor 'AddN_329:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_330:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_331:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_332:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_333:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_334:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_335:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_336:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_337:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_338:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_339:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_340:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_341:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_342:0' shape=(768, 3072) dtype=float32>, <tf.Tensor 'AddN_343:0' shape=(3072,) dtype=float32>, <tf.Tensor 'AddN_344:0' shape=(3072, 768) dtype=float32>, <tf.Tensor 'AddN_345:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_346:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_347:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_348:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_349:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_350:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_351:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_352:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_353:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_354:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_355:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_356:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_357:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_358:0' shape=(768, 3072) dtype=float32>, <tf.Tensor 'AddN_359:0' shape=(3072,) dtype=float32>, <tf.Tensor 'AddN_360:0' shape=(3072, 768) dtype=float32>, <tf.Tensor 'AddN_361:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_362:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_363:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_364:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_365:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_366:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_367:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_368:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_369:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_370:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_371:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_372:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_373:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_374:0' shape=(768, 3072) dtype=float32>, <tf.Tensor 'AddN_375:0' shape=(3072,) dtype=float32>, <tf.Tensor 'AddN_376:0' shape=(3072, 768) dtype=float32>, <tf.Tensor 'AddN_377:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_378:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_379:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_380:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_381:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_382:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_383:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_384:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_385:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_386:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_387:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_388:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_389:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_390:0' shape=(768, 3072) dtype=float32>, <tf.Tensor 'AddN_391:0' shape=(3072,) dtype=float32>, <tf.Tensor 'AddN_392:0' shape=(3072, 768) dtype=float32>, <tf.Tensor 'AddN_393:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_394:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_395:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_396:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_397:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_398:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_399:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_400:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_401:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_402:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_403:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_404:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_405:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_406:0' shape=(768, 3072) dtype=float32>, <tf.Tensor 'AddN_407:0' shape=(3072,) dtype=float32>, <tf.Tensor 'AddN_408:0' shape=(3072, 768) dtype=float32>, <tf.Tensor 'AddN_409:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_410:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_411:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_412:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_413:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_414:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_415:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_416:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_417:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_418:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_419:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_420:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_421:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_422:0' shape=(768, 3072) dtype=float32>, <tf.Tensor 'AddN_423:0' shape=(3072,) dtype=float32>, <tf.Tensor 'AddN_424:0' shape=(3072, 768) dtype=float32>, <tf.Tensor 'AddN_425:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_426:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_427:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_428:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_429:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_430:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_431:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_432:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_433:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_434:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_435:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_436:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_437:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_438:0' shape=(768, 3072) dtype=float32>, <tf.Tensor 'AddN_439:0' shape=(3072,) dtype=float32>, <tf.Tensor 'AddN_440:0' shape=(3072, 768) dtype=float32>, <tf.Tensor 'AddN_441:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_442:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_443:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_444:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_445:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_446:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_447:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_448:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_449:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_450:0' shape=(768, 768) dtype=float32>, <tf.Tensor 'AddN_451:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_452:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_453:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_454:0' shape=(768, 3072) dtype=float32>, <tf.Tensor 'AddN_455:0' shape=(3072,) dtype=float32>, <tf.Tensor 'AddN_456:0' shape=(3072, 768) dtype=float32>, <tf.Tensor 'AddN_457:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_458:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_459:0' shape=(768,) dtype=float32>, None, None, <tensorflow.python.framework.indexed_slices.IndexedSlices object at 0x7dceb03ae710>, <tensorflow.python.framework.indexed_slices.IndexedSlices object at 0x7dce400ad750>, <tf.Tensor 'AddN_460:0' shape=(768,) dtype=float32>, <tf.Tensor 'AddN_461:0' shape=(768,) dtype=float32>]\n","Trainable Variables: [<tf.Variable 'tfmp_net_model_1/mpnet/encoder/relative_attention_bias/embeddings:0' shape=(32, 12) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._0/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._1/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._2/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._3/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._4/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._5/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._6/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._7/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._8/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._9/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._10/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/attention/attn/q/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/attention/attn/q/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/attention/attn/k/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/attention/attn/k/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/attention/attn/v/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/attention/attn/v/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/attention/attn/o/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/attention/attn/o/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/attention/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/attention/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/intermediate/dense/bias:0' shape=(3072,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/output/dense/kernel:0' shape=(3072, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/output/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/encoder/layer_._11/output/LayerNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/pooler/dense/kernel:0' shape=(768, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/pooler/dense/bias:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/embeddings/word_embeddings/weight:0' shape=(30527, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/embeddings/position_embeddings/embeddings:0' shape=(514, 768) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/embeddings/LayerNorm/gamma:0' shape=(768,) dtype=float32>, <tf.Variable 'tfmp_net_model_1/mpnet/embeddings/LayerNorm/beta:0' shape=(768,) dtype=float32>]\n","\u001b[1m1360/1360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m788s\u001b[0m 492ms/step - loss: 0.3641\n","Epoch 2/4\n","\u001b[1m1360/1360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m669s\u001b[0m 492ms/step - loss: 0.2938\n","Epoch 3/4\n","\u001b[1m1360/1360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m669s\u001b[0m 492ms/step - loss: 0.2198\n","Epoch 4/4\n","\u001b[1m1360/1360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m669s\u001b[0m 492ms/step - loss: 0.1725\n","Transformer model saved to /content/drive/Othercomputers/ASUS Main/MLCP/05_MODELS/02_Transformer_Models/transformer_model_20250209_234116\n"]}],"source":["# local application/library specific imports\n","from app_src import NLITransformerWrapper\n","from app_config import AppConfig\n","\n","# define configuration proxy\n","configProxy = AppConfig()\n","CONFIG = configProxy.return_config()\n","\n","nli_transformer_wrapper = NLITransformerWrapper('sentence-transformers/all-mpnet-base-v2')\n","nli_transformer_wrapper.train_model(\n","    train_dataset_path=CONFIG[f'NLI_TRAINING_DATASET_PATH'],\n","    epochs=4,\n","    batch_size=16,\n","    transformer_model_path=os.path.join(CONFIG[\"TRANSFORMER_SAVE_PATH_ROOT\"], 'transformer_model_20250209_231314_1_epoch_full_nli')\n","  )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":637},"executionInfo":{"elapsed":142731,"status":"error","timestamp":1739132626961,"user":{"displayName":"Dinu Ion George","userId":"14034259797438922081"},"user_tz":-120},"id":"ZxccKR9-LaVM","outputId":"ad2b0abe-77f1-463b-93d4-d3c54ebcf08c"},"outputs":[{"name":"stderr","output_type":"stream","text":["All model checkpoint layers were used when initializing TFMPNetModel.\n","\n","All the layers of TFMPNetModel were initialized from the model checkpoint at /content/drive/Othercomputers/ASUS Main/MLCP/05_MODELS/02_Transformer_Models/transformer_model_20250209_201638.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMPNetModel for predictions without further training.\n"]},{"name":"stdout","output_type":"stream","text":["Loaded transformer model from: /content/drive/Othercomputers/ASUS Main/MLCP/05_MODELS/02_Transformer_Models/transformer_model_20250209_201638\n","Using GPU\n"]},{"name":"stderr","output_type":"stream","text":["Encoding problem statements: 100%|██████████| 140/140 [00:31<00:00,  4.47it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Using GPU\n"]},{"name":"stderr","output_type":"stream","text":["Encoding problem statements: 100%|██████████| 39/39 [00:08<00:00,  4.66it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Benchmarking estimator model: LogisticRegression\n","Benchmarking estimator model: KNeighborsClassifier\n","Benchmarking estimator model: DecisionTreeClassifier\n","Benchmarking estimator model: GaussianNB\n","Benchmarking estimator model: RandomForestClassifier\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-b190db2c1167>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mdecisionTreeEvaluator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecisionTreeEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m decisionTreeEvaluator.benchmark_model(\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mencoder_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mnumber_of_tags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/Othercomputers/ASUS Main/MLCP/03_Classifier/app_src/DecisionTreeEvaluator.py\u001b[0m in \u001b[0;36mbenchmark_model\u001b[0;34m(self, encoder_batch_size, number_of_tags, validation, transformer_name, transformer_model_path)\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0mclassifierChainWrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_problem_statements\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_problem_statements\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0mclassifierChainWrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_problem_statements\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mmetrics_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifierChainWrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_problem_statements\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/Othercomputers/ASUS Main/MLCP/03_Classifier/app_src/ClassifierChainWrapper.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_problem_statements, train_tags, validation_problem_statements, validation_tags)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_problem_statements\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_problem_statements\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_tags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_problem_statements\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_problem_statements_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/skmultilearn/problem_transform/cc.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, order)\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0my_subset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_data_subset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             self.classifiers_[label] = self.classifier.fit(self._ensure_input_format(\n\u001b[0m\u001b[1;32m    155\u001b[0m                 X_extended), self._ensure_output_format(y_subset))\n\u001b[1;32m    156\u001b[0m             \u001b[0mX_extended\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_extended\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_subset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0;31m# since correctness does not rely on using threads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m             trees = Parallel(\n\u001b[0m\u001b[1;32m    488\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"balanced\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         tree._fit(\n\u001b[0m\u001b[1;32m    190\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    470\u001b[0m             )\n\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing_values_in_feature_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import os\n","\n","# local application/library specific imports\n","from app_src import SentenceTransformerWrapper\n","from app_config import AppConfig\n","\n","# define configuration proxy\n","configProxy = AppConfig()\n","CONFIG = configProxy.return_config()\n","\n","from app_src import DecisionTreeEvaluator\n","\n","decisionTreeEvaluator = DecisionTreeEvaluator()\n","decisionTreeEvaluator.benchmark_model(\n","    encoder_batch_size=32,\n","    number_of_tags=5,\n","    transformer_name='sentence-transformers/all-mpnet-base-v2',\n","    transformer_model_path=os.path.join(CONFIG[\"TRANSFORMER_SAVE_PATH_ROOT\"], 'transformer_model_20250209_231314')\n","    )"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":667283,"status":"ok","timestamp":1739149975059,"user":{"displayName":"Dinu Ion George","userId":"14034259797438922081"},"user_tz":-120},"id":"E67PUmrfEy0H","outputId":"319ed687-401f-4fab-bf7d-0dc4117bf6a1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training and evaluating model: sentence-transformers/all-mpnet-base-v2\n","Starting training for label: 0\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFMPNetModel.\n","\n","All the layers of TFMPNetModel were initialized from the model checkpoint at /content/drive/Othercomputers/ASUS Main/MLCP/05_MODELS/02_Transformer_Models/transformer_model_20250209_231314_1_epoch_full_nli.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMPNetModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/15\n","\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - auc: 0.3057 - binary_accuracy: 0.3769 - f1: 0.2542 - loss: 0.7191 - prc_auc: 0.2697 - precision: 0.2305 - recall: 0.2836\n","Epoch 1: Validation Metrics:\n","loss: 0.7124927639961243\n","val_binary_accuracy: 0.48991936445236206\n","val_precision: 0.3571428656578064\n","val_recall: 0.38860103487968445\n","val_auc: 0.45339351892471313\n","val_prc_auc: 0.3573424816131592\n","\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 179ms/step - auc: 0.3059 - binary_accuracy: 0.3770 - f1: 0.2542 - loss: 0.7191 - prc_auc: 0.2697 - precision: 0.2306 - recall: 0.2835 - val_auc: 0.4534 - val_binary_accuracy: 0.4899 - val_f1: 0.3722 - val_loss: 0.6943 - val_prc_auc: 0.3573 - val_precision: 0.3571 - val_recall: 0.3886\n","Epoch 2/15\n","\u001b[1m139/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - auc: 0.4579 - binary_accuracy: 0.4627 - f1: 0.3501 - loss: 0.6940 - prc_auc: 0.3493 - precision: 0.3211 - recall: 0.3858\n","Epoch 2: Validation Metrics:\n","loss: 0.6879702210426331\n","val_binary_accuracy: 0.6169354915618896\n","val_precision: 0.5095541477203369\n","val_recall: 0.41450777649879456\n","val_auc: 0.6026607751846313\n","val_prc_auc: 0.5098655223846436\n","\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 79ms/step - auc: 0.4588 - binary_accuracy: 0.4635 - f1: 0.3508 - loss: 0.6939 - prc_auc: 0.3500 - precision: 0.3219 - recall: 0.3861 - val_auc: 0.6027 - val_binary_accuracy: 0.6169 - val_f1: 0.4571 - val_loss: 0.6799 - val_prc_auc: 0.5099 - val_precision: 0.5096 - val_recall: 0.4145\n","Epoch 3/15\n","\u001b[1m139/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - auc: 0.6893 - binary_accuracy: 0.6812 - f1: 0.5584 - loss: 0.6721 - prc_auc: 0.5496 - precision: 0.5804 - recall: 0.5382\n","Epoch 3: Validation Metrics:\n","loss: 0.6665804386138916\n","val_binary_accuracy: 0.6612903475761414\n","val_precision: 0.5722543597221375\n","val_recall: 0.5129533410072327\n","val_auc: 0.6927187442779541\n","val_prc_auc: 0.5711369514465332\n","\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 79ms/step - auc: 0.6899 - binary_accuracy: 0.6815 - f1: 0.5589 - loss: 0.6720 - prc_auc: 0.5501 - precision: 0.5810 - recall: 0.5386 - val_auc: 0.6927 - val_binary_accuracy: 0.6613 - val_f1: 0.5410 - val_loss: 0.6679 - val_prc_auc: 0.5711 - val_precision: 0.5723 - val_recall: 0.5130\n","Epoch 4/15\n","\u001b[1m139/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - auc: 0.7915 - binary_accuracy: 0.7416 - f1: 0.6478 - loss: 0.6529 - prc_auc: 0.6329 - precision: 0.6616 - recall: 0.6349\n","Epoch 4: Validation Metrics:\n","loss: 0.6479220986366272\n","val_binary_accuracy: 0.6713709831237793\n","val_precision: 0.5773195624351501\n","val_recall: 0.5803108811378479\n","val_auc: 0.7103062868118286\n","val_prc_auc: 0.5808560252189636\n","\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 79ms/step - auc: 0.7917 - binary_accuracy: 0.7418 - f1: 0.6481 - loss: 0.6528 - prc_auc: 0.6330 - precision: 0.6618 - recall: 0.6353 - val_auc: 0.7103 - val_binary_accuracy: 0.6714 - val_f1: 0.5788 - val_loss: 0.6581 - val_prc_auc: 0.5809 - val_precision: 0.5773 - val_recall: 0.5803\n","Epoch 5/15\n","\u001b[1m139/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - auc: 0.8130 - binary_accuracy: 0.7561 - f1: 0.6799 - loss: 0.6362 - prc_auc: 0.6568 - precision: 0.6688 - recall: 0.6917\n","Epoch 5: Validation Metrics:\n","loss: 0.6316736340522766\n","val_binary_accuracy: 0.663306474685669\n","val_precision: 0.561904788017273\n","val_recall: 0.6113989353179932\n","val_auc: 0.713333010673523\n","val_prc_auc: 0.583845853805542\n","\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 80ms/step - auc: 0.8131 - binary_accuracy: 0.7562 - f1: 0.6801 - loss: 0.6362 - prc_auc: 0.6568 - precision: 0.6689 - recall: 0.6919 - val_auc: 0.7133 - val_binary_accuracy: 0.6633 - val_f1: 0.5856 - val_loss: 0.6500 - val_prc_auc: 0.5838 - val_precision: 0.5619 - val_recall: 0.6114\n","Epoch 6/15\n","\u001b[1m139/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.8191 - binary_accuracy: 0.7623 - f1: 0.6946 - loss: 0.6217 - prc_auc: 0.6636 - precision: 0.6698 - recall: 0.7217\n","Epoch 6: Validation Metrics:\n","loss: 0.6175428032875061\n","val_binary_accuracy: 0.6733871102333069\n","val_precision: 0.5720930099487305\n","val_recall: 0.6373056769371033\n","val_auc: 0.7150600552558899\n","val_prc_auc: 0.5848091840744019\n","\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 80ms/step - auc: 0.8191 - binary_accuracy: 0.7623 - f1: 0.6948 - loss: 0.6216 - prc_auc: 0.6637 - precision: 0.6699 - recall: 0.7220 - val_auc: 0.7151 - val_binary_accuracy: 0.6734 - val_f1: 0.6029 - val_loss: 0.6435 - val_prc_auc: 0.5848 - val_precision: 0.5721 - val_recall: 0.6373\n","Epoch 7/15\n","\u001b[1m139/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.8217 - binary_accuracy: 0.7635 - f1: 0.7021 - loss: 0.6091 - prc_auc: 0.6696 - precision: 0.6647 - recall: 0.7443\n","Epoch 7: Validation Metrics:\n","loss: 0.6052640676498413\n","val_binary_accuracy: 0.6814516186714172\n","val_precision: 0.5791855454444885\n","val_recall: 0.6632124185562134\n","val_auc: 0.7149575352668762\n","val_prc_auc: 0.5834043025970459\n","\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 80ms/step - auc: 0.8218 - binary_accuracy: 0.7635 - f1: 0.7023 - loss: 0.6090 - prc_auc: 0.6696 - precision: 0.6648 - recall: 0.7445 - val_auc: 0.7150 - val_binary_accuracy: 0.6815 - val_f1: 0.6184 - val_loss: 0.6384 - val_prc_auc: 0.5834 - val_precision: 0.5792 - val_recall: 0.6632\n","Epoch 8/15\n","\u001b[1m139/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.8225 - binary_accuracy: 0.7625 - f1: 0.7020 - loss: 0.5981 - prc_auc: 0.6712 - precision: 0.6628 - recall: 0.7467\n","Epoch 8: Validation Metrics:\n","loss: 0.5945984125137329\n","val_binary_accuracy: 0.6834677457809448\n","val_precision: 0.5810810923576355\n","val_recall: 0.6683937907218933\n","val_auc: 0.715744137763977\n","val_prc_auc: 0.5869191288948059\n","\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 80ms/step - auc: 0.8225 - binary_accuracy: 0.7626 - f1: 0.7022 - loss: 0.5981 - prc_auc: 0.6713 - precision: 0.6629 - recall: 0.7469 - val_auc: 0.7157 - val_binary_accuracy: 0.6835 - val_f1: 0.6217 - val_loss: 0.6343 - val_prc_auc: 0.5869 - val_precision: 0.5811 - val_recall: 0.6684\n","Epoch 9/15\n","\u001b[1m139/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.8224 - binary_accuracy: 0.7634 - f1: 0.7047 - loss: 0.5886 - prc_auc: 0.6718 - precision: 0.6620 - recall: 0.7538\n","Epoch 9: Validation Metrics:\n","loss: 0.5853330492973328\n","val_binary_accuracy: 0.6774193644523621\n","val_precision: 0.5733333230018616\n","val_recall: 0.6683937907218933\n","val_auc: 0.7151455283164978\n","val_prc_auc: 0.5849623680114746\n","\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 80ms/step - auc: 0.8225 - binary_accuracy: 0.7634 - f1: 0.7048 - loss: 0.5886 - prc_auc: 0.6719 - precision: 0.6621 - recall: 0.7541 - val_auc: 0.7151 - val_binary_accuracy: 0.6774 - val_f1: 0.6172 - val_loss: 0.6313 - val_prc_auc: 0.5850 - val_precision: 0.5733 - val_recall: 0.6684\n","Epoch 10/15\n","\u001b[1m139/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - auc: 0.8233 - binary_accuracy: 0.7665 - f1: 0.7101 - loss: 0.5803 - prc_auc: 0.6742 - precision: 0.6646 - recall: 0.7631\n","Epoch 10: Validation Metrics:\n","loss: 0.5772798657417297\n","val_binary_accuracy: 0.6774193644523621\n","val_precision: 0.5720524191856384\n","val_recall: 0.6787564754486084\n","val_auc: 0.7145471572875977\n","val_prc_auc: 0.5832721590995789\n","\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 82ms/step - auc: 0.8233 - binary_accuracy: 0.7665 - f1: 0.7102 - loss: 0.5803 - prc_auc: 0.6743 - precision: 0.6646 - recall: 0.7632 - val_auc: 0.7145 - val_binary_accuracy: 0.6774 - val_f1: 0.6209 - val_loss: 0.6290 - val_prc_auc: 0.5833 - val_precision: 0.5721 - val_recall: 0.6788\n","Epoch 11/15\n","\u001b[1m139/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - auc: 0.8236 - binary_accuracy: 0.7648 - f1: 0.7092 - loss: 0.5731 - prc_auc: 0.6749 - precision: 0.6612 - recall: 0.7653\n","Epoch 11: Validation Metrics:\n","loss: 0.5702740550041199\n","val_binary_accuracy: 0.6754032373428345\n","val_precision: 0.5695652365684509\n","val_recall: 0.6787564754486084\n","val_auc: 0.7150173783302307\n","val_prc_auc: 0.5833678245544434\n","\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 81ms/step - auc: 0.8236 - binary_accuracy: 0.7649 - f1: 0.7093 - loss: 0.5731 - prc_auc: 0.6750 - precision: 0.6612 - recall: 0.7655 - val_auc: 0.7150 - val_binary_accuracy: 0.6754 - val_f1: 0.6194 - val_loss: 0.6274 - val_prc_auc: 0.5834 - val_precision: 0.5696 - val_recall: 0.6788\n","Starting training for label: 1\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFMPNetModel.\n","\n","All the layers of TFMPNetModel were initialized from the model checkpoint at /content/drive/Othercomputers/ASUS Main/MLCP/05_MODELS/02_Transformer_Models/transformer_model_20250209_231314_1_epoch_full_nli.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMPNetModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/15\n","\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - auc: 0.7014 - binary_accuracy: 0.6550 - f1: 0.5452 - loss: 0.6663 - prc_auc: 0.5466 - precision: 0.5590 - recall: 0.5324\n","Epoch 1: Validation Metrics:\n","loss: 0.6594681143760681\n","val_binary_accuracy: 0.6653226017951965\n","val_precision: 0.5492228269577026\n","val_recall: 0.5729729533195496\n","val_auc: 0.6921525597572327\n","val_prc_auc: 0.5309271812438965\n","\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 180ms/step - auc: 0.7016 - binary_accuracy: 0.6553 - f1: 0.5454 - loss: 0.6663 - prc_auc: 0.5468 - precision: 0.5591 - recall: 0.5327 - val_auc: 0.6922 - val_binary_accuracy: 0.6653 - val_f1: 0.5608 - val_loss: 0.6601 - val_prc_auc: 0.5309 - val_precision: 0.5492 - val_recall: 0.5730\n","Epoch 2/15\n","\u001b[1m139/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.7741 - binary_accuracy: 0.7269 - f1: 0.6660 - loss: 0.6459 - prc_auc: 0.6236 - precision: 0.6345 - recall: 0.7014\n","Epoch 2: Validation Metrics:\n","loss: 0.6388412714004517\n","val_binary_accuracy: 0.6794354915618896\n","val_precision: 0.563725471496582\n","val_recall: 0.6216216087341309\n","val_auc: 0.7136090993881226\n","val_prc_auc: 0.5557743906974792\n","\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 80ms/step - auc: 0.7744 - binary_accuracy: 0.7271 - f1: 0.6661 - loss: 0.6458 - prc_auc: 0.6238 - precision: 0.6345 - recall: 0.7015 - val_auc: 0.7136 - val_binary_accuracy: 0.6794 - val_f1: 0.5913 - val_loss: 0.6497 - val_prc_auc: 0.5558 - val_precision: 0.5637 - val_recall: 0.6216\n","Epoch 3/15\n","\u001b[1m139/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.7965 - binary_accuracy: 0.7518 - f1: 0.7054 - loss: 0.6288 - prc_auc: 0.6525 - precision: 0.6551 - recall: 0.7647\n","Epoch 3: Validation Metrics:\n","loss: 0.6214515566825867\n","val_binary_accuracy: 0.6834677457809448\n","val_precision: 0.5660377144813538\n","val_recall: 0.6486486196517944\n","val_auc: 0.723698616027832\n","val_prc_auc: 0.5819292068481445\n","\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 80ms/step - auc: 0.7967 - binary_accuracy: 0.7519 - f1: 0.7054 - loss: 0.6287 - prc_auc: 0.6527 - precision: 0.6551 - recall: 0.7646 - val_auc: 0.7237 - val_binary_accuracy: 0.6835 - val_f1: 0.6045 - val_loss: 0.6417 - val_prc_auc: 0.5819 - val_precision: 0.5660 - val_recall: 0.6486\n","Epoch 4/15\n","\u001b[1m139/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - auc: 0.8052 - binary_accuracy: 0.7582 - f1: 0.7138 - loss: 0.6146 - prc_auc: 0.6644 - precision: 0.6615 - recall: 0.7756\n","Epoch 4: Validation Metrics:\n","loss: 0.6068267226219177\n","val_binary_accuracy: 0.6935483813285828\n","val_precision: 0.5760368704795837\n","val_recall: 0.6756756901741028\n","val_auc: 0.7280263900756836\n","val_prc_auc: 0.5891098380088806\n","\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 80ms/step - auc: 0.8054 - binary_accuracy: 0.7584 - f1: 0.7138 - loss: 0.6145 - prc_auc: 0.6646 - precision: 0.6616 - recall: 0.7757 - val_auc: 0.7280 - val_binary_accuracy: 0.6935 - val_f1: 0.6219 - val_loss: 0.6356 - val_prc_auc: 0.5891 - val_precision: 0.5760 - val_recall: 0.6757\n","Epoch 5/15\n","\u001b[1m139/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.8108 - binary_accuracy: 0.7615 - f1: 0.7198 - loss: 0.6028 - prc_auc: 0.6792 - precision: 0.6628 - recall: 0.7881\n","Epoch 5: Validation Metrics:\n","loss: 0.5945470929145813\n","val_binary_accuracy: 0.6854838728904724\n","val_precision: 0.5650224089622498\n","val_recall: 0.6810810565948486\n","val_auc: 0.7319979071617126\n","val_prc_auc: 0.5981103181838989\n","\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 80ms/step - auc: 0.8110 - binary_accuracy: 0.7617 - f1: 0.7198 - loss: 0.6026 - prc_auc: 0.6793 - precision: 0.6628 - recall: 0.7881 - val_auc: 0.7320 - val_binary_accuracy: 0.6855 - val_f1: 0.6176 - val_loss: 0.6312 - val_prc_auc: 0.5981 - val_precision: 0.5650 - val_recall: 0.6811\n","Epoch 6/15\n","\u001b[1m139/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.8144 - binary_accuracy: 0.7640 - f1: 0.7244 - loss: 0.5930 - prc_auc: 0.6887 - precision: 0.6634 - recall: 0.7982\n","Epoch 6: Validation Metrics:\n","loss: 0.5842455625534058\n","val_binary_accuracy: 0.6854838728904724\n","val_precision: 0.5633187890052795\n","val_recall: 0.6972972750663757\n","val_auc: 0.7335708141326904\n","val_prc_auc: 0.6020270586013794\n","\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 80ms/step - auc: 0.8146 - binary_accuracy: 0.7641 - f1: 0.7244 - loss: 0.5928 - prc_auc: 0.6889 - precision: 0.6635 - recall: 0.7982 - val_auc: 0.7336 - val_binary_accuracy: 0.6855 - val_f1: 0.6232 - val_loss: 0.6280 - val_prc_auc: 0.6020 - val_precision: 0.5633 - val_recall: 0.6973\n","Epoch 7/15\n","\u001b[1m139/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.8161 - binary_accuracy: 0.7631 - f1: 0.7241 - loss: 0.5848 - prc_auc: 0.6948 - precision: 0.6615 - recall: 0.8002\n","Epoch 7: Validation Metrics:\n","loss: 0.5756028890609741\n","val_binary_accuracy: 0.6854838728904724\n","val_precision: 0.5627705454826355\n","val_recall: 0.7027027010917664\n","val_auc: 0.7344312071800232\n","val_prc_auc: 0.6002475023269653\n","\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 80ms/step - auc: 0.8163 - binary_accuracy: 0.7632 - f1: 0.7241 - loss: 0.5847 - prc_auc: 0.6950 - precision: 0.6616 - recall: 0.8002 - val_auc: 0.7344 - val_binary_accuracy: 0.6855 - val_f1: 0.6250 - val_loss: 0.6258 - val_prc_auc: 0.6002 - val_precision: 0.5628 - val_recall: 0.7027\n","Epoch 8/15\n","\u001b[1m139/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - auc: 0.8182 - binary_accuracy: 0.7622 - f1: 0.7234 - loss: 0.5781 - prc_auc: 0.7025 - precision: 0.6603 - recall: 0.8002\n","Epoch 8: Validation Metrics:\n","loss: 0.5683469176292419\n","val_binary_accuracy: 0.6854838728904724\n","val_precision: 0.5627705454826355\n","val_recall: 0.7027027010917664\n","val_auc: 0.7351524829864502\n","val_prc_auc: 0.6025282740592957\n","\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 80ms/step - auc: 0.8184 - binary_accuracy: 0.7624 - f1: 0.7234 - loss: 0.5780 - prc_auc: 0.7026 - precision: 0.6604 - recall: 0.8002 - val_auc: 0.7352 - val_binary_accuracy: 0.6855 - val_f1: 0.6250 - val_loss: 0.6244 - val_prc_auc: 0.6025 - val_precision: 0.5628 - val_recall: 0.7027\n","Epoch 9/15\n","\u001b[1m139/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.8195 - binary_accuracy: 0.7613 - f1: 0.7230 - loss: 0.5726 - prc_auc: 0.7076 - precision: 0.6587 - recall: 0.8016\n","Epoch 9: Validation Metrics:\n","loss: 0.5622463822364807\n","val_binary_accuracy: 0.6814516186714172\n","val_precision: 0.5584415793418884\n","val_recall: 0.6972972750663757\n","val_auc: 0.7360997200012207\n","val_prc_auc: 0.6069937944412231\n","\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 80ms/step - auc: 0.8197 - binary_accuracy: 0.7615 - f1: 0.7230 - loss: 0.5724 - prc_auc: 0.7078 - precision: 0.6587 - recall: 0.8016 - val_auc: 0.7361 - val_binary_accuracy: 0.6815 - val_f1: 0.6202 - val_loss: 0.6236 - val_prc_auc: 0.6070 - val_precision: 0.5584 - val_recall: 0.6973\n","Epoch 10/15\n","\u001b[1m139/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.8203 - binary_accuracy: 0.7614 - f1: 0.7233 - loss: 0.5680 - prc_auc: 0.7099 - precision: 0.6585 - recall: 0.8027\n","Epoch 10: Validation Metrics:\n","loss: 0.5571062564849854\n","val_binary_accuracy: 0.6774193644523621\n","val_precision: 0.553648054599762\n","val_recall: 0.6972972750663757\n","val_auc: 0.736351728439331\n","val_prc_auc: 0.6052607297897339\n","\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 80ms/step - auc: 0.8204 - binary_accuracy: 0.7615 - f1: 0.7233 - loss: 0.5678 - prc_auc: 0.7101 - precision: 0.6585 - recall: 0.8027 - val_auc: 0.7364 - val_binary_accuracy: 0.6774 - val_f1: 0.6172 - val_loss: 0.6233 - val_prc_auc: 0.6053 - val_precision: 0.5536 - val_recall: 0.6973\n","Starting training for label: 2\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFMPNetModel.\n","\n","All the layers of TFMPNetModel were initialized from the model checkpoint at /content/drive/Othercomputers/ASUS Main/MLCP/05_MODELS/02_Transformer_Models/transformer_model_20250209_231314_1_epoch_full_nli.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMPNetModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/15\n","\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - auc: 0.7405 - binary_accuracy: 0.6470 - f1: 0.5659 - loss: 0.6648 - prc_auc: 0.5920 - precision: 0.4702 - recall: 0.7127\n","Epoch 1: Validation Metrics:\n","loss: 0.6620209217071533\n","val_binary_accuracy: 0.6068548560142517\n","val_precision: 0.4557522237300873\n","val_recall: 0.5885714292526245\n","val_auc: 0.6362438797950745\n","val_prc_auc: 0.46027088165283203\n","\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 183ms/step - auc: 0.7405 - binary_accuracy: 0.6470 - f1: 0.5661 - loss: 0.6648 - prc_auc: 0.5923 - precision: 0.4704 - recall: 0.7127 - val_auc: 0.6362 - val_binary_accuracy: 0.6069 - val_f1: 0.5137 - val_loss: 0.6749 - val_prc_auc: 0.4603 - val_precision: 0.4558 - val_recall: 0.5886\n","Epoch 2/15\n","\u001b[1m139/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.7502 - binary_accuracy: 0.6747 - f1: 0.5767 - loss: 0.6521 - prc_auc: 0.6069 - precision: 0.4983 - recall: 0.6864\n","Epoch 2: Validation Metrics:\n","loss: 0.6497229337692261\n","val_binary_accuracy: 0.6189516186714172\n","val_precision: 0.46759259700775146\n","val_recall: 0.5771428346633911\n","val_auc: 0.6365197896957397\n","val_prc_auc: 0.4588184058666229\n","\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 80ms/step - auc: 0.7502 - binary_accuracy: 0.6748 - f1: 0.5769 - loss: 0.6521 - prc_auc: 0.6074 - precision: 0.4986 - recall: 0.6865 - val_auc: 0.6365 - val_binary_accuracy: 0.6190 - val_f1: 0.5166 - val_loss: 0.6701 - val_prc_auc: 0.4588 - val_precision: 0.4676 - val_recall: 0.5771\n","Epoch 3/15\n","\u001b[1m139/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.7532 - binary_accuracy: 0.6879 - f1: 0.5828 - loss: 0.6413 - prc_auc: 0.6099 - precision: 0.5137 - recall: 0.6747\n","Epoch 3: Validation Metrics:\n","loss: 0.6392654776573181\n","val_binary_accuracy: 0.6229838728904724\n","val_precision: 0.4716981053352356\n","val_recall: 0.5714285969734192\n","val_auc: 0.6372852921485901\n","val_prc_auc: 0.46212783455848694\n","\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 80ms/step - auc: 0.7532 - binary_accuracy: 0.6880 - f1: 0.5830 - loss: 0.6412 - prc_auc: 0.6103 - precision: 0.5140 - recall: 0.6750 - val_auc: 0.6373 - val_binary_accuracy: 0.6230 - val_f1: 0.5168 - val_loss: 0.6665 - val_prc_auc: 0.4621 - val_precision: 0.4717 - val_recall: 0.5714\n","Epoch 4/15\n","\u001b[1m139/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.7556 - binary_accuracy: 0.6962 - f1: 0.5872 - loss: 0.6321 - prc_auc: 0.6102 - precision: 0.5241 - recall: 0.6691\n","Epoch 4: Validation Metrics:\n","loss: 0.6303383111953735\n","val_binary_accuracy: 0.6270161271095276\n","val_precision: 0.4759615361690521\n","val_recall: 0.5657142996788025\n","val_auc: 0.6386826634407043\n","val_prc_auc: 0.4664367139339447\n","\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 80ms/step - auc: 0.7556 - binary_accuracy: 0.6962 - f1: 0.5874 - loss: 0.6321 - prc_auc: 0.6106 - precision: 0.5243 - recall: 0.6693 - val_auc: 0.6387 - val_binary_accuracy: 0.6270 - val_f1: 0.5170 - val_loss: 0.6640 - val_prc_auc: 0.4664 - val_precision: 0.4760 - val_recall: 0.5657\n","Epoch 5/15\n","\u001b[1m139/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - auc: 0.7573 - binary_accuracy: 0.6960 - f1: 0.5834 - loss: 0.6243 - prc_auc: 0.6103 - precision: 0.5241 - recall: 0.6593\n","Epoch 5: Validation Metrics:\n","loss: 0.6226942539215088\n","val_binary_accuracy: 0.625\n","val_precision: 0.47342994809150696\n","val_recall: 0.5600000023841858\n","val_auc: 0.6395816802978516\n","val_prc_auc: 0.46700555086135864\n","\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 80ms/step - auc: 0.7573 - binary_accuracy: 0.6961 - f1: 0.5837 - loss: 0.6243 - prc_auc: 0.6107 - precision: 0.5244 - recall: 0.6596 - val_auc: 0.6396 - val_binary_accuracy: 0.6250 - val_f1: 0.5131 - val_loss: 0.6624 - val_prc_auc: 0.4670 - val_precision: 0.4734 - val_recall: 0.5600\n","Epoch 6/15\n","\u001b[1m139/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - auc: 0.7585 - binary_accuracy: 0.6987 - f1: 0.5851 - loss: 0.6176 - prc_auc: 0.6110 - precision: 0.5275 - recall: 0.6583\n","Epoch 6: Validation Metrics:\n","loss: 0.6161265969276428\n","val_binary_accuracy: 0.6310483813285828\n","val_precision: 0.4801980257034302\n","val_recall: 0.5542857050895691\n","val_auc: 0.6402136087417603\n","val_prc_auc: 0.46610409021377563\n","\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 79ms/step - auc: 0.7585 - binary_accuracy: 0.6987 - f1: 0.5854 - loss: 0.6176 - prc_auc: 0.6114 - precision: 0.5278 - recall: 0.6586 - val_auc: 0.6402 - val_binary_accuracy: 0.6310 - val_f1: 0.5146 - val_loss: 0.6614 - val_prc_auc: 0.4661 - val_precision: 0.4802 - val_recall: 0.5543\n","Epoch 7/15\n","\u001b[1m139/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - auc: 0.7597 - binary_accuracy: 0.7003 - f1: 0.5862 - loss: 0.6119 - prc_auc: 0.6125 - precision: 0.5297 - recall: 0.6578\n","Epoch 7: Validation Metrics:\n","loss: 0.6104623079299927\n","val_binary_accuracy: 0.625\n","val_precision: 0.47236180305480957\n","val_recall: 0.5371428728103638\n","val_auc: 0.6408010721206665\n","val_prc_auc: 0.4671843647956848\n","\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 79ms/step - auc: 0.7597 - binary_accuracy: 0.7004 - f1: 0.5865 - loss: 0.6119 - prc_auc: 0.6129 - precision: 0.5300 - recall: 0.6580 - val_auc: 0.6408 - val_binary_accuracy: 0.6250 - val_f1: 0.5027 - val_loss: 0.6610 - val_prc_auc: 0.4672 - val_precision: 0.4724 - val_recall: 0.5371\n","Starting training for label: 3\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFMPNetModel.\n","\n","All the layers of TFMPNetModel were initialized from the model checkpoint at /content/drive/Othercomputers/ASUS Main/MLCP/05_MODELS/02_Transformer_Models/transformer_model_20250209_231314_1_epoch_full_nli.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMPNetModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/15\n","\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - auc: 0.5482 - binary_accuracy: 0.4121 - f1: 0.4003 - loss: 0.6989 - prc_auc: 0.3058 - precision: 0.2771 - recall: 0.7263\n","Epoch 1: Validation Metrics:\n","loss: 0.6906633973121643\n","val_binary_accuracy: 0.7056451439857483\n","val_precision: 0.42767295241355896\n","val_recall: 0.5528455376625061\n","val_auc: 0.7160139083862305\n","val_prc_auc: 0.5046844482421875\n","\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 177ms/step - auc: 0.5488 - binary_accuracy: 0.4129 - f1: 0.4006 - loss: 0.6988 - prc_auc: 0.3063 - precision: 0.2775 - recall: 0.7262 - val_auc: 0.7160 - val_binary_accuracy: 0.7056 - val_f1: 0.4823 - val_loss: 0.6763 - val_prc_auc: 0.5047 - val_precision: 0.4277 - val_recall: 0.5528\n","Epoch 2/15\n","\u001b[1m139/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - auc: 0.7561 - binary_accuracy: 0.7556 - f1: 0.5789 - loss: 0.6688 - prc_auc: 0.5476 - precision: 0.5394 - recall: 0.6265\n","Epoch 2: Validation Metrics:\n","loss: 0.6611076593399048\n","val_binary_accuracy: 0.7540322542190552\n","val_precision: 0.5042017102241516\n","val_recall: 0.4878048896789551\n","val_auc: 0.7293096780776978\n","val_prc_auc: 0.5059092044830322\n","\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 79ms/step - auc: 0.7563 - binary_accuracy: 0.7558 - f1: 0.5790 - loss: 0.6687 - prc_auc: 0.5481 - precision: 0.5398 - recall: 0.6262 - val_auc: 0.7293 - val_binary_accuracy: 0.7540 - val_f1: 0.4959 - val_loss: 0.6510 - val_prc_auc: 0.5059 - val_precision: 0.5042 - val_recall: 0.4878\n","Epoch 3/15\n","\u001b[1m139/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - auc: 0.7809 - binary_accuracy: 0.7704 - f1: 0.5862 - loss: 0.6431 - prc_auc: 0.5658 - precision: 0.5674 - recall: 0.6073\n","Epoch 3: Validation Metrics:\n","loss: 0.6357547044754028\n","val_binary_accuracy: 0.7520161271095276\n","val_precision: 0.5\n","val_recall: 0.47154471278190613\n","val_auc: 0.725386381149292\n","val_prc_auc: 0.4967987537384033\n","\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 79ms/step - auc: 0.7810 - binary_accuracy: 0.7705 - f1: 0.5862 - loss: 0.6430 - prc_auc: 0.5661 - precision: 0.5675 - recall: 0.6071 - val_auc: 0.7254 - val_binary_accuracy: 0.7520 - val_f1: 0.4854 - val_loss: 0.6295 - val_prc_auc: 0.4968 - val_precision: 0.5000 - val_recall: 0.4715\n","Epoch 4/15\n","\u001b[1m139/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - auc: 0.7827 - binary_accuracy: 0.7665 - f1: 0.5809 - loss: 0.6210 - prc_auc: 0.5639 - precision: 0.5602 - recall: 0.6043\n","Epoch 4: Validation Metrics:\n","loss: 0.6139694452285767\n","val_binary_accuracy: 0.7520161271095276\n","val_precision: 0.5\n","val_recall: 0.47154471278190613\n","val_auc: 0.724296510219574\n","val_prc_auc: 0.4918295741081238\n","\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 80ms/step - auc: 0.7828 - binary_accuracy: 0.7666 - f1: 0.5809 - loss: 0.6209 - prc_auc: 0.5642 - precision: 0.5603 - recall: 0.6041 - val_auc: 0.7243 - val_binary_accuracy: 0.7520 - val_f1: 0.4854 - val_loss: 0.6112 - val_prc_auc: 0.4918 - val_precision: 0.5000 - val_recall: 0.4715\n","Epoch 5/15\n","\u001b[1m139/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - auc: 0.7826 - binary_accuracy: 0.7684 - f1: 0.5843 - loss: 0.6020 - prc_auc: 0.5609 - precision: 0.5632 - recall: 0.6080\n","Epoch 5: Validation Metrics:\n","loss: 0.5952478647232056\n","val_binary_accuracy: 0.7459677457809448\n","val_precision: 0.48739495873451233\n","val_recall: 0.47154471278190613\n","val_auc: 0.723326563835144\n","val_prc_auc: 0.4873405694961548\n","\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 79ms/step - auc: 0.7827 - binary_accuracy: 0.7685 - f1: 0.5843 - loss: 0.6019 - prc_auc: 0.5612 - precision: 0.5632 - recall: 0.6077 - val_auc: 0.7233 - val_binary_accuracy: 0.7460 - val_f1: 0.4793 - val_loss: 0.5958 - val_prc_auc: 0.4873 - val_precision: 0.4874 - val_recall: 0.4715\n","Starting training for label: 4\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFMPNetModel.\n","\n","All the layers of TFMPNetModel were initialized from the model checkpoint at /content/drive/Othercomputers/ASUS Main/MLCP/05_MODELS/02_Transformer_Models/transformer_model_20250209_231314_1_epoch_full_nli.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMPNetModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/15\n","\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - auc: 0.6427 - binary_accuracy: 0.6065 - f1: 0.4090 - loss: 0.6819 - prc_auc: 0.4201 - precision: 0.3166 - recall: 0.5853\n","Epoch 1: Validation Metrics:\n","loss: 0.6715880632400513\n","val_binary_accuracy: 0.8064516186714172\n","val_precision: 0.517241358757019\n","val_recall: 0.4545454680919647\n","val_auc: 0.7191944718360901\n","val_prc_auc: 0.5048447251319885\n","\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 181ms/step - auc: 0.6431 - binary_accuracy: 0.6071 - f1: 0.4094 - loss: 0.6818 - prc_auc: 0.4207 - precision: 0.3171 - recall: 0.5852 - val_auc: 0.7192 - val_binary_accuracy: 0.8065 - val_f1: 0.4839 - val_loss: 0.6518 - val_prc_auc: 0.5048 - val_precision: 0.5172 - val_recall: 0.4545\n","Epoch 2/15\n","\u001b[1m139/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - auc: 0.7679 - binary_accuracy: 0.8189 - f1: 0.5819 - loss: 0.6448 - prc_auc: 0.5961 - precision: 0.6190 - recall: 0.5494\n","Epoch 2: Validation Metrics:\n","loss: 0.6353346705436707\n","val_binary_accuracy: 0.8205645084381104\n","val_precision: 0.5625\n","val_recall: 0.4545454680919647\n","val_auc: 0.7348930239677429\n","val_prc_auc: 0.5142070055007935\n","\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 79ms/step - auc: 0.7680 - binary_accuracy: 0.8190 - f1: 0.5821 - loss: 0.6447 - prc_auc: 0.5963 - precision: 0.6192 - recall: 0.5495 - val_auc: 0.7349 - val_binary_accuracy: 0.8206 - val_f1: 0.5028 - val_loss: 0.6204 - val_prc_auc: 0.5142 - val_precision: 0.5625 - val_recall: 0.4545\n","Epoch 3/15\n","\u001b[1m139/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - auc: 0.7905 - binary_accuracy: 0.8214 - f1: 0.5760 - loss: 0.6129 - prc_auc: 0.6114 - precision: 0.6315 - recall: 0.5300\n","Epoch 3: Validation Metrics:\n","loss: 0.6041723489761353\n","val_binary_accuracy: 0.8125\n","val_precision: 0.5375000238418579\n","val_recall: 0.4343434274196625\n","val_auc: 0.7350329756736755\n","val_prc_auc: 0.5280248522758484\n","\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 79ms/step - auc: 0.7906 - binary_accuracy: 0.8215 - f1: 0.5763 - loss: 0.6128 - prc_auc: 0.6116 - precision: 0.6317 - recall: 0.5303 - val_auc: 0.7350 - val_binary_accuracy: 0.8125 - val_f1: 0.4804 - val_loss: 0.5936 - val_prc_auc: 0.5280 - val_precision: 0.5375 - val_recall: 0.4343\n","Epoch 4/15\n","\u001b[1m139/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - auc: 0.7965 - binary_accuracy: 0.8235 - f1: 0.5795 - loss: 0.5857 - prc_auc: 0.6124 - precision: 0.6379 - recall: 0.5318\n","Epoch 4: Validation Metrics:\n","loss: 0.5774235725402832\n","val_binary_accuracy: 0.8145161271095276\n","val_precision: 0.5432098507881165\n","val_recall: 0.4444444477558136\n","val_auc: 0.7341551184654236\n","val_prc_auc: 0.5246623754501343\n","\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 79ms/step - auc: 0.7966 - binary_accuracy: 0.8235 - f1: 0.5797 - loss: 0.5856 - prc_auc: 0.6125 - precision: 0.6380 - recall: 0.5321 - val_auc: 0.7342 - val_binary_accuracy: 0.8145 - val_f1: 0.4889 - val_loss: 0.5709 - val_prc_auc: 0.5247 - val_precision: 0.5432 - val_recall: 0.4444\n","Epoch 5/15\n","\u001b[1m139/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - auc: 0.7992 - binary_accuracy: 0.8223 - f1: 0.5773 - loss: 0.5624 - prc_auc: 0.6132 - precision: 0.6346 - recall: 0.5305\n","Epoch 5: Validation Metrics:\n","loss: 0.5545118451118469\n","val_binary_accuracy: 0.8145161271095276\n","val_precision: 0.5432098507881165\n","val_recall: 0.4444444477558136\n","val_auc: 0.7310892343521118\n","val_prc_auc: 0.5189231038093567\n","\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 79ms/step - auc: 0.7993 - binary_accuracy: 0.8224 - f1: 0.5775 - loss: 0.5623 - prc_auc: 0.6134 - precision: 0.6347 - recall: 0.5308 - val_auc: 0.7311 - val_binary_accuracy: 0.8145 - val_f1: 0.4889 - val_loss: 0.5517 - val_prc_auc: 0.5189 - val_precision: 0.5432 - val_recall: 0.4444\n","\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 242ms/step\n","\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 207ms/step\n","\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 209ms/step\n","\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 207ms/step\n","\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 213ms/step\n","[[0 0 1 0 0]\n"," [0 1 1 0 1]\n"," [0 1 0 0 0]\n"," ...\n"," [0 1 0 1 0]\n"," [1 0 1 0 0]\n"," [1 0 1 0 0]]\n"]}],"source":["# local application/library specific imports\n","from app_src import OneVsAllTransformerEvaluator\n","from app_config import AppConfig\n","\n","# define configuration proxy\n","configProxy = AppConfig()\n","CONFIG = configProxy.return_config()\n","\n","transformer_evaluator = OneVsAllTransformerEvaluator()\n","transformer_evaluator.evaluate_models(\n","    epochs=15,\n","    batch_size=32,\n","    number_of_tags=5,\n","    train_model=True,\n","    threshold=0.5,\n","    transformer_model_path=os.path.join(CONFIG[\"TRANSFORMER_SAVE_PATH_ROOT\"], 'transformer_model_20250209_231314_1_epoch_full_nli')\n","  )\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.15"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0607fc3d5ee2416ea523bd5cce0e8c80":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_15ff464abd8f4c98bcacb672569a8dc6","placeholder":"​","style":"IPY_MODEL_824bdcb736db49b0b01e101f3d4bbe32","value":"tokenizer.json: 100%"}},"0aca9bdca6b844d28e52f26c9c8d7c0b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3b9df8fc691f40fe98990c54561e2c74","IPY_MODEL_6fc42a1b658b45b882fb43b7b6d8f232","IPY_MODEL_e4f13c189dab4b3da8efd1a02af84723"],"layout":"IPY_MODEL_8ddf1c28f5994768ac08be730fb5d4f9"}},"0e0b5259abee4a00b2aed377427faebc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"10bdce2086a5473ba4cb6ddcee7d16e4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_346f0e433bfe4df4a4ce6b9f884a3c43","placeholder":"​","style":"IPY_MODEL_dde9c02f2afc4fbc9e2f6a8041de369c","value":" 232k/232k [00:00&lt;00:00, 4.87MB/s]"}},"115fb8c0adf84e5687cdfecaddc7fc88":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a2a9545b11644b6a7c683c3baa92760","placeholder":"​","style":"IPY_MODEL_54739fd4f13d4b128a24de6e70c503f3","value":" 466k/466k [00:00&lt;00:00, 10.7MB/s]"}},"133aa1b9c30c413c801aa5af370d7152":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"143f3f72648947628be426536c54165a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"156e7b3dc5c74377abf997088f5d5039":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e0b5259abee4a00b2aed377427faebc","placeholder":"​","style":"IPY_MODEL_b1298a3565b24588878eab99a2f778d2","value":" 532M/532M [00:11&lt;00:00, 34.3MB/s]"}},"15ff464abd8f4c98bcacb672569a8dc6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1872502f5bbf42d1827f773b9bcd37f6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"18c24b0b760a4b5e8dfcbace68fd7469":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1969c0b6db9c4f47b22607038a7b1a79":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"197034abbdda48c48ca7327db1cc3187":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"21560efa6dab4552925f64ef39b9617a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"224fb6448d974adc8c89a7e5d92893f2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"229551aa84fd4c7787adcc867eabba53":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"256df0b0bbc543caa79c397cbbfba153":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"25c1221c444c4c6e81dc25e8ab06143f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26f4c11f6dbc43b19112f4502a4546a7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2c6bd7e97ea2414aac33c456b046e873":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a75a543c4a414a9e98646af48b6bbe80","max":239,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c77a74539b4a4ca7a56aae18c674362e","value":239}},"31789b2281b447e9a533802256b73e32":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3199f9cac05842f1beff8c0e221fa4b9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"346f0e433bfe4df4a4ce6b9f884a3c43":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"34c364b6aba24f388bf8b35d3f87fa11":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"359a25604e64401bba227185ae1c76c6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"36965743912a4bbc83a05b3c55b9e544":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3752e4d3b48a4f7d8897efc0c72b92bf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"37822c1dea7c4066891fe9dc2bfad9de":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0607fc3d5ee2416ea523bd5cce0e8c80","IPY_MODEL_4b42397cb6fa4f13a52fcdfc4e7d7095","IPY_MODEL_115fb8c0adf84e5687cdfecaddc7fc88"],"layout":"IPY_MODEL_b469b0c304d84cbe9467cc76204428c5"}},"38a505fae5844b9a99dc7240b56f1f35":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"394054e7098d42d68f43d75f088d00f0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3a0217664365498e81c1375d11696089":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_42f47dbea5d44dff9749627569b344e3","placeholder":"​","style":"IPY_MODEL_afd7b77b1b5e41b9aa344ef3d4af6eee","value":" 438M/438M [00:01&lt;00:00, 238MB/s]"}},"3b9df8fc691f40fe98990c54561e2c74":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_38a505fae5844b9a99dc7240b56f1f35","placeholder":"​","style":"IPY_MODEL_e7479909d47f4b32a70cdd30a9469d7d","value":"tokenizer_config.json: 100%"}},"3de58cc6746d4b5bb0e82579661a960b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3e5076a0f2ca4e6cbc4fd25a35cb23dc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3199f9cac05842f1beff8c0e221fa4b9","max":239,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f19b119822bb42c2aeddb790466c3d78","value":239}},"40fb4eea828a47eebde3deffb34eccaf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d465a2294ac346fcaeb1e40f905431ed","placeholder":"​","style":"IPY_MODEL_e8217faab1a64813989a48f3bc71980e","value":"model.safetensors: 100%"}},"42d356c365ca4c2ab154f640a5a50f15":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"42f3658a4ec241fc93c783ee84caa9cf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42f47dbea5d44dff9749627569b344e3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"445c5becb50647a098e68e6bca012276":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_26f4c11f6dbc43b19112f4502a4546a7","placeholder":"​","style":"IPY_MODEL_42d356c365ca4c2ab154f640a5a50f15","value":" 466k/466k [00:00&lt;00:00, 22.8MB/s]"}},"4549d9c099ab48e88afcbb1d8f4c789d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4905559eaa5b40779fa2d579308ddeb3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_baa06b0ec2d34d02b113f8503fddc216","max":363,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d441459d1c884b52b9772285b2fd17f0","value":363}},"4b42397cb6fa4f13a52fcdfc4e7d7095":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dbca1f857e064aeeabac2f8eac9c9ba7","max":466021,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ae0bdf64c6fe474ba4b361a10d3406ab","value":466021}},"4bdb5352724c40eb9d493cb1efd743d1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ae478ac2bfec4d3694f2550a6e7d28e8","placeholder":"​","style":"IPY_MODEL_1969c0b6db9c4f47b22607038a7b1a79","value":" 232k/232k [00:00&lt;00:00, 4.81MB/s]"}},"4dc6ae2f526f4653a54c0c07381768db":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6812d035d701491cb605814f86bcbb3e","placeholder":"​","style":"IPY_MODEL_4549d9c099ab48e88afcbb1d8f4c789d","value":"vocab.txt: 100%"}},"54739fd4f13d4b128a24de6e70c503f3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"56ced83d439f4233974c0b732c4b8008":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a2a9545b11644b6a7c683c3baa92760":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a58397a57e14097b5e26abd6bd2277f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d03f852c858b4a9b82b9edfed62aae95","IPY_MODEL_f4bb3a71389b4b16b94077ba86f72fac","IPY_MODEL_10bdce2086a5473ba4cb6ddcee7d16e4"],"layout":"IPY_MODEL_133aa1b9c30c413c801aa5af370d7152"}},"5c535e651f1c4adcbaa60791742af937":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5dd7e4033d7d4a7687910dd2e7030454":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"629d0c97f04945dfa52d407952c3123d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ecaeca1869b54dd3816f8198d483adc4","IPY_MODEL_af49943242a24b8f87ffdb4bc1265ab4","IPY_MODEL_445c5becb50647a098e68e6bca012276"],"layout":"IPY_MODEL_7a441379e706410388285282d411606c"}},"64e547717c224c02981da5a4ff0462a4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_810f6376fd9e4b4589f1d76bdbbba698","IPY_MODEL_65b9b5ea95334da8a0519f61e9dd4c8b","IPY_MODEL_3a0217664365498e81c1375d11696089"],"layout":"IPY_MODEL_a763decf210b4bb19269b66251f8ff6f"}},"65b9b5ea95334da8a0519f61e9dd4c8b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b654486cb19443ec88cba4f74f18da7b","max":437971872,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ece2b16bff0d42e1a080df7ff743a59d","value":437971872}},"67b0ded56d4342698ab40156896dbef3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6812d035d701491cb605814f86bcbb3e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c28c29db2b54a68809be4927f2e6561":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_31789b2281b447e9a533802256b73e32","max":531998632,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ffa3a842b49943ba8fffe30c63b7e803","value":531998632}},"6dd438058253463ab15a65ad8f33192f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_67b0ded56d4342698ab40156896dbef3","placeholder":"​","style":"IPY_MODEL_da87376fe355463386e1bcae2d6146d4","value":" 239/239 [00:00&lt;00:00, 21.9kB/s]"}},"6fc42a1b658b45b882fb43b7b6d8f232":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_394054e7098d42d68f43d75f088d00f0","max":363,"min":0,"orientation":"horizontal","style":"IPY_MODEL_aa2a78105181497c80a88a6cd6272e7a","value":363}},"71ffb7c2f5e24755a283cd5294e40f7e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c9f2cd35619145e5b043f9b8883bd639","placeholder":"​","style":"IPY_MODEL_34c364b6aba24f388bf8b35d3f87fa11","value":"special_tokens_map.json: 100%"}},"76c7ede6ff5e462a8312821bad1a8202":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4dc6ae2f526f4653a54c0c07381768db","IPY_MODEL_c42874edb92e43cca983af83dc96420b","IPY_MODEL_4bdb5352724c40eb9d493cb1efd743d1"],"layout":"IPY_MODEL_d4da583e676f4d08a7e4356e6c19f394"}},"7a441379e706410388285282d411606c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d29ee8eb0d9461d87c32913f11ea7b5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ee763e22b064a0fbcf9b42ccf97b9ca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_40fb4eea828a47eebde3deffb34eccaf","IPY_MODEL_6c28c29db2b54a68809be4927f2e6561","IPY_MODEL_156e7b3dc5c74377abf997088f5d5039"],"layout":"IPY_MODEL_143f3f72648947628be426536c54165a"}},"80172f1094b240ebb9474fd8d5ec92b6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"810f6376fd9e4b4589f1d76bdbbba698":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_36965743912a4bbc83a05b3c55b9e544","placeholder":"​","style":"IPY_MODEL_fcb2b70818dd4586bc146ccf99f08869","value":"model.safetensors: 100%"}},"824bdcb736db49b0b01e101f3d4bbe32":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"82c4db0038a142ffa98f46da523d95f3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ddf1c28f5994768ac08be730fb5d4f9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ef25127aea24b65aed075efbf782bd9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aa274429fd264ada9935779eaa1ca354","placeholder":"​","style":"IPY_MODEL_faaa9d56430f4cd1854b1775bc3172ba","value":"tokenizer_config.json: 100%"}},"9c4cd35bd3c64007a9744c871f60f686":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a75a543c4a414a9e98646af48b6bbe80":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a763decf210b4bb19269b66251f8ff6f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a76d43f7e247423d8bd0267409172ae1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8ef25127aea24b65aed075efbf782bd9","IPY_MODEL_4905559eaa5b40779fa2d579308ddeb3","IPY_MODEL_ec09a2d650af46328f1be4ad121ed09f"],"layout":"IPY_MODEL_42f3658a4ec241fc93c783ee84caa9cf"}},"aa274429fd264ada9935779eaa1ca354":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aa2a78105181497c80a88a6cd6272e7a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ae0bdf64c6fe474ba4b361a10d3406ab":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ae139a3cd6614bdb8b13997dfa0926c7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_25c1221c444c4c6e81dc25e8ab06143f","placeholder":"​","style":"IPY_MODEL_359a25604e64401bba227185ae1c76c6","value":"config.json: 100%"}},"ae478ac2bfec4d3694f2550a6e7d28e8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af49943242a24b8f87ffdb4bc1265ab4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d0944baaa43845f8a21712e9b942719d","max":466021,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3752e4d3b48a4f7d8897efc0c72b92bf","value":466021}},"afd7b77b1b5e41b9aa344ef3d4af6eee":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b1298a3565b24588878eab99a2f778d2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b1ae715e675d45c2957a160069ac7d08":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b2026bb57acb43ccabb5254e5a74b4d6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_18c24b0b760a4b5e8dfcbace68fd7469","placeholder":"​","style":"IPY_MODEL_f9642ec2fd524b17a5a1ce5f4f5c1ee1","value":" 571/571 [00:00&lt;00:00, 52.1kB/s]"}},"b469b0c304d84cbe9467cc76204428c5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b654486cb19443ec88cba4f74f18da7b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"baa06b0ec2d34d02b113f8503fddc216":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bcb1e5270d0b4d76ad75e7f33f15895a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c42874edb92e43cca983af83dc96420b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_224fb6448d974adc8c89a7e5d92893f2","max":231536,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ca45baf4c67c4a679a7300e9fb6e18d3","value":231536}},"c77a74539b4a4ca7a56aae18c674362e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c9f2cd35619145e5b043f9b8883bd639":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca45baf4c67c4a679a7300e9fb6e18d3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cb5b456f7378455e9fb95392b2c27b24":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_21560efa6dab4552925f64ef39b9617a","placeholder":"​","style":"IPY_MODEL_ec472bf3c24e4a489f6b4666c3e0a9e4","value":" 239/239 [00:00&lt;00:00, 24.3kB/s]"}},"cd6805a6b4bc490bb3dabb61504ab8e6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fed591da0d304c0f8b320ff464c5666c","IPY_MODEL_2c6bd7e97ea2414aac33c456b046e873","IPY_MODEL_6dd438058253463ab15a65ad8f33192f"],"layout":"IPY_MODEL_1872502f5bbf42d1827f773b9bcd37f6"}},"d03f852c858b4a9b82b9edfed62aae95":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e4aca2b76727418c876b97d847884b6f","placeholder":"​","style":"IPY_MODEL_5dd7e4033d7d4a7687910dd2e7030454","value":"vocab.txt: 100%"}},"d0944baaa43845f8a21712e9b942719d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d441459d1c884b52b9772285b2fd17f0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d465a2294ac346fcaeb1e40f905431ed":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4da583e676f4d08a7e4356e6c19f394":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"da87376fe355463386e1bcae2d6146d4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dbca1f857e064aeeabac2f8eac9c9ba7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dde9c02f2afc4fbc9e2f6a8041de369c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e2efb55c88a44d24941853097ec4a60d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e3067982ccdd4f4b9e25d084782b196b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e2efb55c88a44d24941853097ec4a60d","max":571,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bcb1e5270d0b4d76ad75e7f33f15895a","value":571}},"e4aca2b76727418c876b97d847884b6f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e4f13c189dab4b3da8efd1a02af84723":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_56ced83d439f4233974c0b732c4b8008","placeholder":"​","style":"IPY_MODEL_80172f1094b240ebb9474fd8d5ec92b6","value":" 363/363 [00:00&lt;00:00, 32.6kB/s]"}},"e7479909d47f4b32a70cdd30a9469d7d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e8217faab1a64813989a48f3bc71980e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ec09a2d650af46328f1be4ad121ed09f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9c4cd35bd3c64007a9744c871f60f686","placeholder":"​","style":"IPY_MODEL_b1ae715e675d45c2957a160069ac7d08","value":" 363/363 [00:00&lt;00:00, 31.3kB/s]"}},"ec472bf3c24e4a489f6b4666c3e0a9e4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ecaeca1869b54dd3816f8198d483adc4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_82c4db0038a142ffa98f46da523d95f3","placeholder":"​","style":"IPY_MODEL_256df0b0bbc543caa79c397cbbfba153","value":"tokenizer.json: 100%"}},"ece2b16bff0d42e1a080df7ff743a59d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ed78789852be40d4b2b16711594f3bc1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_71ffb7c2f5e24755a283cd5294e40f7e","IPY_MODEL_3e5076a0f2ca4e6cbc4fd25a35cb23dc","IPY_MODEL_cb5b456f7378455e9fb95392b2c27b24"],"layout":"IPY_MODEL_7d29ee8eb0d9461d87c32913f11ea7b5"}},"eee4ebb41d904ad7a3084540c6b2f33a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f19b119822bb42c2aeddb790466c3d78":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f394cc47b76c424ea50f39894d33a95e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ae139a3cd6614bdb8b13997dfa0926c7","IPY_MODEL_e3067982ccdd4f4b9e25d084782b196b","IPY_MODEL_b2026bb57acb43ccabb5254e5a74b4d6"],"layout":"IPY_MODEL_5c535e651f1c4adcbaa60791742af937"}},"f4bb3a71389b4b16b94077ba86f72fac":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_197034abbdda48c48ca7327db1cc3187","max":231536,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3de58cc6746d4b5bb0e82579661a960b","value":231536}},"f9642ec2fd524b17a5a1ce5f4f5c1ee1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"faaa9d56430f4cd1854b1775bc3172ba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fcb2b70818dd4586bc146ccf99f08869":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fed591da0d304c0f8b320ff464c5666c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eee4ebb41d904ad7a3084540c6b2f33a","placeholder":"​","style":"IPY_MODEL_229551aa84fd4c7787adcc867eabba53","value":"special_tokens_map.json: 100%"}},"ffa3a842b49943ba8fffe30c63b7e803":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
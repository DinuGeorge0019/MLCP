
# Directory Structure

### 00_CODEFORCES_DATA
  List of all Codeforces contests that were scraped.

### 01_CODEFORCES_DATASET
  JSON format of the proposed dataset.

### 01_TASK_DATASETS

The dataset is available on Kaggle at the following link:  
[https://www.kaggle.com/datasets/dinuiongeorge/codeforces-competitive-programming-dataset?select=01_TASK_DATASETS](https://www.kaggle.com/datasets/dinuiongeorge/codeforces-competitive-programming-dataset?select=01_TASK_DATASETS)

To fetch it locally, you can use the `fetch_dataset_from_kaggle` command from the `02_DatasetBuilder` directory.

The `01_TASK_DATASETS` directory contains all datasets used and generated by the DatasetBuilder pipeline. Its structure is as follows:

- `00_Datasets_Info/`
  - Contains metadata and plots about the raw Codeforces dataset (e.g., tag and difficulty distributions, dataset info files).
- `01_Raw_Datasets/`
  - Contains the raw and preprocessed Codeforces datasets (`raw_dataset.csv`, `preprocessed_dataset.csv`, etc.).
- `02_Base_Datasets/`
  - `OUR_DATASET/`: Base train/test/validation splits derived from the preprocessed Codeforces dataset.
  - `PSG_PREDICTING_ALGO/`: Base train/test/validation splits from Kim et al.'s dataset.
- `03_Task_Datasets/`
  - `00_DATASET_INFO/`: Tag distribution and other info for both datasets.
  - `01_DATASETS_W_TAG_ENCODING/`
    - Datasets for experiments (train/test/val) with tag encoding, for both our dataset and Kim et al.'s dataset.
  - `02_DATASETS_WO_TAG_ENCODING/`
    - Datasets for experiments (train/test/val) without tag encoding, for both our dataset and Kim et al.'s dataset.
  - `03_DATASETS_ALPACA_ENCODING/`
    - Alpaca-style JSON datasets for instruction-based learning.
  - `04_DATASETS_ENHANCED_W_TAG_ENCODING/`
    - Enhanced datasets with tag encoding (with additional metadata like editorials).
  - `05_DATASETS_ENHANCED_WO_TAG_ENCODING/`
    - Enhanced datasets without tag encoding (with additional metadata like editorials).
- `04_NLI_Datasets/`
  - `AUGMENTED_NLI_DATASETS/`: NLI datasets with augmentation for domain adaptation.
  - `AUGUMENTED_NLI_DYNAMIC_SAMPLING_DATASETS/`: NLI datasets with dynamic sampling.
  - `BASIC_NLI_DATASETS/`: NLI datasets without augmentation.

### 02_DatasetBuilder
Library is designed to facilitate the construction of the proposed dataset and the processing of other datasets referenced in the paper. It includes scripts for data processing, dataset construction, and exploratory data analysis.

For more information, please refer to the README inside the `02_DatasetBuilder` directory.

### 03_Classifier
This framework is used for training and evaluating the classifiers proposed in the paper. It includes all necessary scripts and configurations to replicate the experiments.

Follow the instructions in the README inside the `03_Classifier` directory to replicate the experiments.

### 04_BENCHMARK
This directory contains the benchmark results obtained from executing the experiments described in the `03_Classifier` directory.

### 05_MODELS
This directory contains the models trained locally. These models are the result of applying the proposed methods and experiments described in the paper.

The most important models are also available on Hugging Face:  
[https://huggingface.co/collections/Sauron0019/tagpredictionmodels-6850a43f3b03e89d4cd74f2f](https://huggingface.co/collections/Sauron0019/tagpredictionmodels-6850a43f3b03e89d4cd74f2f)
